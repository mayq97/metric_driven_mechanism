{"text": "Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.", "tokens": ["Neural", "machine", "translation", "(", "NMT", ")", "encodes", "the", "source", "sentence", "in", "a", "universal", "way", "to", "generate", "the", "target", "sentence", "word-by-word", ".", "However", ",", "NMT", "does", "not", "consider", "the", "importance", "of", "word", "in", "the", "sentence", "meaning", ",", "for", "example", ",", "some", "words", "(", "i.e.", ",", "content", "words", ")", "express", "more", "important", "meaning", "than", "others", "(", "i.e.", ",", "function", "words", ")", ".", "To", "address", "this", "limitation", ",", "we", "first", "utilize", "word", "frequency", "information", "to", "distinguish", "between", "content", "and", "function", "words", "in", "a", "sentence", ",", "and", "then", "design", "a", "content", "word-aware", "NMT", "to", "improve", "translation", "performance", ".", "Empirical", "results", "on", "the", "WMT14", "English-to-German", ",", "WMT14", "English-to-French", ",", "and", "WMT17", "Chinese-to-English", "translation", "tasks", "show", "that", "the", "proposed", "methods", "can", "significantly", "improve", "the", "performance", "of", "Transformer-based", "NMT", "."], "entities": [{"type": "Operation", "start": 86, "end": 89, "text": "content word-aware NMT", "sent_idx": 2}, {"type": "Effect", "start": 92, "end": 93, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--34"}
{"text": "Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis.", "tokens": ["Atomic", "clauses", "are", "fundamental", "text", "units", "for", "understanding", "complex", "sentences", ".", "Identifying", "the", "atomic", "sentences", "within", "complex", "sentences", "is", "important", "for", "applications", "such", "as", "summarization", ",", "argument", "mining", ",", "discourse", "analysis", ",", "discourse", "parsing", ",", "and", "question", "answering", ".", "Previous", "work", "mainly", "relies", "on", "rule-based", "methods", "dependent", "on", "parsing", ".", "We", "propose", "a", "new", "task", "to", "decompose", "each", "complex", "sentence", "into", "simple", "sentences", "derived", "from", "the", "tensed", "clauses", "in", "the", "source", ",", "and", "a", "novel", "problem", "formulation", "as", "a", "graph", "edit", "task", ".", "Our", "neural", "model", "learns", "to", "Accept", ",", "Break", ",", "Copy", "or", "Drop", "elements", "of", "a", "graph", "that", "combines", "word", "adjacency", "and", "grammatical", "dependencies", ".", "The", "full", "processing", "pipeline", "includes", "modules", "for", "graph", "construction", ",", "graph", "editing", ",", "and", "sentence", "generation", "from", "the", "output", "graph", ".", "We", "introduce", "DeSSE", ",", "a", "new", "dataset", "designed", "to", "train", "and", "evaluate", "complex", "sentence", "decomposition", ",", "and", "MinWiki", ",", "a", "subset", "of", "MinWikiSplit", ".", "ABCD", "achieves", "comparable", "performance", "as", "two", "parsing", "baselines", "on", "MinWiki", ".", "On", "DeSSE", ",", "which", "has", "a", "more", "even", "balance", "of", "complex", "sentence", "types", ",", "our", "model", "achieves", "higher", "accuracy", "on", "the", "number", "of", "atomic", "sentences", "than", "an", "encoder-decoder", "baseline", ".", "Results", "include", "a", "detailed", "error", "analysis", "."], "entities": [{"type": "Operation", "start": 152, "end": 153, "text": "ABCD", "sent_idx": 7}, {"type": "Effect", "start": 155, "end": 156, "text": "performance", "sent_idx": 7}, {"type": "Operation", "start": 88, "end": 99, "text": "Accept, Break, Copy or Drop elements of a graph", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--303"}
{"text": "Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.", "tokens": ["Several", "attempts", "have", "been", "made", "to", "learn", "phrase", "translation", "probabilities", "for", "phrase-based", "statistical", "machine", "translation", "that", "go", "beyond", "pure", "counting", "of", "phrases", "in", "word-aligned", "training", "data", ".", "Most", "approaches", "report", "problems", "with", "over-fitting", ".", "We", "describe", "a", "novel", "leaving-one-out", "approach", "to", "prevent", "over-fitting", "that", "allows", "us", "to", "train", "phrase", "models", "that", "show", "improved", "translation", "performance", "on", "the", "WMT08", "Europarl", "German-English", "task", ".", "In", "contrast", "to", "most", "previous", "work", "where", "phrase", "models", "were", "trained", "separately", "from", "other", "models", "used", "in", "translation", ",", "we", "include", "all", "components", "such", "as", "single", "word", "lexica", "and", "reordering", "models", "in", "training", ".", "Using", "this", "consistent", "training", "of", "phrase", "models", "we", "are", "able", "to", "achieve", "improvements", "of", "up", "to", "1.4", "points", "in", "BLEU", ".", "As", "a", "side", "effect", ",", "the", "phrase", "table", "size", "is", "reduced", "by", "more", "than", "80", "%", "."], "entities": [{"type": "Operation", "start": 38, "end": 40, "text": "leaving-one-out approach", "sent_idx": 2}, {"type": "Effect", "start": 115, "end": 116, "text": "BLEU", "sent_idx": 4}, {"type": "Effect", "start": 123, "end": 126, "text": "phrase table size", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "P10-1049"}
{"text": "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align.", "tokens": ["The", "cross-lingual", "language", "models", "are", "typically", "pretrained", "with", "masked", "language", "modeling", "on", "multilingual", "text", "or", "parallel", "sentences", ".", "In", "this", "paper", ",", "we", "introduce", "denoising", "word", "alignment", "as", "a", "new", "cross-lingual", "pre-training", "task", ".", "Specifically", ",", "the", "model", "first", "self-label", "word", "alignments", "for", "parallel", "sentences", ".", "Then", "we", "randomly", "mask", "tokens", "in", "a", "bitext", "pair", ".", "Given", "a", "masked", "token", ",", "the", "model", "uses", "a", "pointer", "network", "to", "predict", "the", "aligned", "token", "in", "the", "other", "language", ".", "We", "alternately", "perform", "the", "above", "two", "steps", "in", "an", "expectation-maximization", "manner", ".", "Experimental", "results", "show", "that", "our", "method", "improves", "cross-lingual", "transferability", "on", "various", "datasets", ",", "especially", "on", "the", "token-level", "tasks", ",", "such", "as", "question", "answering", ",", "and", "structured", "prediction", ".", "Moreover", ",", "the", "model", "can", "serve", "as", "a", "pretrained", "word", "aligner", ",", "which", "achieves", "reasonably", "low", "error", "rate", "on", "the", "alignment", "benchmarks", ".", "The", "code", "and", "pretrained", "parameters", "are", "available", "at", "github.com/CZWin32768/XLM-Align", "."], "entities": [{"type": "Operation", "start": 23, "end": 33, "text": "introduce denoising word alignment as a new cross-lingual pre-training task", "sent_idx": 1}, {"type": "Effect", "start": 133, "end": 135, "text": "error rate", "sent_idx": 7}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--265"}
{"text": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.", "tokens": ["Pretrained", "contextualized", "embeddings", "are", "powerful", "word", "representations", "for", "structured", "prediction", "tasks", ".", "Recent", "work", "found", "that", "better", "word", "representations", "can", "be", "obtained", "by", "concatenating", "different", "types", "of", "embeddings", ".", "However", ",", "the", "selection", "of", "embeddings", "to", "form", "the", "best", "concatenated", "representation", "usually", "varies", "depending", "on", "the", "task", "and", "the", "collection", "of", "candidate", "embeddings", ",", "and", "the", "ever-increasing", "number", "of", "embedding", "types", "makes", "it", "a", "more", "difficult", "problem", ".", "In", "this", "paper", ",", "we", "propose", "Automated", "Concatenation", "of", "Embeddings", "(", "ACE", ")", "to", "automate", "the", "process", "of", "finding", "better", "concatenations", "of", "embeddings", "for", "structured", "prediction", "tasks", ",", "based", "on", "a", "formulation", "inspired", "by", "recent", "progress", "on", "neural", "architecture", "search", ".", "Specifically", ",", "a", "controller", "alternately", "samples", "a", "concatenation", "of", "embeddings", ",", "according", "to", "its", "current", "belief", "of", "the", "effectiveness", "of", "individual", "embedding", "types", "in", "consideration", "for", "a", "task", ",", "and", "updates", "the", "belief", "based", "on", "a", "reward", ".", "We", "follow", "strategies", "in", "reinforcement", "learning", "to", "optimize", "the", "parameters", "of", "the", "controller", "and", "compute", "the", "reward", "based", "on", "the", "accuracy", "of", "a", "task", "model", ",", "which", "is", "fed", "with", "the", "sampled", "concatenation", "as", "input", "and", "trained", "on", "a", "task", "dataset", ".", "Empirical", "results", "on", "6", "tasks", "and", "21", "datasets", "show", "that", "our", "approach", "outperforms", "strong", "baselines", "and", "achieves", "state-of-the-art", "performance", "with", "fine-tuned", "embeddings", "in", "all", "the", "evaluations", "."], "entities": [{"type": "Operation", "start": 74, "end": 81, "text": "Automated Concatenation of Embeddings (ACE)", "sent_idx": 3}, {"type": "Effect", "start": 207, "end": 208, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--206"}
{"text": "We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.", "tokens": ["We", "use", "search", "engine", "results", "to", "address", "a", "particularly", "difficult", "cross-domain", "language", "processing", "task", ",", "the", "adaptation", "of", "named", "entity", "recognition", "(", "NER", ")", "from", "news", "text", "to", "web", "queries", ".", "The", "key", "novelty", "of", "the", "method", "is", "that", "we", "submit", "a", "token", "with", "context", "to", "a", "search", "engine", "and", "use", "similar", "contexts", "in", "the", "search", "results", "as", "additional", "information", "for", "correctly", "classifying", "the", "token", ".", "We", "achieve", "strong", "gains", "in", "NER", "performance", "on", "news", ",", "in-domain", "and", "out-of-domain", ",", "and", "on", "web", "queries", "."], "entities": [{"type": "Operation", "start": 40, "end": 49, "text": "submit a token with context to a search engine", "sent_idx": 1}, {"type": "Effect", "start": 72, "end": 73, "text": "performance", "sent_idx": 2}, {"type": "Operation", "start": 50, "end": 60, "text": "use similar contexts in the search results as additional information", "sent_idx": 1}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "P11-1097"}
{"text": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.", "tokens": ["Event", "extraction", "is", "challenging", "due", "to", "the", "complex", "structure", "of", "event", "records", "and", "the", "semantic", "gap", "between", "text", "and", "event", ".", "Traditional", "methods", "usually", "extract", "event", "records", "by", "decomposing", "the", "complex", "structure", "prediction", "task", "into", "multiple", "subtasks", ".", "In", "this", "paper", ",", "we", "propose", "Text2Event", ",", "a", "sequence-to-structure", "generation", "paradigm", "that", "can", "directly", "extract", "events", "from", "the", "text", "in", "an", "end-to-end", "manner", ".", "Specifically", ",", "we", "design", "a", "sequence-to-structure", "network", "for", "unified", "event", "extraction", ",", "a", "constrained", "decoding", "algorithm", "for", "event", "knowledge", "injection", "during", "inference", ",", "and", "a", "curriculum", "learning", "algorithm", "for", "efficient", "model", "learning", ".", "Experimental", "results", "show", "that", ",", "by", "uniformly", "modeling", "all", "tasks", "in", "a", "single", "model", "and", "universally", "predicting", "different", "labels", ",", "our", "method", "can", "achieve", "competitive", "performance", "using", "only", "record-level", "annotations", "in", "both", "supervised", "learning", "and", "transfer", "learning", "settings", "."], "entities": [{"type": "Operation", "start": 44, "end": 45, "text": "Text2Event", "sent_idx": 2}, {"type": "Effect", "start": 121, "end": 122, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--217"}
{"text": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.", "tokens": ["Unsupervised", "Domain", "Adaptation", "(", "UDA", ")", "aims", "to", "transfer", "the", "knowledge", "of", "source", "domain", "to", "the", "unlabeled", "target", "domain", ".", "Existing", "methods", "typically", "require", "to", "learn", "to", "adapt", "the", "target", "model", "by", "exploiting", "the", "source", "data", "and", "sharing", "the", "network", "architecture", "across", "domains", ".", "However", ",", "this", "pipeline", "makes", "the", "source", "data", "risky", "and", "is", "inflexible", "for", "deploying", "the", "target", "model", ".", "This", "paper", "tackles", "a", "novel", "setting", "where", "only", "a", "trained", "source", "model", "is", "available", "and", "different", "network", "architectures", "can", "be", "adapted", "for", "target", "domain", "in", "terms", "of", "deployment", "environments", ".", "We", "propose", "a", "generic", "framework", "named", "Cross-domain", "Knowledge", "Distillation", "(", "CdKD", ")", "without", "needing", "any", "source", "data", ".", "CdKD", "matches", "the", "joint", "distributions", "between", "a", "trained", "source", "model", "and", "a", "set", "of", "target", "data", "during", "distilling", "the", "knowledge", "from", "the", "source", "model", "to", "the", "target", "domain", ".", "As", "a", "type", "of", "important", "knowledge", "in", "the", "source", "domain", ",", "for", "the", "first", "time", ",", "the", "gradient", "information", "is", "exploited", "to", "boost", "the", "transfer", "performance", ".", "Experiments", "on", "cross-domain", "text", "classification", "demonstrate", "that", "CdKD", "achieves", "superior", "performance", ",", "which", "verifies", "the", "effectiveness", "in", "this", "novel", "setting", "."], "entities": [{"type": "Operation", "start": 156, "end": 158, "text": "gradient information", "sent_idx": 6}, {"type": "Effect", "start": 163, "end": 165, "text": "transfer performance", "sent_idx": 6}, {"type": "Operation", "start": 173, "end": 174, "text": "CdKD", "sent_idx": 7}, {"type": "Effect", "start": 176, "end": 177, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--421"}
{"text": "How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.", "tokens": ["How", "does", "the", "input", "segmentation", "of", "pretrained", "language", "models", "(", "PLMs", ")", "affect", "their", "interpretations", "of", "complex", "words", "?", "We", "present", "the", "first", "study", "investigating", "this", "question", ",", "taking", "BERT", "as", "the", "example", "PLM", "and", "focusing", "on", "its", "semantic", "representations", "of", "English", "derivatives", ".", "We", "show", "that", "PLMs", "can", "be", "interpreted", "as", "serial", "dual-route", "models", ",", "i.e.", ",", "the", "meanings", "of", "complex", "words", "are", "either", "stored", "or", "else", "need", "to", "be", "computed", "from", "the", "subwords", ",", "which", "implies", "that", "maximally", "meaningful", "input", "tokens", "should", "allow", "for", "the", "best", "generalization", "on", "new", "words", ".", "This", "hypothesis", "is", "confirmed", "by", "a", "series", "of", "semantic", "probing", "tasks", "on", "which", "DelBERT", "(", "Derivation", "leveraging", "BERT", ")", ",", "a", "model", "with", "derivational", "input", "segmentation", ",", "substantially", "outperforms", "BERT", "with", "WordPiece", "segmentation", ".", "Our", "results", "suggest", "that", "the", "generalization", "capabilities", "of", "PLMs", "could", "be", "further", "improved", "if", "a", "morphologically-informed", "vocabulary", "of", "input", "tokens", "were", "used", "."], "entities": [{"type": "Operation", "start": 142, "end": 147, "text": "morphologically-informed vocabulary of input tokens", "sent_idx": 4}, {"type": "Effect", "start": 132, "end": 136, "text": "generalization capabilities of PLMs", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--279"}
{"text": "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations\u2014including shuffling word order within sentences and deleting all words other than nouns\u2014remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.", "tokens": ["Transformer-based", "language", "models", "benefit", "from", "conditioning", "on", "contexts", "of", "hundreds", "to", "thousands", "of", "previous", "tokens", ".", "What", "aspects", "of", "these", "contexts", "contribute", "to", "accurate", "model", "prediction", "?", "We", "describe", "a", "series", "of", "experiments", "that", "measure", "usable", "information", "by", "selectively", "ablating", "lexical", "and", "structural", "information", "in", "transformer", "language", "models", "trained", "on", "English", "Wikipedia", ".", "In", "both", "mid-", "and", "long-range", "contexts", ",", "we", "find", "that", "several", "extremely", "destructive", "context", "manipulations", "\u2014", "including", "shuffling", "word", "order", "within", "sentences", "and", "deleting", "all", "words", "other", "than", "nouns", "\u2014", "remove", "less", "than", "15", "%", "of", "the", "usable", "information", ".", "Our", "results", "suggest", "that", "long", "contexts", ",", "but", "not", "their", "detailed", "syntactic", "and", "propositional", "content", ",", "are", "important", "for", "the", "low", "perplexity", "of", "current", "transformer", "language", "models", "."], "entities": [{"type": "Operation", "start": 97, "end": 99, "text": "long contexts", "sent_idx": 4}, {"type": "Effect", "start": 114, "end": 115, "text": "perplexity", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--70"}
{"text": "Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.", "tokens": ["Recently", ",", "various", "neural", "models", "for", "multi-party", "conversation", "(", "MPC", ")", "have", "achieved", "impressive", "improvements", "on", "a", "variety", "of", "tasks", "such", "as", "addressee", "recognition", ",", "speaker", "identification", "and", "response", "prediction", ".", "However", ",", "these", "existing", "methods", "on", "MPC", "usually", "represent", "interlocutors", "and", "utterances", "individually", "and", "ignore", "the", "inherent", "complicated", "structure", "in", "MPC", "which", "may", "provide", "crucial", "interlocutor", "and", "utterance", "semantics", "and", "would", "enhance", "the", "conversation", "understanding", "process", ".", "To", "this", "end", ",", "we", "present", "MPC-BERT", ",", "a", "pre-trained", "model", "for", "MPC", "understanding", "that", "considers", "learning", "who", "says", "what", "to", "whom", "in", "a", "unified", "model", "with", "several", "elaborated", "self-supervised", "tasks", ".", "Particularly", ",", "these", "tasks", "can", "be", "generally", "categorized", "into", "(", "1", ")", "interlocutor", "structure", "modeling", "including", "reply-to", "utterance", "recognition", ",", "identical", "speaker", "searching", "and", "pointer", "consistency", "distinction", ",", "and", "(", "2", ")", "utterance", "semantics", "modeling", "including", "masked", "shared", "utterance", "restoration", "and", "shared", "node", "detection", ".", "We", "evaluate", "MPC-BERT", "on", "three", "downstream", "tasks", "including", "addressee", "recognition", ",", "speaker", "identification", "and", "response", "selection", ".", "Experimental", "results", "show", "that", "MPC-BERT", "outperforms", "previous", "methods", "by", "large", "margins", "and", "achieves", "new", "state-of-the-art", "performance", "on", "all", "three", "downstream", "tasks", "at", "two", "benchmarks", "."], "entities": [{"type": "Operation", "start": 166, "end": 167, "text": "MPC-BERT", "sent_idx": 5}, {"type": "Effect", "start": 177, "end": 178, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--285"}
{"text": "The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma. This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography. The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others. Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices. However, the different modeling granularity can make joint modeling more difficult. Our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level). We use Arabic as a test case, and achieve state-of-the-art results for Modern Standard Arabic with 20% relative error reduction, and Egyptian Arabic with 11% relative error reduction.", "tokens": ["The", "written", "forms", "of", "Semitic", "languages", "are", "both", "highly", "ambiguous", "and", "morphologically", "rich", ":", "a", "word", "can", "have", "multiple", "interpretations", "and", "is", "one", "of", "many", "inflected", "forms", "of", "the", "same", "concept", "or", "lemma", ".", "This", "is", "further", "exacerbated", "for", "dialectal", "content", ",", "which", "is", "more", "prone", "to", "noise", "and", "lacks", "a", "standard", "orthography", ".", "The", "morphological", "features", "can", "be", "lexicalized", ",", "like", "lemmas", "and", "diacritized", "forms", ",", "or", "non-lexicalized", ",", "like", "gender", ",", "number", ",", "and", "part-of-speech", "tags", ",", "among", "others", ".", "Joint", "modeling", "of", "the", "lexicalized", "and", "non-lexicalized", "features", "can", "identify", "more", "intricate", "morphological", "patterns", ",", "which", "provide", "better", "context", "modeling", ",", "and", "further", "disambiguate", "ambiguous", "lexical", "choices", ".", "However", ",", "the", "different", "modeling", "granularity", "can", "make", "joint", "modeling", "more", "difficult", ".", "Our", "approach", "models", "the", "different", "features", "jointly", ",", "whether", "lexicalized", "(", "on", "the", "character-level", ")", ",", "or", "non-lexicalized", "(", "on", "the", "word-level", ")", ".", "We", "use", "Arabic", "as", "a", "test", "case", ",", "and", "achieve", "state-of-the-art", "results", "for", "Modern", "Standard", "Arabic", "with", "20", "%", "relative", "error", "reduction", ",", "and", "Egyptian", "Arabic", "with", "11", "%", "relative", "error", "reduction", "."], "entities": [{"type": "Operation", "start": 125, "end": 130, "text": "models the different features jointly", "sent_idx": 5}, {"type": "Effect", "start": 166, "end": 168, "text": "relative error", "sent_idx": 6}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--736"}
{"text": "This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker's opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.", "tokens": ["This", "paper", "presents", "a", "pilot", "study", "of", "opinion", "summarization", "on", "conversations", ".", "We", "create", "a", "corpus", "containing", "extractive", "and", "abstractive", "summaries", "of", "speaker", "'s", "opinion", "towards", "a", "given", "topic", "using", "88", "telephone", "conversations", ".", "We", "adopt", "two", "methods", "to", "perform", "extractive", "summarization", ".", "The", "first", "one", "is", "a", "sentence-ranking", "method", "that", "linearly", "combines", "scores", "measured", "from", "different", "aspects", "including", "topic", "relevance", ",", "subjectivity", ",", "and", "sentence", "importance", ".", "The", "second", "one", "is", "a", "graph-based", "method", ",", "which", "incorporates", "topic", "and", "sentiment", "information", ",", "as", "well", "as", "additional", "information", "about", "sentence-to-sentence", "relations", "extracted", "based", "on", "dialogue", "structure", ".", "Our", "evaluation", "results", "show", "that", "both", "methods", "significantly", "outperform", "the", "baseline", "approach", "that", "extracts", "the", "longest", "utterances", ".", "In", "particular", ",", "we", "find", "that", "incorporating", "dialogue", "structure", "in", "the", "graph-based", "method", "contributes", "to", "the", "improved", "system", "performance", "."], "entities": [{"type": "Operation", "start": 121, "end": 128, "text": "incorporating dialogue structure in the graph-based method", "sent_idx": 6}, {"type": "Effect", "start": 133, "end": 134, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1034"}
{"text": "Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The \u201cGenerative Parsing\u201d idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The \u201cStructural Scaffold\u201d idea guides the language model\u2019s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models\u2019 syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.", "tokens": ["Transformer-based", "language", "models", "pre-trained", "on", "large", "amounts", "of", "text", "data", "have", "proven", "remarkably", "successful", "in", "learning", "generic", "transferable", "linguistic", "representations", ".", "Here", "we", "study", "whether", "structural", "guidance", "leads", "to", "more", "human-like", "systematic", "linguistic", "generalization", "in", "Transformer", "language", "models", "without", "resorting", "to", "pre-training", "on", "very", "large", "amounts", "of", "data", ".", "We", "explore", "two", "general", "ideas", ".", "The", "\u201c", "Generative", "Parsing", "\u201d", "idea", "jointly", "models", "the", "incremental", "parse", "and", "word", "sequence", "as", "part", "of", "the", "same", "sequence", "modeling", "task", ".", "The", "\u201c", "Structural", "Scaffold", "\u201d", "idea", "guides", "the", "language", "model", "\u2019s", "representation", "via", "additional", "structure", "loss", "that", "separately", "predicts", "the", "incremental", "constituency", "parse", ".", "We", "train", "the", "proposed", "models", "along", "with", "a", "vanilla", "Transformer", "language", "model", "baseline", "on", "a", "14", "million-token", "and", "a", "46", "million-token", "subset", "of", "the", "BLLIP", "dataset", ",", "and", "evaluate", "models", "\u2019", "syntactic", "generalization", "performances", "on", "SG", "Test", "Suites", "and", "sized", "BLiMP", ".", "Experiment", "results", "across", "two", "benchmarks", "suggest", "converging", "evidence", "that", "generative", "structural", "supervisions", "can", "induce", "more", "robust", "and", "humanlike", "linguistic", "generalization", "in", "Transformer", "language", "models", "without", "the", "need", "for", "data", "intensive", "pre-training", "."], "entities": [{"type": "Operation", "start": 153, "end": 156, "text": "generative structural supervisions", "sent_idx": 6}, {"type": "Effect", "start": 163, "end": 164, "text": "generalization", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--289"}
{"text": "Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description. DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision. Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-art models and human performance, suggesting that the data will support significant future work.", "tokens": ["Short", "textual", "descriptions", "of", "entities", "provide", "summaries", "of", "their", "key", "attributes", "and", "have", "been", "shown", "to", "be", "useful", "sources", "of", "background", "knowledge", "for", "tasks", "such", "as", "entity", "linking", "and", "question", "answering", ".", "However", ",", "generating", "entity", "descriptions", ",", "especially", "for", "new", "and", "long-tail", "entities", ",", "can", "be", "challenging", "since", "relevant", "information", "is", "often", "scattered", "across", "multiple", "sources", "with", "varied", "content", "and", "style", ".", "We", "introduce", "DESCGEN", ":", "given", "mentions", "spread", "over", "multiple", "documents", ",", "the", "goal", "is", "to", "generate", "an", "entity", "summary", "description", ".", "DESCGEN", "consists", "of", "37", "K", "entity", "descriptions", "from", "Wikipedia", "and", "Fandom", ",", "each", "paired", "with", "nine", "evidence", "documents", "on", "average", ".", "The", "documents", "were", "collected", "using", "a", "combination", "of", "entity", "linking", "and", "hyperlinks", "into", "the", "entity", "pages", ",", "which", "together", "provide", "high-quality", "distant", "supervision", ".", "Compared", "to", "other", "multi-document", "summarization", "tasks", ",", "our", "task", "is", "entity-centric", ",", "more", "abstractive", ",", "and", "covers", "a", "wide", "range", "of", "domains", ".", "We", "also", "propose", "a", "two-stage", "extract-then-generate", "baseline", "and", "show", "that", "there", "exists", "a", "large", "gap", "(", "19.9", "%", "in", "ROUGE-L", ")", "between", "state-of-art", "models", "and", "human", "performance", ",", "suggesting", "that", "the", "data", "will", "support", "significant", "future", "work", "."], "entities": [{"type": "Operation", "start": 156, "end": 159, "text": "two-stage extract-then-generate baseline", "sent_idx": 6}, {"type": "Effect", "start": 171, "end": 172, "text": "ROUGE-L", "sent_idx": 6}, {"type": "Effect", "start": 178, "end": 179, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--35"}
{"text": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding.", "tokens": ["In", "this", "paper", ",", "we", "propose", "Shallow", "Aggressive", "Decoding", "(", "SAD", ")", "to", "improve", "the", "online", "inference", "efficiency", "of", "the", "Transformer", "for", "instantaneous", "Grammatical", "Error", "Correction", "(", "GEC", ")", ".", "SAD", "optimizes", "the", "online", "inference", "efficiency", "for", "GEC", "by", "two", "innovations", ":", "1", ")", "it", "aggressively", "decodes", "as", "many", "tokens", "as", "possible", "in", "parallel", "instead", "of", "always", "decoding", "only", "one", "token", "in", "each", "step", "to", "improve", "computational", "parallelism", ";", "2", ")", "it", "uses", "a", "shallow", "decoder", "instead", "of", "the", "conventional", "Transformer", "architecture", "with", "balanced", "encoder-decoder", "depth", "to", "reduce", "the", "computational", "cost", "during", "inference", ".", "Experiments", "in", "both", "English", "and", "Chinese", "GEC", "benchmarks", "show", "that", "aggressive", "decoding", "could", "yield", "identical", "predictions", "to", "greedy", "decoding", "but", "with", "significant", "speedup", "for", "online", "inference", ".", "Its", "combination", "with", "the", "shallow", "decoder", "could", "offer", "an", "even", "higher", "online", "inference", "speedup", "over", "the", "powerful", "Transformer", "baseline", "without", "quality", "loss", ".", "Not", "only", "does", "our", "approach", "allow", "a", "single", "model", "to", "achieve", "the", "state-of-the-art", "results", "in", "English", "GEC", "benchmarks", ":", "66.4", "F0.5", "in", "the", "CoNLL-14", "and", "72.9", "F0.5", "in", "the", "BEA-19", "test", "set", "with", "an", "almost", "10x", "online", "inference", "speedup", "over", "the", "Transformer-big", "model", ",", "but", "also", "it", "is", "easily", "adapted", "to", "other", "languages", ".", "Our", "code", "is", "available", "at", "https://github.com/AutoTemp/Shallow-Aggressive-Decoding", "."], "entities": [{"type": "Operation", "start": 6, "end": 12, "text": "Shallow Aggressive Decoding (SAD)", "sent_idx": 0}, {"type": "Effect", "start": 164, "end": 165, "text": "F0.5", "sent_idx": 4}, {"type": "Effect", "start": 170, "end": 171, "text": "F0.5", "sent_idx": 4}, {"type": "Effect", "start": 181, "end": 183, "text": "inference speedup", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--462"}
{"text": "Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.", "tokens": ["Extracting", "knowledge", "from", "unstructured", "text", "is", "a", "long-standing", "goal", "of", "NLP", ".", "Although", "learning", "approaches", "to", "many", "of", "its", "subtasks", "have", "been", "developed", "(", "e.g.", ",", "parsing", ",", "taxonomy", "induction", ",", "information", "extraction", ")", ",", "all", "end-to-end", "solutions", "to", "date", "require", "heavy", "supervision", "and/or", "manual", "engineering", ",", "limiting", "their", "scope", "and", "scalability", ".", "We", "present", "OntoUSP", ",", "a", "system", "that", "induces", "and", "populates", "a", "probabilistic", "ontology", "using", "only", "dependency-parsed", "text", "as", "input", ".", "OntoUSP", "builds", "on", "the", "USP", "unsupervised", "semantic", "parser", "by", "jointly", "forming", "ISA", "and", "IS-PART", "hierarchies", "of", "lambda-form", "clusters", ".", "The", "ISA", "hierarchy", "allows", "more", "general", "knowledge", "to", "be", "learned", ",", "and", "the", "use", "of", "smoothing", "for", "parameter", "estimation", ".", "We", "evaluate", "OntoUSP", "by", "using", "it", "to", "extract", "a", "knowledge", "base", "from", "biomedical", "abstracts", "and", "answer", "questions", ".", "OntoUSP", "improves", "on", "the", "recall", "of", "USP", "by", "47", "%", "and", "greatly", "outperforms", "previous", "state-of-the-art", "approaches", "."], "entities": [{"type": "Operation", "start": 130, "end": 131, "text": "OntoUSP", "sent_idx": 6}, {"type": "Effect", "start": 134, "end": 135, "text": "recall", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1031"}
{"text": "Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.", "tokens": ["Mixed", "counting", "models", "that", "use", "the", "negative", "binomial", "distribution", "as", "the", "prior", "can", "well", "model", "over-dispersed", "and", "hierarchically", "dependent", "random", "variables", ";", "thus", "they", "have", "attracted", "much", "attention", "in", "mining", "dispersed", "document", "topics", ".", "However", ",", "the", "existing", "parameter", "inference", "method", "like", "Monte", "Carlo", "sampling", "is", "quite", "time-consuming", ".", "In", "this", "paper", ",", "we", "propose", "two", "efficient", "neural", "mixed", "counting", "models", ",", "i.e.", ",", "the", "Negative", "Binomial-Neural", "Topic", "Model", "(", "NB-NTM", ")", "and", "the", "Gamma", "Negative", "Binomial-Neural", "Topic", "Model", "(", "GNB-NTM", ")", "for", "dispersed", "topic", "discovery", ".", "Neural", "variational", "inference", "algorithms", "are", "developed", "to", "infer", "model", "parameters", "by", "using", "the", "reparameterization", "of", "Gamma", "distribution", "and", "the", "Gaussian", "approximation", "of", "Poisson", "distribution", ".", "Experiments", "on", "real-world", "datasets", "indicate", "that", "our", "models", "outperform", "state-of-the-art", "baseline", "models", "in", "terms", "of", "perplexity", "and", "topic", "coherence", ".", "The", "results", "also", "validate", "that", "both", "NB-NTM", "and", "GNB-NTM", "can", "produce", "explainable", "intermediate", "variables", "by", "generating", "dispersed", "proportions", "of", "document", "topics", "."], "entities": [{"type": "Operation", "start": 64, "end": 72, "text": "the Negative Binomial-Neural Topic Model (NB-NTM)", "sent_idx": 2}, {"type": "Effect", "start": 127, "end": 128, "text": "perplexity", "sent_idx": 4}, {"type": "Effect", "start": 129, "end": 131, "text": "topic coherence", "sent_idx": 4}, {"type": "Operation", "start": 73, "end": 82, "text": "the Gamma Negative Binomial-Neural Topic Model (GNB-NTM)", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 1}, {"type": "Pos_Affect", "head": 3, "tail": 2}], "id": "abstract-2020--acl-main--548"}
{"text": "Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of \u201cChinese words\u201d and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.", "tokens": ["Fully", "supervised", "neural", "approaches", "have", "achieved", "significant", "progress", "in", "the", "task", "of", "Chinese", "word", "segmentation", "(", "CWS", ")", ".", "Nevertheless", ",", "the", "performance", "of", "supervised", "models", "always", "drops", "gravely", "if", "the", "domain", "shifts", "due", "to", "the", "distribution", "gap", "across", "domains", "and", "the", "out", "of", "vocabulary", "(", "OOV", ")", "problem", ".", "In", "order", "to", "simultaneously", "alleviate", "the", "issues", ",", "this", "paper", "intuitively", "couples", "distant", "annotation", "and", "adversarial", "training", "for", "cross-domain", "CWS", ".", "1", ")", "We", "rethink", "the", "essence", "of", "\u201c", "Chinese", "words", "\u201d", "and", "design", "an", "automatic", "distant", "annotation", "mechanism", ",", "which", "does", "not", "need", "any", "supervision", "or", "pre-defined", "dictionaries", "on", "the", "target", "domain", ".", "The", "method", "could", "effectively", "explore", "domain-specific", "words", "and", "distantly", "annotate", "the", "raw", "texts", "for", "the", "target", "domain", ".", "2", ")", "We", "further", "develop", "a", "sentence-level", "adversarial", "training", "procedure", "to", "perform", "noise", "reduction", "and", "maximum", "utilization", "of", "the", "source", "domain", "information", ".", "Experiments", "on", "multiple", "real-world", "datasets", "across", "various", "domains", "show", "the", "superiority", "and", "robustness", "of", "our", "model", ",", "significantly", "outperforming", "previous", "state-of-the-arts", "cross-domain", "CWS", "methods", "."], "entities": [{"type": "Operation", "start": 61, "end": 67, "text": "couples distant annotation and adversarial training", "sent_idx": 2}, {"type": "Effect", "start": 157, "end": 158, "text": "robustness", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--595"}
{"text": "Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts. CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors. To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token \u201c[MASK]\u201d as in BERT. Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level. Moreover, phonological and visual similarity knowledge is important to this task. PLOME utilizes GRU networks to model such knowledge based on characters\u2019 phonics and strokes. Experiments are conducted on widely used benchmarks. Our method achieves superior performance against state-of-the-art approaches by a remarkable margin. We release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/PLOME).", "tokens": ["Chinese", "spelling", "correction", "(", "CSC", ")", "is", "a", "task", "to", "detect", "and", "correct", "spelling", "errors", "in", "texts", ".", "CSC", "is", "essentially", "a", "linguistic", "problem", ",", "thus", "the", "ability", "of", "language", "understanding", "is", "crucial", "to", "this", "task", ".", "In", "this", "paper", ",", "we", "propose", "a", "Pre-trained", "masked", "Language", "model", "with", "Misspelled", "knowledgE", "(", "PLOME", ")", "for", "CSC", ",", "which", "jointly", "learns", "how", "to", "understand", "language", "and", "correct", "spelling", "errors", ".", "To", "this", "end", ",", "PLOME", "masks", "the", "chosen", "tokens", "with", "similar", "characters", "according", "to", "a", "confusion", "set", "rather", "than", "the", "fixed", "token", "\u201c", "[", "MASK", "]", "\u201d", "as", "in", "BERT", ".", "Besides", "character", "prediction", ",", "PLOME", "also", "introduces", "pronunciation", "prediction", "to", "learn", "the", "misspelled", "knowledge", "on", "phonic", "level", ".", "Moreover", ",", "phonological", "and", "visual", "similarity", "knowledge", "is", "important", "to", "this", "task", ".", "PLOME", "utilizes", "GRU", "networks", "to", "model", "such", "knowledge", "based", "on", "characters", "\u2019", "phonics", "and", "strokes", ".", "Experiments", "are", "conducted", "on", "widely", "used", "benchmarks", ".", "Our", "method", "achieves", "superior", "performance", "against", "state-of-the-art", "approaches", "by", "a", "remarkable", "margin", ".", "We", "release", "the", "source", "code", "and", "pre-trained", "model", "for", "further", "use", "by", "the", "community", "(", "https://github.com/liushulinle/PLOME", ")", "."], "entities": [{"type": "Operation", "start": 44, "end": 54, "text": "Pre-trained masked Language model with Misspelled knowledgE (PLOME)", "sent_idx": 2}, {"type": "Effect", "start": 159, "end": 160, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--233"}
{"text": "Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets.", "tokens": ["Recent", "proposed", "approaches", "have", "made", "promising", "progress", "in", "dialogue", "state", "tracking", "(", "DST", ")", ".", "However", ",", "in", "multi-domain", "scenarios", ",", "ellipsis", "and", "reference", "are", "frequently", "adopted", "by", "users", "to", "express", "values", "that", "have", "been", "mentioned", "by", "slots", "from", "other", "domains", ".", "To", "handle", "these", "phenomena", ",", "we", "propose", "a", "Dialogue", "State", "Tracking", "with", "Slot", "Connections", "(", "DST-SC", ")", "model", "to", "explicitly", "consider", "slot", "correlations", "across", "different", "domains", ".", "Given", "a", "target", "slot", ",", "the", "slot", "connecting", "mechanism", "in", "DST-SC", "can", "infer", "its", "source", "slot", "and", "copy", "the", "source", "slot", "value", "directly", ",", "thus", "significantly", "reducing", "the", "difficulty", "of", "learning", "and", "reasoning", ".", "Experimental", "results", "verify", "the", "benefits", "of", "explicit", "slot", "connection", "modeling", ",", "and", "our", "model", "achieves", "state-of-the-art", "performance", "on", "MultiWOZ", "2.0", "and", "MultiWOZ", "2.1", "datasets", "."], "entities": [{"type": "Operation", "start": 50, "end": 60, "text": "Dialogue State Tracking with Slot Connections (DST-SC) model", "sent_idx": 2}, {"type": "Effect", "start": 119, "end": 120, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--5"}
{"text": "This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.", "tokens": ["This", "paper", "presents", "a", "tree-structured", "neural", "topic", "model", ",", "which", "has", "a", "topic", "distribution", "over", "a", "tree", "with", "an", "infinite", "number", "of", "branches", ".", "Our", "model", "parameterizes", "an", "unbounded", "ancestral", "and", "fraternal", "topic", "distribution", "by", "applying", "doubly-recurrent", "neural", "networks", ".", "With", "the", "help", "of", "autoencoding", "variational", "Bayes", ",", "our", "model", "improves", "data", "scalability", "and", "achieves", "competitive", "performance", "when", "inducing", "latent", "topics", "and", "tree", "structures", ",", "as", "compared", "to", "a", "prior", "tree-structured", "topic", "model", "(", "Blei", "et", "al.", ",", "2010", ")", ".", "This", "work", "extends", "the", "tree-structured", "topic", "model", "such", "that", "it", "can", "be", "incorporated", "with", "neural", "models", "for", "downstream", "tasks", "."], "entities": [{"type": "Operation", "start": 44, "end": 47, "text": "autoencoding variational Bayes", "sent_idx": 2}, {"type": "Effect", "start": 52, "end": 53, "text": "scalability", "sent_idx": 2}, {"type": "Effect", "start": 56, "end": 57, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--73"}
{"text": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.", "tokens": ["Transformers", "are", "not", "suited", "for", "processing", "long", "documents", ",", "due", "to", "their", "quadratically", "increasing", "memory", "and", "time", "consumption", ".", "Simply", "truncating", "a", "long", "document", "or", "applying", "the", "sparse", "attention", "mechanism", "will", "incur", "the", "context", "fragmentation", "problem", "or", "lead", "to", "an", "inferior", "modeling", "capability", "against", "comparable", "model", "sizes", ".", "In", "this", "paper", ",", "we", "propose", "ERNIE-Doc", ",", "a", "document-level", "language", "pretraining", "model", "based", "on", "Recurrence", "Transformers", ".", "Two", "well-designed", "techniques", ",", "namely", "the", "retrospective", "feed", "mechanism", "and", "the", "enhanced", "recurrence", "mechanism", ",", "enable", "ERNIE-Doc", ",", "which", "has", "a", "much", "longer", "effective", "context", "length", ",", "to", "capture", "the", "contextual", "information", "of", "a", "complete", "document", ".", "We", "pretrain", "ERNIE-Doc", "to", "explicitly", "learn", "the", "relationships", "among", "segments", "with", "an", "additional", "document-aware", "segment-reordering", "objective", ".", "Various", "experiments", "were", "conducted", "on", "both", "English", "and", "Chinese", "document-level", "tasks", ".", "ERNIE-Doc", "improved", "the", "state-of-the-art", "language", "modeling", "result", "of", "perplexity", "to", "16.8", "on", "WikiText-103", ".", "Moreover", ",", "it", "outperformed", "competitive", "pretraining", "models", "by", "a", "large", "margin", "on", "most", "language", "understanding", "tasks", ",", "such", "as", "text", "classification", "and", "question", "answering", "."], "entities": [{"type": "Operation", "start": 132, "end": 133, "text": "ERNIE-Doc", "sent_idx": 6}, {"type": "Effect", "start": 140, "end": 141, "text": "perplexity", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--227"}
{"text": "We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.", "tokens": ["We", "propose", "variable-in-situ", "logico-semantic", "graphs", "to", "bridge", "the", "gap", "between", "semantic", "graph", "and", "logical", "form", "parsing", ".", "The", "new", "type", "of", "graph-based", "meaning", "representation", "allows", "us", "to", "include", "analysis", "for", "scope-related", "phenomena", ",", "such", "as", "quantification", ",", "negation", "and", "modality", ",", "in", "a", "way", "that", "is", "consistent", "with", "the", "state-of-the-art", "underspecification", "approach", ".", "Moreover", ",", "the", "well-formedness", "of", "such", "a", "graph", "is", "clear", ",", "since", "model-theoretic", "interpretation", "is", "available", ".", "We", "demonstrate", "the", "effectiveness", "of", "this", "new", "perspective", "by", "developing", "a", "new", "state-of-the-art", "semantic", "parser", "for", "English", "Resource", "Semantics", ".", "At", "the", "core", "of", "this", "parser", "is", "a", "novel", "neural", "graph", "rewriting", "system", "which", "combines", "the", "strengths", "of", "Hyperedge", "Replacement", "Grammar", ",", "a", "knowledge-intensive", "model", ",", "and", "Graph", "Neural", "Networks", ",", "a", "data-intensive", "model", ".", "Our", "parser", "achieves", "an", "accuracy", "of", "92.39", "%", "in", "terms", "of", "elementary", "dependency", "match", ",", "which", "is", "a", "2.88", "point", "improvement", "over", "the", "best", "data-driven", "model", "in", "the", "literature", ".", "The", "output", "of", "our", "parser", "is", "highly", "coherent", ":", "at", "least", "91", "%", "graphs", "are", "valid", ",", "in", "that", "they", "allow", "at", "least", "one", "sound", "scope-resolved", "logical", "form", "."], "entities": [{"type": "Operation", "start": 99, "end": 103, "text": "neural graph rewriting system", "sent_idx": 4}, {"type": "Effect", "start": 129, "end": 130, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--605"}
{"text": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don\u2019t have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.", "tokens": ["Recent", "Transformer-based", "contextual", "word", "representations", ",", "including", "BERT", "and", "XLNet", ",", "have", "shown", "state-of-the-art", "performance", "in", "multiple", "disciplines", "within", "NLP", ".", "Fine-tuning", "the", "trained", "contextual", "models", "on", "task-specific", "datasets", "has", "been", "the", "key", "to", "achieving", "superior", "performance", "downstream", ".", "While", "fine-tuning", "these", "pre-trained", "models", "is", "straightforward", "for", "lexical", "applications", "(", "applications", "with", "only", "language", "modality", ")", ",", "it", "is", "not", "trivial", "for", "multimodal", "language", "(", "a", "growing", "area", "in", "NLP", "focused", "on", "modeling", "face-to-face", "communication", ")", ".", "More", "specifically", ",", "this", "is", "due", "to", "the", "fact", "that", "pre-trained", "models", "do", "n\u2019t", "have", "the", "necessary", "components", "to", "accept", "two", "extra", "modalities", "of", "vision", "and", "acoustic", ".", "In", "this", "paper", ",", "we", "proposed", "an", "attachment", "to", "BERT", "and", "XLNet", "called", "Multimodal", "Adaptation", "Gate", "(", "MAG", ")", ".", "MAG", "allows", "BERT", "and", "XLNet", "to", "accept", "multimodal", "nonverbal", "data", "during", "fine-tuning", ".", "It", "does", "so", "by", "generating", "a", "shift", "to", "internal", "representation", "of", "BERT", "and", "XLNet", ";", "a", "shift", "that", "is", "conditioned", "on", "the", "visual", "and", "acoustic", "modalities", ".", "In", "our", "experiments", ",", "we", "study", "the", "commonly", "used", "CMU-MOSI", "and", "CMU-MOSEI", "datasets", "for", "multimodal", "sentiment", "analysis", ".", "Fine-tuning", "MAG-BERT", "and", "MAG-XLNet", "significantly", "boosts", "the", "sentiment", "analysis", "performance", "over", "previous", "baselines", "as", "well", "as", "language-only", "fine-tuning", "of", "BERT", "and", "XLNet", ".", "On", "the", "CMU-MOSI", "dataset", ",", "MAG-XLNet", "achieves", "human-level", "multimodal", "sentiment", "analysis", "performance", "for", "the", "first", "time", "in", "the", "NLP", "community", "."], "entities": [{"type": "Operation", "start": 183, "end": 187, "text": "Fine-tuning MAG-BERT and MAG-XLNet", "sent_idx": 8}, {"type": "Effect", "start": 192, "end": 193, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--214"}
{"text": "Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks. However, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling. In this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks. To further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers. Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit. The token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it. The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%\u223c75% inference cost with minimal performance degradation. Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2\u00d7, 3\u00d7, and 4\u00d7.", "tokens": ["Both", "performance", "and", "efficiency", "are", "crucial", "factors", "for", "sequence", "labeling", "tasks", "in", "many", "real-world", "scenarios", ".", "Although", "the", "pre-trained", "models", "(", "PTMs", ")", "have", "significantly", "improved", "the", "performance", "of", "various", "sequence", "labeling", "tasks", ",", "their", "computational", "cost", "is", "expensive", ".", "To", "alleviate", "this", "problem", ",", "we", "extend", "the", "recent", "successful", "early-exit", "mechanism", "to", "accelerate", "the", "inference", "of", "PTMs", "for", "sequence", "labeling", "tasks", ".", "However", ",", "existing", "early-exit", "mechanisms", "are", "specifically", "designed", "for", "sequence-level", "tasks", ",", "rather", "than", "sequence", "labeling", ".", "In", "this", "paper", ",", "we", "first", "propose", "a", "simple", "extension", "of", "sentence-level", "early-exit", "for", "sequence", "labeling", "tasks", ".", "To", "further", "reduce", "the", "computational", "cost", ",", "we", "also", "propose", "a", "token-level", "early-exit", "mechanism", "that", "allows", "partial", "tokens", "to", "exit", "early", "at", "different", "layers", ".", "Considering", "the", "local", "dependency", "inherent", "in", "sequence", "labeling", ",", "we", "employed", "a", "window-based", "criterion", "to", "decide", "for", "a", "token", "whether", "or", "not", "to", "exit", ".", "The", "token-level", "early-exit", "brings", "the", "gap", "between", "training", "and", "inference", ",", "so", "we", "introduce", "an", "extra", "self-sampling", "fine-tuning", "stage", "to", "alleviate", "it", ".", "The", "extensive", "experiments", "on", "three", "popular", "sequence", "labeling", "tasks", "show", "that", "our", "approach", "can", "save", "up", "to", "66%\u223c75", "%", "inference", "cost", "with", "minimal", "performance", "degradation", ".", "Compared", "with", "competitive", "compressed", "models", "such", "as", "DistilBERT", ",", "our", "approach", "can", "achieve", "better", "performance", "under", "the", "same", "speed-up", "ratios", "of", "2", "\u00d7", ",", "3", "\u00d7", ",", "and", "4", "\u00d7", "."], "entities": [{"type": "Operation", "start": 109, "end": 112, "text": "token-level early-exit mechanism", "sent_idx": 5}, {"type": "Effect", "start": 102, "end": 104, "text": "computational cost", "sent_idx": 5}, {"type": "Operation", "start": 91, "end": 93, "text": "sentence-level early-exit", "sent_idx": 4}, {"type": "Effect", "start": 190, "end": 192, "text": "inference cost", "sent_idx": 8}, {"type": "Effect", "start": 194, "end": 195, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 3}, {"type": "Affect", "head": 2, "tail": 4}], "id": "abstract-2021--acl-long--16"}
{"text": "We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars.", "tokens": ["We", "present", "an", "efficient", "algorithm", "for", "computing", "the", "weakest", "readings", "of", "semantically", "ambiguous", "sentences", ".", "A", "corpus-based", "evaluation", "with", "a", "large-scale", "grammar", "shows", "that", "our", "algorithm", "reduces", "over", "80", "%", "of", "sentences", "to", "one", "or", "two", "readings", ",", "in", "negligible", "runtime", ",", "and", "thus", "makes", "it", "possible", "to", "work", "with", "semantic", "representations", "derived", "by", "deep", "large-scale", "grammars", "."], "entities": [{"type": "Operation", "start": 2, "end": 5, "text": "an efficient algorithm", "sent_idx": 0}, {"type": "Effect", "start": 36, "end": 37, "text": "readings", "sent_idx": 1}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1004"}
{"text": "Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data.", "tokens": ["Many", "natural", "language", "questions", "require", "qualitative", ",", "quantitative", "or", "logical", "comparisons", "between", "two", "entities", "or", "events", ".", "This", "paper", "addresses", "the", "problem", "of", "improving", "the", "accuracy", "and", "consistency", "of", "responses", "to", "comparison", "questions", "by", "integrating", "logic", "rules", "and", "neural", "models", ".", "Our", "method", "leverages", "logical", "and", "linguistic", "knowledge", "to", "augment", "labeled", "training", "data", "and", "then", "uses", "a", "consistency-based", "regularizer", "to", "train", "the", "model", ".", "Improving", "the", "global", "consistency", "of", "predictions", ",", "our", "approach", "achieves", "large", "improvements", "over", "previous", "methods", "in", "a", "variety", "of", "question", "answering", "(", "QA", ")", "tasks", ",", "including", "multiple-choice", "qualitative", "reasoning", ",", "cause-effect", "reasoning", ",", "and", "extractive", "machine", "reading", "comprehension", ".", "In", "particular", ",", "our", "method", "significantly", "improves", "the", "performance", "of", "RoBERTa-based", "models", "by", "1", "-", "5", "%", "across", "datasets", ".", "We", "advance", "state", "of", "the", "art", "by", "around", "5", "-", "8", "%", "on", "WIQA", "and", "QuaRel", "and", "reduce", "consistency", "violations", "by", "58", "%", "on", "HotpotQA", ".", "We", "further", "demonstrate", "that", "our", "approach", "can", "learn", "effectively", "from", "limited", "data", "."], "entities": [{"type": "Operation", "start": 34, "end": 40, "text": "integrating logic rules and neural models", "sent_idx": 1}, {"type": "Effect", "start": 25, "end": 26, "text": "accuracy", "sent_idx": 1}, {"type": "Effect", "start": 27, "end": 28, "text": "consistency", "sent_idx": 1}, {"type": "Effect", "start": 112, "end": 113, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--499"}
{"text": "Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.", "tokens": ["Via", "an", "oracle", "experiment", ",", "we", "show", "that", "the", "upper", "bound", "on", "accuracy", "of", "a", "CCG", "parser", "is", "significantly", "lowered", "when", "its", "search", "space", "is", "pruned", "using", "a", "supertagger", ",", "though", "the", "supertagger", "also", "prunes", "many", "bad", "parses", ".", "Inspired", "by", "this", "analysis", ",", "we", "design", "a", "single", "model", "with", "both", "supertagging", "and", "parsing", "features", ",", "rather", "than", "separating", "them", "into", "distinct", "models", "chained", "together", "in", "a", "pipeline", ".", "To", "overcome", "the", "resulting", "increase", "in", "complexity", ",", "we", "experiment", "with", "both", "belief", "propagation", "and", "dual", "decomposition", "approaches", "to", "inference", ",", "the", "first", "empirical", "comparison", "of", "these", "algorithms", "that", "we", "are", "aware", "of", "on", "a", "structured", "natural", "language", "processing", "problem", ".", "On", "CCGbank", "we", "achieve", "a", "labelled", "dependency", "F-measure", "of", "88.8", "%", "on", "gold", "POS", "tags", ",", "and", "86.7", "%", "on", "automatic", "part-of-speeoch", "tags", ",", "the", "best", "reported", "results", "for", "this", "task", "."], "entities": [{"type": "Operation", "start": 46, "end": 55, "text": "a single model with both supertagging and parsing features", "sent_idx": 1}, {"type": "Effect", "start": 117, "end": 118, "text": "F-measure", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1048"}
{"text": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.", "tokens": ["This", "paper", "proposes", "Dynamic", "Memory", "Induction", "Networks", "(", "DMIN", ")", "for", "few-short", "text", "classification", ".", "The", "model", "develops", "a", "dynamic", "routing", "mechanism", "over", "static", "memory", ",", "enabling", "it", "to", "better", "adapt", "to", "unseen", "classes", ",", "a", "critical", "capability", "for", "few-short", "classification", ".", "The", "model", "also", "expands", "the", "induction", "process", "with", "supervised", "learning", "weights", "and", "query", "information", "to", "enhance", "the", "generalization", "ability", "of", "meta-learning", ".", "The", "proposed", "model", "brings", "forward", "the", "state-of-the-art", "performance", "significantly", "by", "2~4", "%", "improvement", "on", "the", "miniRCV1", "and", "ODIC", "datasets", ".", "Detailed", "analysis", "is", "further", "performed", "to", "show", "how", "the", "proposed", "network", "achieves", "the", "new", "performance", "."], "entities": [{"type": "Operation", "start": 3, "end": 10, "text": "Dynamic Memory Induction Networks (DMIN)", "sent_idx": 0}, {"type": "Effect", "start": 71, "end": 72, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--102"}
{"text": "Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response.", "tokens": ["Human", "conversations", "contain", "many", "types", "of", "information", ",", "e.g.", ",", "knowledge", ",", "common", "sense", ",", "and", "language", "habits", ".", "In", "this", "paper", ",", "we", "propose", "a", "conversational", "word", "embedding", "method", "named", "PR-Embedding", ",", "which", "utilizes", "the", "conversation", "pairs", "<", "post", ",", "reply", ">", "to", "learn", "word", "embedding", ".", "Different", "from", "previous", "works", ",", "PR-Embedding", "uses", "the", "vectors", "from", "two", "different", "semantic", "spaces", "to", "represent", "the", "words", "in", "post", "and", "reply", ".", "To", "catch", "the", "information", "among", "the", "pair", ",", "we", "first", "introduce", "the", "word", "alignment", "model", "from", "statistical", "machine", "translation", "to", "generate", "the", "cross-sentence", "window", ",", "then", "train", "the", "embedding", "on", "word-level", "and", "sentence-level", ".", "We", "evaluate", "the", "method", "on", "single-turn", "and", "multi-turn", "response", "selection", "tasks", "for", "retrieval-based", "dialog", "systems", ".", "The", "experiment", "results", "show", "that", "PR-Embedding", "can", "improve", "the", "quality", "of", "the", "selected", "response", "."], "entities": [{"type": "Operation", "start": 126, "end": 127, "text": "PR-Embedding", "sent_idx": 5}, {"type": "Effect", "start": 130, "end": 135, "text": "quality of the selected response", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--127"}
{"text": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.", "tokens": ["Hypernymy", "detection", ",", "a.k.a", ",", "lexical", "entailment", ",", "is", "a", "fundamental", "sub-task", "of", "many", "natural", "language", "understanding", "tasks", ".", "Previous", "explorations", "mostly", "focus", "on", "monolingual", "hypernymy", "detection", "on", "high-resource", "languages", ",", "e.g.", ",", "English", ",", "but", "few", "investigate", "the", "low-resource", "scenarios", ".", "This", "paper", "addresses", "the", "problem", "of", "low-resource", "hypernymy", "detection", "by", "combining", "high-resource", "languages", ".", "We", "extensively", "compare", "three", "joint", "training", "paradigms", "and", "for", "the", "first", "time", "propose", "applying", "meta", "learning", "to", "relieve", "the", "low-resource", "issue", ".", "Experiments", "demonstrate", "the", "superiority", "of", "our", "method", "among", "the", "three", "settings", ",", "which", "substantially", "improves", "the", "performance", "of", "extremely", "low-resource", "languages", "by", "preventing", "over-fitting", "on", "small", "datasets", "."], "entities": [{"type": "Operation", "start": 69, "end": 72, "text": "applying meta learning", "sent_idx": 3}, {"type": "Effect", "start": 94, "end": 95, "text": "performance", "sent_idx": 4}, {"type": "Effect", "start": 101, "end": 102, "text": "over-fitting", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--336"}
{"text": "The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCG-bank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.", "tokens": ["The", "definition", "of", "combinatory", "categorial", "grammar", "(", "CCG", ")", "in", "the", "literature", "varies", "quite", "a", "bit", "from", "author", "to", "author", ".", "However", ",", "the", "differences", "between", "the", "definitions", "are", "important", "in", "terms", "of", "the", "language", "classes", "of", "each", "CCG", ".", "We", "prove", "that", "a", "wide", "range", "of", "CCGs", "are", "strongly", "context-free", ",", "including", "the", "CCG", "of", "CCG-bank", "and", "of", "the", "parser", "of", "Clark", "and", "Curran", "(", "2007", ")", ".", "In", "light", "of", "these", "new", "results", ",", "we", "train", "the", "PCFG", "parser", "of", "Petrov", "and", "Klein", "(", "2007", ")", "on", "CCGbank", "and", "achieve", "state", "of", "the", "art", "results", "in", "supertagging", "accuracy", ",", "PARSEVAL", "measures", "and", "dependency", "accuracy", "."], "entities": [{"type": "Operation", "start": 79, "end": 81, "text": "PCFG parser", "sent_idx": 3}, {"type": "Effect", "start": 98, "end": 100, "text": "supertagging accuracy", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 103, "text": "PARSEVAL measures", "sent_idx": 3}, {"type": "Effect", "start": 104, "end": 106, "text": "dependency accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "P10-1035"}
{"text": "Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.", "tokens": ["Knowledge", "graph", "(", "KG", ")", "embeddings", "learn", "low-", "dimensional", "representations", "of", "entities", "and", "relations", "to", "predict", "missing", "facts", ".", "KGs", "often", "exhibit", "hierarchical", "and", "logical", "patterns", "which", "must", "be", "preserved", "in", "the", "embedding", "space", ".", "For", "hierarchical", "data", ",", "hyperbolic", "embedding", "methods", "have", "shown", "promise", "for", "high-fidelity", "and", "parsimonious", "representations", ".", "However", ",", "existing", "hyperbolic", "embedding", "methods", "do", "not", "account", "for", "the", "rich", "logical", "patterns", "in", "KGs", ".", "In", "this", "work", ",", "we", "introduce", "a", "class", "of", "hyperbolic", "KG", "embedding", "models", "that", "simultaneously", "capture", "hierarchical", "and", "logical", "patterns", ".", "Our", "approach", "combines", "hyperbolic", "reflections", "and", "rotations", "with", "attention", "to", "model", "complex", "relational", "patterns", ".", "Experimental", "results", "on", "standard", "KG", "benchmarks", "show", "that", "our", "method", "improves", "over", "previous", "Euclidean-", "and", "hyperbolic-based", "efforts", "by", "up", "to", "6.1", "%", "in", "mean", "reciprocal", "rank", "(", "MRR", ")", "in", "low", "dimensions", ".", "Furthermore", ",", "we", "observe", "that", "different", "geometric", "transformations", "capture", "different", "types", "of", "relations", "while", "attention-", "based", "transformations", "generalize", "to", "multiple", "relations", ".", "In", "high", "dimensions", ",", "our", "approach", "yields", "new", "state-of-the-art", "MRRs", "of", "49.6", "%", "on", "WN18RR", "and", "57.7", "%", "on", "YAGO3", "-", "10", "."], "entities": [{"type": "Operation", "start": 74, "end": 81, "text": "a class of hyperbolic KG embedding models", "sent_idx": 4}, {"type": "Effect", "start": 127, "end": 133, "text": "mean reciprocal rank (MRR)", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--617"}
{"text": "We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 Bleu score increase over a state-of-the-art syntax-based machine translation system.", "tokens": ["We", "present", "a", "simple", "yet", "powerful", "hierarchical", "search", "algorithm", "for", "automatic", "word", "alignment", ".", "Our", "algorithm", "induces", "a", "forest", "of", "alignments", "from", "which", "we", "can", "efficiently", "extract", "a", "ranked", "k-best", "list", ".", "We", "score", "a", "given", "alignment", "within", "the", "forest", "with", "a", "flexible", ",", "linear", "discriminative", "model", "incorporating", "hundreds", "of", "features", ",", "and", "trained", "on", "a", "relatively", "small", "amount", "of", "annotated", "data", ".", "We", "report", "results", "on", "Arabic-English", "word", "alignment", "and", "translation", "tasks", ".", "Our", "model", "outperforms", "a", "GIZA++", "Model-4", "baseline", "by", "6.3", "points", "in", "F-measure", ",", "yielding", "a", "1.1", "Bleu", "score", "increase", "over", "a", "state-of-the-art", "syntax-based", "machine", "translation", "system", "."], "entities": [{"type": "Operation", "start": 6, "end": 9, "text": "hierarchical search algorithm", "sent_idx": 0}, {"type": "Effect", "start": 85, "end": 86, "text": "F-measure", "sent_idx": 4}, {"type": "Effect", "start": 90, "end": 91, "text": "Bleu", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P10-1017"}
{"text": "The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations. Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints. Experimental results show that our model achieves the best performance on two public datasets of different structures.", "tokens": ["The", "goal", "of", "argumentation", "mining", "is", "to", "automatically", "extract", "argumentation", "structures", "from", "argumentative", "texts", ".", "Most", "existing", "methods", "determine", "argumentative", "relations", "by", "exhaustively", "enumerating", "all", "possible", "pairs", "of", "argument", "components", ",", "which", "suffer", "from", "low", "efficiency", "and", "class", "imbalance", ".", "Moreover", ",", "due", "to", "the", "complex", "nature", "of", "argumentation", ",", "there", "is", ",", "so", "far", ",", "no", "universal", "method", "that", "can", "address", "both", "tree", "and", "non-tree", "structured", "argumentation", ".", "Towards", "these", "issues", ",", "we", "propose", "a", "neural", "transition-based", "model", "for", "argumentation", "mining", ",", "which", "incrementally", "builds", "an", "argumentation", "graph", "by", "generating", "a", "sequence", "of", "actions", ",", "avoiding", "inefficient", "enumeration", "operations", ".", "Furthermore", ",", "our", "model", "can", "handle", "both", "tree", "and", "non-tree", "structured", "argumentation", "without", "introducing", "any", "structural", "constraints", ".", "Experimental", "results", "show", "that", "our", "model", "achieves", "the", "best", "performance", "on", "two", "public", "datasets", "of", "different", "structures", "."], "entities": [{"type": "Operation", "start": 76, "end": 79, "text": "neural transition-based model", "sent_idx": 3}, {"type": "Effect", "start": 128, "end": 129, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--497"}
{"text": "In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations. However, existing approaches in NLP mainly focus on \u201cWHY A\u201d rather than contrastive \u201cWHY A NOT B\u201d, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields.In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novel K nowledge- A ware C ontrastive E xplanation generation framework ( KACE ).Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples. After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations. Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones. Moreover, we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9% on SNLI, gaining improvements of 5.7% against ETPA (\u201cExplain-Then-Predict-Attention\u201d) and 0.6% against NILE (\u201cWHY A\u201d).", "tokens": ["In", "order", "to", "better", "understand", "the", "reason", "behind", "model", "behaviors", "(", "i.e.", ",", "making", "predictions", ")", ",", "most", "recent", "works", "have", "exploited", "generative", "models", "to", "provide", "complementary", "explanations", ".", "However", ",", "existing", "approaches", "in", "NLP", "mainly", "focus", "on", "\u201c", "WHY", "A", "\u201d", "rather", "than", "contrastive", "\u201c", "WHY", "A", "NOT", "B", "\u201d", ",", "which", "is", "shown", "to", "be", "able", "to", "better", "distinguish", "confusing", "candidates", "and", "improve", "data", "efficiency", "in", "other", "research", "fields", ".", "In", "this", "paper", ",", "we", "focus", "on", "generating", "contrastive", "explanations", "with", "counterfactual", "examples", "in", "NLI", "and", "propose", "a", "novel", "K", "nowledge-", "A", "ware", "C", "ontrastive", "E", "xplanation", "generation", "framework", "(", "KACE", ")", ".Specifically", ",", "we", "first", "identify", "rationales", "(", "i.e.", ",", "key", "phrases", ")", "from", "input", "sentences", ",", "and", "use", "them", "as", "key", "perturbations", "for", "generating", "counterfactual", "examples", ".", "After", "obtaining", "qualified", "counterfactual", "examples", ",", "we", "take", "them", "along", "with", "original", "examples", "and", "external", "knowledge", "as", "input", ",", "and", "employ", "a", "knowledge-aware", "generative", "pre-trained", "language", "model", "to", "generate", "contrastive", "explanations", ".", "Experimental", "results", "show", "that", "contrastive", "explanations", "are", "beneficial", "to", "fit", "the", "scenarios", "by", "clarifying", "the", "difference", "between", "the", "predicted", "answer", "and", "other", "possible", "wrong", "ones", ".", "Moreover", ",", "we", "train", "an", "NLI", "model", "enhanced", "with", "contrastive", "explanations", "and", "achieves", "an", "accuracy", "of", "91.9", "%", "on", "SNLI", ",", "gaining", "improvements", "of", "5.7", "%", "against", "ETPA", "(", "\u201c", "Explain-Then-Predict-Attention", "\u201d", ")", "and", "0.6", "%", "against", "NILE", "(", "\u201c", "WHY", "A", "\u201d", ")", "."], "entities": [{"type": "Operation", "start": 194, "end": 200, "text": "NLI model enhanced with contrastive explanations", "sent_idx": 5}, {"type": "Effect", "start": 203, "end": 204, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--196"}
{"text": "Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word. This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies. To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language. We generate data from a finite state transducer to train an encoder-decoder model. We improve the model by \u201challucinating\u201d missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes. The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the FST baseline. This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources.", "tokens": ["Polysynthetic", "languages", "have", "exceptionally", "large", "and", "sparse", "vocabularies", ",", "thanks", "to", "the", "number", "of", "morpheme", "slots", "and", "combinations", "in", "a", "word", ".", "This", "complexity", ",", "together", "with", "a", "general", "scarcity", "of", "written", "data", ",", "poses", "a", "challenge", "to", "the", "development", "of", "natural", "language", "technologies", ".", "To", "address", "this", "challenge", ",", "we", "offer", "linguistically-informed", "approaches", "for", "bootstrapping", "a", "neural", "morphological", "analyzer", ",", "and", "demonstrate", "its", "application", "to", "Kunwinjku", ",", "a", "polysynthetic", "Australian", "language", ".", "We", "generate", "data", "from", "a", "finite", "state", "transducer", "to", "train", "an", "encoder-decoder", "model", ".", "We", "improve", "the", "model", "by", "\u201c", "hallucinating", "\u201d", "missing", "linguistic", "structure", "into", "the", "training", "data", ",", "and", "by", "resampling", "from", "a", "Zipf", "distribution", "to", "simulate", "a", "more", "natural", "distribution", "of", "morphemes", ".", "The", "best", "model", "accounts", "for", "all", "instances", "of", "reduplication", "in", "the", "test", "set", "and", "achieves", "an", "accuracy", "of", "94.7", "%", "overall", ",", "a", "10", "percentage", "point", "improvement", "over", "the", "FST", "baseline", ".", "This", "process", "demonstrates", "the", "feasibility", "of", "bootstrapping", "a", "neural", "morph", "analyzer", "from", "minimal", "resources", "."], "entities": [{"type": "Operation", "start": 84, "end": 86, "text": "encoder-decoder model", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 136, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--594"}
{"text": "We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1 to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.", "tokens": ["We", "present", "a", "first", "known", "result", "of", "high", "precision", "rare", "word", "bilingual", "extraction", "from", "comparable", "corpora", ",", "using", "aligned", "comparable", "documents", "and", "supervised", "classification", ".", "We", "incorporate", "two", "features", ",", "a", "context-vector", "similarity", "and", "a", "co-occurrence", "model", "between", "words", "in", "aligned", "documents", "in", "a", "machine", "learning", "approach", ".", "We", "test", "our", "hypothesis", "on", "different", "pairs", "of", "languages", "and", "corpora", ".", "We", "obtain", "very", "high", "F-Measure", "between", "80", "%", "and", "98", "%", "for", "recognizing", "and", "extracting", "correct", "translations", "for", "rare", "terms", "(", "from", "1", "to", "5", "occurrences", ")", ".", "Moreover", ",", "we", "show", "that", "our", "system", "can", "be", "trained", "on", "a", "pair", "of", "languages", "and", "test", "on", "a", "different", "pair", "of", "languages", ",", "obtaining", "a", "F-Measure", "of", "77", "%", "for", "the", "classification", "of", "Chinese-English", "translations", "using", "a", "training", "corpus", "of", "Spanish-French", ".", "Our", "method", "is", "therefore", "even", "potentially", "applicable", "to", "low", "resources", "languages", "without", "training", "data", "."], "entities": [{"type": "Operation", "start": 17, "end": 24, "text": "using aligned comparable documents and supervised classification", "sent_idx": 0}, {"type": "Effect", "start": 64, "end": 65, "text": "F-Measure", "sent_idx": 3}, {"type": "Effect", "start": 114, "end": 115, "text": "F-Measure", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P11-1133"}
{"text": "We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.", "tokens": ["We", "study", "unsupervised", "multi-document", "summarization", "evaluation", "metrics", ",", "which", "require", "neither", "human-written", "reference", "summaries", "nor", "human", "annotations", "(", "e.g.", "preferences", ",", "ratings", ",", "etc", ".", ")", ".", "We", "propose", "SUPERT", ",", "which", "rates", "the", "quality", "of", "a", "summary", "by", "measuring", "its", "semantic", "similarity", "with", "a", "pseudo", "reference", "summary", ",", "i.e.", "selected", "salient", "sentences", "from", "the", "source", "documents", ",", "using", "contextualized", "embeddings", "and", "soft", "token", "alignment", "techniques", ".", "Compared", "to", "the", "state-of-the-art", "unsupervised", "evaluation", "metrics", ",", "SUPERT", "correlates", "better", "with", "human", "ratings", "by", "18-", "39", "%", ".", "Furthermore", ",", "we", "use", "SUPERT", "as", "rewards", "to", "guide", "a", "neural-based", "reinforcement", "learning", "summarizer", ",", "yielding", "favorable", "performance", "compared", "to", "the", "state-of-the-art", "unsupervised", "summarizers", ".", "All", "source", "code", "is", "available", "at", "https://github.com/yg211/acl20-ref-free-eval", "."], "entities": [{"type": "Operation", "start": 89, "end": 100, "text": "use SUPERT as rewards to guide a neural-based reinforcement learning summarizer", "sent_idx": 3}, {"type": "Effect", "start": 103, "end": 104, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--124"}
{"text": "Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models\u2019 top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.", "tokens": ["Advanced", "machine", "learning", "techniques", "have", "boosted", "the", "performance", "of", "natural", "language", "processing", ".", "Nevertheless", ",", "recent", "studies", ",", "e.g.", ",", "(", "CITATION", ")", "show", "that", "these", "techniques", "inadvertently", "capture", "the", "societal", "bias", "hidden", "in", "the", "corpus", "and", "further", "amplify", "it", ".", "However", ",", "their", "analysis", "is", "conducted", "only", "on", "models", "\u2019", "top", "predictions", ".", "In", "this", "paper", ",", "we", "investigate", "the", "gender", "bias", "amplification", "issue", "from", "the", "distribution", "perspective", "and", "demonstrate", "that", "the", "bias", "is", "amplified", "in", "the", "view", "of", "predicted", "probability", "distribution", "over", "labels", ".", "We", "further", "propose", "a", "bias", "mitigation", "approach", "based", "on", "posterior", "regularization", ".", "With", "little", "performance", "loss", ",", "our", "method", "can", "almost", "remove", "the", "bias", "amplification", "in", "the", "distribution", ".", "Our", "study", "sheds", "the", "light", "on", "understanding", "the", "bias", "amplification", "."], "entities": [{"type": "Operation", "start": 90, "end": 97, "text": "bias mitigation approach based on posterior regularization", "sent_idx": 4}, {"type": "Effect", "start": 100, "end": 101, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--264"}
{"text": "Visual Dialogue involves \u201cunderstanding\u201d the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don\u2019t, achieving state-of-the-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63%.", "tokens": ["Visual", "Dialogue", "involves", "\u201c", "understanding", "\u201d", "the", "dialogue", "history", "(", "what", "has", "been", "discussed", "previously", ")", "and", "the", "current", "question", "(", "what", "is", "asked", ")", ",", "in", "addition", "to", "grounding", "information", "in", "the", "image", ",", "to", "accurately", "generate", "the", "correct", "response", ".", "In", "this", "paper", ",", "we", "show", "that", "co-attention", "models", "which", "explicitly", "encode", "dialoh", "history", "outperform", "models", "that", "do", "n\u2019t", ",", "achieving", "state-of-the-art", "performance", "(", "72", "%", "NDCG", "on", "val", "set", ")", ".", "However", ",", "we", "also", "expose", "shortcomings", "of", "the", "crowdsourcing", "dataset", "collection", "procedure", ",", "by", "showing", "that", "dialogue", "history", "is", "indeed", "only", "required", "for", "a", "small", "amount", "of", "the", "data", ",", "and", "that", "the", "current", "evaluation", "metric", "encourages", "generic", "replies", ".", "To", "that", "end", ",", "we", "propose", "a", "challenging", "subset", "(", "VisdialConv", ")", "of", "the", "VisdialVal", "set", "and", "the", "benchmark", "NDCG", "of", "63", "%", "."], "entities": [{"type": "Operation", "start": 49, "end": 51, "text": "co-attention models", "sent_idx": 1}, {"type": "Effect", "start": 68, "end": 69, "text": "NDCG", "sent_idx": 1}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--728"}
{"text": "Transformer-based QA models use input-wide self-attention \u2013 i.e. across both the question and the input passage \u2013 at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.", "tokens": ["Transformer-based", "QA", "models", "use", "input-wide", "self-attention", "\u2013", "i.e.", "across", "both", "the", "question", "and", "the", "input", "passage", "\u2013", "at", "all", "layers", ",", "causing", "them", "to", "be", "slow", "and", "memory-intensive", ".", "It", "turns", "out", "that", "we", "can", "get", "by", "without", "input-wide", "self-attention", "at", "all", "layers", ",", "especially", "in", "the", "lower", "layers", ".", "We", "introduce", "DeFormer", ",", "a", "decomposed", "transformer", ",", "which", "substitutes", "the", "full", "self-attention", "with", "question-wide", "and", "passage-wide", "self-attentions", "in", "the", "lower", "layers", ".", "This", "allows", "for", "question-independent", "processing", "of", "the", "input", "text", "representations", ",", "which", "in", "turn", "enables", "pre-computing", "passage", "representations", "reducing", "runtime", "compute", "drastically", ".", "Furthermore", ",", "because", "DeFormer", "is", "largely", "similar", "to", "the", "original", "model", ",", "we", "can", "initialize", "DeFormer", "with", "the", "pre-training", "weights", "of", "a", "standard", "transformer", ",", "and", "directly", "fine-tune", "on", "the", "target", "QA", "dataset", ".", "We", "show", "DeFormer", "versions", "of", "BERT", "and", "XLNet", "can", "be", "used", "to", "speed", "up", "QA", "by", "over", "4.3x", "and", "with", "simple", "distillation-based", "losses", "they", "incur", "only", "a", "1", "%", "drop", "in", "accuracy", ".", "We", "open", "source", "the", "code", "at", "https://github.com/StonyBrookNLP/deformer", "."], "entities": [{"type": "Operation", "start": 132, "end": 138, "text": "DeFormer versions of BERT and XLNet", "sent_idx": 5}, {"type": "Effect", "start": 142, "end": 143, "text": "speed", "sent_idx": 5}, {"type": "Effect", "start": 161, "end": 162, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--411"}
{"text": "Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.", "tokens": ["Marking", "up", "search", "queries", "with", "linguistic", "annotations", "such", "as", "part-of-speech", "tags", ",", "capitalization", ",", "and", "segmentation", ",", "is", "an", "important", "part", "of", "query", "processing", "and", "understanding", "in", "information", "retrieval", "systems", ".", "Due", "to", "their", "brevity", "and", "idiosyncratic", "structure", ",", "search", "queries", "pose", "a", "challenge", "to", "existing", "NLP", "tools", ".", "To", "address", "this", "challenge", ",", "we", "propose", "a", "probabilistic", "approach", "for", "performing", "joint", "query", "annotation", ".", "First", ",", "we", "derive", "a", "robust", "set", "of", "unsupervised", "independent", "annotations", ",", "using", "queries", "and", "pseudo-relevance", "feedback", ".", "Then", ",", "we", "stack", "additional", "classifiers", "on", "the", "independent", "annotations", ",", "and", "exploit", "the", "dependencies", "between", "them", "to", "further", "improve", "the", "accuracy", ",", "even", "with", "a", "very", "limited", "amount", "of", "available", "training", "data", ".", "We", "evaluate", "our", "method", "using", "a", "range", "of", "queries", "extracted", "from", "a", "web", "search", "log", ".", "Experimental", "results", "verify", "the", "effectiveness", "of", "our", "approach", "for", "both", "short", "keyword", "queries", ",", "and", "verbose", "natural", "language", "queries", "."], "entities": [{"type": "Operation", "start": 86, "end": 93, "text": "stack additional classifiers on the independent annotations", "sent_idx": 4}, {"type": "Effect", "start": 104, "end": 105, "text": "accuracy", "sent_idx": 4}, {"type": "Operation", "start": 95, "end": 100, "text": "exploit the dependencies between them", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P11-1011"}
{"text": "Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.", "tokens": ["Question", "Answering", "(", "QA", ")", "is", "in", "increasing", "demand", "as", "the", "amount", "of", "information", "available", "online", "and", "the", "desire", "for", "quick", "access", "to", "this", "content", "grows", ".", "A", "common", "approach", "to", "QA", "has", "been", "to", "fine-tune", "a", "pretrained", "language", "model", "on", "a", "task-specific", "labeled", "dataset", ".", "This", "paradigm", ",", "however", ",", "relies", "on", "scarce", ",", "and", "costly", "to", "obtain", ",", "large-scale", "human-labeled", "data", ".", "We", "propose", "an", "unsupervised", "approach", "to", "training", "QA", "models", "with", "generated", "pseudo-training", "data", ".", "We", "show", "that", "generating", "questions", "for", "QA", "training", "by", "applying", "a", "simple", "template", "on", "a", "related", ",", "retrieved", "sentence", "rather", "than", "the", "original", "context", "sentence", "improves", "downstream", "QA", "performance", "by", "allowing", "the", "model", "to", "learn", "more", "complex", "context-question", "relationships", ".", "Training", "a", "QA", "model", "on", "this", "data", "gives", "a", "relative", "improvement", "over", "a", "previous", "unsupervised", "model", "in", "F1", "score", "on", "the", "SQuAD", "dataset", "by", "about", "14", "%", ",", "and", "20", "%", "when", "the", "answer", "is", "a", "named", "entity", ",", "achieving", "state-of-the-art", "performance", "on", "SQuAD", "for", "unsupervised", "QA", "."], "entities": [{"type": "Operation", "start": 67, "end": 77, "text": "unsupervised approach to training QA models with generated pseudo-training data", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 137, "text": "F1 score", "sent_idx": 5}, {"type": "Effect", "start": 159, "end": 160, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--413"}
{"text": "Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of KPA substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.", "tokens": ["Previous", "work", "on", "review", "summarization", "focused", "on", "measuring", "the", "sentiment", "toward", "the", "main", "aspects", "of", "the", "reviewed", "product", "or", "business", ",", "or", "on", "creating", "a", "textual", "summary", ".", "These", "approaches", "provide", "only", "a", "partial", "view", "of", "the", "data", ":", "aspect-based", "sentiment", "summaries", "lack", "sufficient", "explanation", "or", "justification", "for", "the", "aspect", "rating", ",", "while", "textual", "summaries", "do", "not", "quantify", "the", "significance", "of", "each", "element", ",", "and", "are", "not", "well-suited", "for", "representing", "conflicting", "views", ".", "Recently", ",", "Key", "Point", "Analysis", "(", "KPA", ")", "has", "been", "proposed", "as", "a", "summarization", "framework", "that", "provides", "both", "textual", "and", "quantitative", "summary", "of", "the", "main", "points", "in", "the", "data", ".", "We", "adapt", "KPA", "to", "review", "data", "by", "introducing", "Collective", "Key", "Point", "Mining", "for", "better", "key", "point", "extraction", ";", "integrating", "sentiment", "analysis", "into", "KPA", ";", "identifying", "good", "key", "point", "candidates", "for", "review", "summaries", ";", "and", "leveraging", "the", "massive", "amount", "of", "available", "reviews", "and", "their", "metadata", ".", "We", "show", "empirically", "that", "these", "novel", "extensions", "of", "KPA", "substantially", "improve", "its", "performance", ".", "We", "demonstrate", "that", "promising", "results", "can", "be", "achieved", "without", "any", "domain-specific", "annotation", ",", "while", "human", "supervision", "can", "lead", "to", "further", "improvement", "."], "entities": [{"type": "Operation", "start": 154, "end": 157, "text": "extensions of KPA", "sent_idx": 4}, {"type": "Effect", "start": 160, "end": 161, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--262"}
{"text": "Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts.", "tokens": ["Tree-to-string", "systems", "(", "and", "their", "forest-based", "extensions", ")", "have", "gained", "steady", "popularity", "thanks", "to", "their", "simplicity", "and", "efficiency", ",", "but", "there", "is", "a", "major", "limitation", ":", "they", "are", "unable", "to", "guarantee", "the", "grammaticality", "of", "the", "output", ",", "which", "is", "explicitly", "modeled", "in", "string-to-tree", "systems", "via", "target-side", "syntax", ".", "We", "thus", "propose", "to", "combine", "the", "advantages", "of", "both", ",", "and", "present", "a", "novel", "constituency-to-dependency", "translation", "model", ",", "which", "uses", "constituency", "forests", "on", "the", "source", "side", "to", "direct", "the", "translation", ",", "and", "dependency", "trees", "on", "the", "target", "side", "(", "as", "a", "language", "model", ")", "to", "ensure", "grammaticality", ".", "Medium-scale", "experiments", "show", "an", "absolute", "and", "statistically", "significant", "improvement", "of", "+", "0.7", "BLEU", "points", "over", "a", "state-of-the-art", "forest-based", "tree-to-string", "system", "even", "with", "fewer", "rules", ".", "This", "is", "also", "the", "first", "time", "that", "a", "tree-to-tree", "model", "can", "surpass", "tree-to-string", "counterparts", "."], "entities": [{"type": "Operation", "start": 62, "end": 65, "text": "constituency-to-dependency translation model", "sent_idx": 1}, {"type": "Effect", "start": 108, "end": 110, "text": "BLEU points", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1145"}
{"text": "In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.", "tokens": ["In", "recent", "years", ",", "we", "have", "seen", "a", "colossal", "effort", "in", "pre-training", "multilingual", "text", "encoders", "using", "large-scale", "corpora", "in", "many", "languages", "to", "facilitate", "cross-lingual", "transfer", "learning", ".", "However", ",", "due", "to", "typological", "differences", "across", "languages", ",", "the", "cross-lingual", "transfer", "is", "challenging", ".", "Nevertheless", ",", "language", "syntax", ",", "e.g.", ",", "syntactic", "dependencies", ",", "can", "bridge", "the", "typological", "gap", ".", "Previous", "works", "have", "shown", "that", "pre-trained", "multilingual", "encoders", ",", "such", "as", "mBERT", "(", "CITATION", ")", ",", "capture", "language", "syntax", ",", "helping", "cross-lingual", "transfer", ".", "This", "work", "shows", "that", "explicitly", "providing", "language", "syntax", "and", "training", "mBERT", "using", "an", "auxiliary", "objective", "to", "encode", "the", "universal", "dependency", "tree", "structure", "helps", "cross-lingual", "transfer", ".", "We", "perform", "rigorous", "experiments", "on", "four", "NLP", "tasks", ",", "including", "text", "classification", ",", "question", "answering", ",", "named", "entity", "recognition", ",", "and", "task-oriented", "semantic", "parsing", ".", "The", "experiment", "results", "show", "that", "syntax-augmented", "mBERT", "improves", "cross-lingual", "transfer", "on", "popular", "benchmarks", ",", "such", "as", "PAWS-X", "and", "MLQA", ",", "by", "1.4", "and", "1.6", "points", "on", "average", "across", "all", "languages", ".", "In", "the", "generalized", "transfer", "setting", ",", "the", "performance", "boosted", "significantly", ",", "with", "3.9", "and", "3.1", "points", "on", "average", "in", "PAWS-X", "and", "MLQA", "."], "entities": [{"type": "Operation", "start": 138, "end": 140, "text": "syntax-augmented mBERT", "sent_idx": 6}, {"type": "Effect", "start": 171, "end": 172, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--350"}
{"text": "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still under-performs compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.", "tokens": ["One", "of", "the", "main", "obstacles", "to", "producing", "high", "quality", "joint", "models", "is", "the", "lack", "of", "jointly", "annotated", "data", ".", "Joint", "modeling", "of", "multiple", "natural", "language", "processing", "tasks", "outperforms", "single-task", "models", "learned", "from", "the", "same", "data", ",", "but", "still", "under-performs", "compared", "to", "single-task", "models", "learned", "on", "the", "more", "abundant", "quantities", "of", "available", "single-task", "annotated", "data", ".", "In", "this", "paper", "we", "present", "a", "novel", "model", "which", "makes", "use", "of", "additional", "single-task", "annotated", "data", "to", "improve", "the", "performance", "of", "a", "joint", "model", ".", "Our", "model", "utilizes", "a", "hierarchical", "prior", "to", "link", "the", "feature", "weights", "for", "shared", "features", "in", "several", "single-task", "models", "and", "the", "joint", "model", ".", "Experiments", "on", "joint", "parsing", "and", "named", "entity", "recognition", ",", "using", "the", "OntoNotes", "corpus", ",", "show", "that", "our", "hierarchical", "joint", "model", "can", "produce", "substantial", "gains", "over", "a", "joint", "model", "trained", "on", "only", "the", "jointly", "annotated", "data", "."], "entities": [{"type": "Operation", "start": 64, "end": 71, "text": "makes use of additional single-task annotated data", "sent_idx": 2}, {"type": "Effect", "start": 74, "end": 75, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1074"}
{"text": "With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., \u201cgay\u201d, \u201cblack\u201d) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like \u201cShe makes me happy to be gay\u201d as abusive simply because of the word \u201cgay.\u201d In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models\u2019 generalization ability.", "tokens": ["With", "the", "recent", "proliferation", "of", "the", "use", "of", "text", "classifications", ",", "researchers", "have", "found", "that", "there", "are", "certain", "unintended", "biases", "in", "text", "classification", "datasets", ".", "For", "example", ",", "texts", "containing", "some", "demographic", "identity-terms", "(", "e.g.", ",", "\u201c", "gay", "\u201d", ",", "\u201c", "black", "\u201d", ")", "are", "more", "likely", "to", "be", "abusive", "in", "existing", "abusive", "language", "detection", "datasets", ".", "As", "a", "result", ",", "models", "trained", "with", "these", "datasets", "may", "consider", "sentences", "like", "\u201c", "She", "makes", "me", "happy", "to", "be", "gay", "\u201d", "as", "abusive", "simply", "because", "of", "the", "word", "\u201c", "gay", ".", "\u201d", "In", "this", "paper", ",", "we", "formalize", "the", "unintended", "biases", "in", "text", "classification", "datasets", "as", "a", "kind", "of", "selection", "bias", "from", "the", "non-discrimination", "distribution", "to", "the", "discrimination", "distribution", ".", "Based", "on", "this", "formalization", ",", "we", "further", "propose", "a", "model-agnostic", "debiasing", "training", "framework", "by", "recovering", "the", "non-discrimination", "distribution", "using", "instance", "weighting", ",", "which", "does", "not", "require", "any", "extra", "resources", "or", "annotations", "apart", "from", "a", "pre-defined", "set", "of", "demographic", "identity-terms", ".", "Experiments", "demonstrate", "that", "our", "method", "can", "effectively", "alleviate", "the", "impacts", "of", "the", "unintended", "biases", "without", "significantly", "hurting", "models", "\u2019", "generalization", "ability", "."], "entities": [{"type": "Operation", "start": 127, "end": 131, "text": "model-agnostic debiasing training framework", "sent_idx": 3}, {"type": "Effect", "start": 177, "end": 178, "text": "generalization", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--380"}
{"text": "We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash_dataset.", "tokens": ["We", "study", "the", "task", "of", "semantic", "parse", "correction", "with", "natural", "language", "feedback", ".", "Given", "a", "natural", "language", "utterance", ",", "most", "semantic", "parsing", "systems", "pose", "the", "problem", "as", "one-shot", "translation", "where", "the", "utterance", "is", "mapped", "to", "a", "corresponding", "logical", "form", ".", "In", "this", "paper", ",", "we", "investigate", "a", "more", "interactive", "scenario", "where", "humans", "can", "further", "interact", "with", "the", "system", "by", "providing", "free-form", "natural", "language", "feedback", "to", "correct", "the", "system", "when", "it", "generates", "an", "inaccurate", "interpretation", "of", "an", "initial", "utterance", ".", "We", "focus", "on", "natural", "language", "to", "SQL", "systems", "and", "construct", ",", "SPLASH", ",", "a", "dataset", "of", "utterances", ",", "incorrect", "SQL", "interpretations", "and", "the", "corresponding", "natural", "language", "feedback", ".", "We", "compare", "various", "reference", "models", "for", "the", "correction", "task", "and", "show", "that", "incorporating", "such", "a", "rich", "form", "of", "feedback", "can", "significantly", "improve", "the", "overall", "semantic", "parsing", "accuracy", "while", "retaining", "the", "flexibility", "of", "natural", "language", "interaction", ".", "While", "we", "estimated", "human", "correction", "accuracy", "is", "81.5", "%", ",", "our", "best", "model", "achieves", "only", "25.1", "%", ",", "which", "leaves", "a", "large", "gap", "for", "improvement", "in", "future", "research", ".", "SPLASH", "is", "publicly", "available", "at", "https://aka.ms/Splash_dataset", "."], "entities": [{"type": "Operation", "start": 119, "end": 126, "text": "incorporating such a rich form of feedback", "sent_idx": 4}, {"type": "Effect", "start": 133, "end": 134, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--187"}
{"text": "Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.", "tokens": ["Twitter", "provides", "access", "to", "large", "volumes", "of", "data", "in", "real", "time", ",", "but", "is", "notoriously", "noisy", ",", "hampering", "its", "utility", "for", "NLP", ".", "In", "this", "paper", ",", "we", "target", "out-of-vocabulary", "words", "in", "short", "text", "messages", "and", "propose", "a", "method", "for", "identifying", "and", "normalising", "ill-formed", "words", ".", "Our", "method", "uses", "a", "classifier", "to", "detect", "ill-formed", "words", ",", "and", "generates", "correction", "candidates", "based", "on", "morphophonemic", "similarity", ".", "Both", "word", "similarity", "and", "context", "are", "then", "exploited", "to", "select", "the", "most", "probable", "correction", "candidate", "for", "the", "word", ".", "The", "proposed", "method", "does", "n't", "require", "any", "annotations", ",", "and", "achieves", "state-of-the-art", "performance", "over", "an", "SMS", "corpus", "and", "a", "novel", "dataset", "based", "on", "Twitter", "."], "entities": [{"type": "Operation", "start": 48, "end": 55, "text": "uses a classifier to detect ill-formed words", "sent_idx": 2}, {"type": "Effect", "start": 96, "end": 97, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 57, "end": 64, "text": "generates correction candidates based on morphophonemic similarity", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}, {"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1038"}
{"text": "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.", "tokens": ["Existing", "leading", "code", "comment", "generation", "approaches", "with", "the", "structure-to-sequence", "framework", "ignores", "the", "type", "information", "of", "the", "interpretation", "of", "the", "code", ",", "e.g.", ",", "operator", ",", "string", ",", "etc", ".", "However", ",", "introducing", "the", "type", "information", "into", "the", "existing", "framework", "is", "non-trivial", "due", "to", "the", "hierarchical", "dependence", "among", "the", "type", "information", ".", "In", "order", "to", "address", "the", "issues", "above", ",", "we", "propose", "a", "Type", "Auxiliary", "Guiding", "encoder-decoder", "framework", "for", "the", "code", "comment", "generation", "task", "which", "considers", "the", "source", "code", "as", "an", "N-ary", "tree", "with", "type", "information", "associated", "with", "each", "node", ".", "Specifically", ",", "our", "framework", "is", "featured", "with", "a", "Type-associated", "Encoder", "and", "a", "Type-restricted", "Decoder", "which", "enables", "adaptive", "summarization", "of", "the", "source", "code", ".", "We", "further", "propose", "a", "hierarchical", "reinforcement", "learning", "method", "to", "resolve", "the", "training", "difficulties", "of", "our", "proposed", "framework", ".", "Extensive", "evaluations", "demonstrate", "the", "state-of-the-art", "performance", "of", "our", "framework", "with", "both", "the", "auto-evaluated", "metrics", "and", "case", "studies", "."], "entities": [{"type": "Operation", "start": 62, "end": 67, "text": "Type Auxiliary Guiding encoder-decoder framework", "sent_idx": 2}, {"type": "Effect", "start": 136, "end": 137, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--27"}
{"text": "This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method.", "tokens": ["This", "paper", "presents", "a", "novel", "method", "for", "nested", "named", "entity", "recognition", ".", "As", "a", "layered", "method", ",", "our", "method", "extends", "the", "prior", "second-best", "path", "recognition", "method", "by", "explicitly", "excluding", "the", "influence", "of", "the", "best", "path", ".", "Our", "method", "maintains", "a", "set", "of", "hidden", "states", "at", "each", "time", "step", "and", "selectively", "leverages", "them", "to", "build", "a", "different", "potential", "function", "for", "recognition", "at", "each", "level", ".", "In", "addition", ",", "we", "demonstrate", "that", "recognizing", "innermost", "entities", "first", "results", "in", "better", "performance", "than", "the", "conventional", "outermost", "entities", "first", "scheme", ".", "We", "provide", "extensive", "experimental", "results", "on", "ACE2004", ",", "ACE2005", ",", "and", "GENIA", "datasets", "to", "show", "the", "effectiveness", "and", "efficiency", "of", "our", "proposed", "method", "."], "entities": [{"type": "Operation", "start": 70, "end": 74, "text": "recognizing innermost entities first", "sent_idx": 3}, {"type": "Effect", "start": 77, "end": 78, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--275"}
{"text": "Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs.", "tokens": ["Grapheme-to-phoneme", "conversion", "(", "G2P", ")", "of", "names", "is", "an", "important", "and", "challenging", "problem", ".", "The", "correct", "pronunciation", "of", "a", "name", "is", "often", "reflected", "in", "its", "transliterations", ",", "which", "are", "expressed", "within", "a", "different", "phonological", "inventory", ".", "We", "investigate", "the", "problem", "of", "using", "transliterations", "to", "correct", "errors", "produced", "by", "state-of-the-art", "G2P", "systems", ".", "We", "present", "a", "novel", "re-ranking", "approach", "that", "incorporates", "a", "variety", "of", "score", "and", "n-gram", "features", ",", "in", "order", "to", "leverage", "transliterations", "from", "multiple", "languages", ".", "Our", "experiments", "demonstrate", "significant", "accuracy", "improvements", "when", "re-ranking", "is", "applied", "to", "n-best", "lists", "generated", "by", "three", "different", "G2P", "programs", "."], "entities": [{"type": "Operation", "start": 84, "end": 85, "text": "re-ranking", "sent_idx": 4}, {"type": "Effect", "start": 81, "end": 82, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1041"}
{"text": "We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.", "tokens": ["We", "propose", "a", "novel", "manifold", "based", "geometric", "approach", "for", "learning", "unsupervised", "alignment", "of", "word", "embeddings", "between", "the", "source", "and", "the", "target", "languages", ".", "Our", "approach", "formulates", "the", "alignment", "learning", "problem", "as", "a", "domain", "adaptation", "problem", "over", "the", "manifold", "of", "doubly", "stochastic", "matrices", ".", "This", "viewpoint", "arises", "from", "the", "aim", "to", "align", "the", "second", "order", "information", "of", "the", "two", "language", "spaces", ".", "The", "rich", "geometry", "of", "the", "doubly", "stochastic", "manifold", "allows", "to", "employ", "efficient", "Riemannian", "conjugate", "gradient", "algorithm", "for", "the", "proposed", "formulation", ".", "Empirically", ",", "the", "proposed", "approach", "outperforms", "state-of-the-art", "optimal", "transport", "based", "approach", "on", "the", "bilingual", "lexicon", "induction", "task", "across", "several", "language", "pairs", ".", "The", "performance", "improvement", "is", "more", "significant", "for", "distant", "language", "pairs", "."], "entities": [{"type": "Operation", "start": 4, "end": 8, "text": "manifold based geometric approach", "sent_idx": 0}, {"type": "Effect", "start": 105, "end": 106, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--276"}
{"text": "Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.", "tokens": ["Hybrid", "data", "combining", "both", "tabular", "and", "textual", "content", "(", "e.g.", ",", "financial", "reports", ")", "are", "quite", "pervasive", "in", "the", "real", "world", ".", "However", ",", "Question", "Answering", "(", "QA", ")", "over", "such", "hybrid", "data", "is", "largely", "neglected", "in", "existing", "research", ".", "In", "this", "work", ",", "we", "extract", "samples", "from", "real", "financial", "reports", "to", "build", "a", "new", "large-scale", "QA", "dataset", "containing", "both", "Tabular", "And", "Textual", "data", ",", "named", "TAT-QA", ",", "where", "numerical", "reasoning", "is", "usually", "required", "to", "infer", "the", "answer", ",", "such", "as", "addition", ",", "subtraction", ",", "multiplication", ",", "division", ",", "counting", ",", "comparison/sorting", ",", "and", "the", "compositions", ".", "We", "further", "propose", "a", "novel", "QA", "model", "termed", "TAGOP", ",", "which", "is", "capable", "of", "reasoning", "over", "both", "tables", "and", "text", ".", "It", "adopts", "sequence", "tagging", "to", "extract", "relevant", "cells", "from", "the", "table", "along", "with", "relevant", "spans", "from", "the", "text", "to", "infer", "their", "semantics", ",", "and", "then", "applies", "symbolic", "reasoning", "over", "them", "with", "a", "set", "of", "aggregation", "operators", "to", "arrive", "at", "the", "final", "answer", ".", "TAGOP", "achieves", "58.0", "%", "inF1", ",", "which", "is", "an", "11.1", "%", "absolute", "increase", "over", "the", "previous", "best", "baseline", "model", ",", "according", "to", "our", "experiments", "on", "TAT-QA", ".", "But", "this", "result", "still", "lags", "far", "behind", "performance", "of", "expert", "human", ",", "i.e.90.8", "%", "in", "F1", ".", "It", "is", "demonstrated", "that", "our", "TAT-QA", "is", "very", "challenging", "and", "can", "serve", "as", "a", "benchmark", "for", "training", "and", "testing", "powerful", "QA", "models", "that", "address", "hybrid", "form", "data", "."], "entities": [{"type": "Operation", "start": 161, "end": 162, "text": "TAGOP", "sent_idx": 5}, {"type": "Effect", "start": 165, "end": 166, "text": "inF1", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--254"}
{"text": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word\u2019s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.", "tokens": ["We", "present", "a", "constituency", "parsing", "algorithm", "that", ",", "like", "a", "supertagger", ",", "works", "by", "assigning", "labels", "to", "each", "word", "in", "a", "sentence", ".", "In", "order", "to", "maximally", "leverage", "current", "neural", "architectures", ",", "the", "model", "scores", "each", "word", "\u2019s", "tags", "in", "parallel", ",", "with", "minimal", "task-specific", "structure", ".", "After", "scoring", ",", "a", "left-to-right", "reconciliation", "phase", "extracts", "a", "tree", "in", "(", "empirically", ")", "linear", "time", ".", "Our", "parser", "achieves", "95.4", "F1", "on", "the", "WSJ", "test", "set", "while", "also", "achieving", "substantial", "speedups", "compared", "to", "current", "state-of-the-art", "parsers", "with", "comparable", "accuracies", "."], "entities": [{"type": "Operation", "start": 3, "end": 6, "text": "constituency parsing algorithm", "sent_idx": 0}, {"type": "Effect", "start": 68, "end": 69, "text": "F1", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--557"}
{"text": "When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. I argue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.", "tokens": ["When", "designing", "grammars", "of", "natural", "language", ",", "typically", ",", "more", "than", "one", "formal", "analysis", "can", "account", "for", "a", "given", "phenomenon", ".", "Moreover", ",", "because", "analyses", "interact", ",", "the", "choices", "made", "by", "the", "engineer", "influence", "the", "possibilities", "available", "in", "further", "grammar", "development", ".", "The", "order", "in", "which", "phenomena", "are", "treated", "may", "therefore", "have", "a", "major", "impact", "on", "the", "resulting", "grammar", ".", "This", "paper", "proposes", "to", "tackle", "this", "problem", "by", "using", "metagrammar", "development", "as", "a", "methodology", "for", "grammar", "engineering", ".", "I", "argue", "that", "metagrammar", "engineering", "as", "an", "approach", "facilitates", "the", "systematic", "exploration", "of", "grammars", "through", "comparison", "of", "competing", "analyses", ".", "The", "idea", "is", "illustrated", "through", "a", "comparative", "study", "of", "auxiliary", "structures", "in", "HPSG-based", "grammars", "for", "German", "and", "Dutch", ".", "Auxiliaries", "form", "a", "central", "phenomenon", "of", "German", "and", "Dutch", "and", "are", "likely", "to", "influence", "many", "components", "of", "the", "grammar", ".", "This", "study", "shows", "that", "a", "special", "auxiliary+verb", "construction", "significantly", "improves", "efficiency", "compared", "to", "the", "standard", "argument-composition", "analysis", "for", "both", "parsing", "and", "generation", "."], "entities": [{"type": "Operation", "start": 142, "end": 145, "text": "special auxiliary+verb construction", "sent_idx": 7}, {"type": "Effect", "start": 147, "end": 148, "text": "efficiency", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1107"}
{"text": "We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and long-document summarization. Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.", "tokens": ["We", "present", "ReadOnce", "Transformers", ",", "an", "approach", "to", "convert", "a", "transformer-based", "model", "into", "one", "that", "can", "build", "an", "information-capturing", ",", "task-independent", ",", "and", "compressed", "representation", "of", "text", ".", "The", "resulting", "representation", "is", "reusable", "across", "different", "examples", "and", "tasks", ",", "thereby", "requiring", "a", "document", "shared", "across", "many", "examples", "or", "tasks", "to", "only", "be", "read", "once", ".", "This", "leads", "to", "faster", "training", "and", "evaluation", "of", "models", ".", "Additionally", ",", "we", "extend", "standard", "text-to-text", "transformer", "models", "to", "Representation+Text-to-text", "models", ",", "and", "evaluate", "on", "multiple", "downstream", "tasks", ":", "multi-hop", "QA", ",", "abstractive", "QA", ",", "and", "long-document", "summarization", ".", "Our", "one-time", "computed", "representation", "results", "in", "a", "2x-5x", "speedup", "compared", "to", "standard", "text-to-text", "models", ",", "while", "the", "compression", "also", "allows", "existing", "language", "models", "to", "handle", "longer", "documents", "without", "the", "need", "for", "designing", "new", "pre-trained", "models", "."], "entities": [{"type": "Operation", "start": 95, "end": 98, "text": "one-time computed representation", "sent_idx": 4}, {"type": "Effect", "start": 102, "end": 103, "text": "speedup", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--554"}
{"text": "Pretraining and multitask learning are widely used to improve the speech translation performance. In this study, we are interested in training a speech translation model along with an auxiliary text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs.", "tokens": ["Pretraining", "and", "multitask", "learning", "are", "widely", "used", "to", "improve", "the", "speech", "translation", "performance", ".", "In", "this", "study", ",", "we", "are", "interested", "in", "training", "a", "speech", "translation", "model", "along", "with", "an", "auxiliary", "text", "translation", "task", ".", "We", "conduct", "a", "detailed", "analysis", "to", "understand", "the", "impact", "of", "the", "auxiliary", "task", "on", "the", "primary", "task", "within", "the", "multitask", "learning", "framework", ".", "Our", "analysis", "confirms", "that", "multitask", "learning", "tends", "to", "generate", "similar", "decoder", "representations", "from", "different", "modalities", "and", "preserve", "more", "information", "from", "the", "pretrained", "text", "translation", "modules", ".", "We", "observe", "minimal", "negative", "transfer", "effect", "between", "the", "two", "tasks", "and", "sharing", "more", "parameters", "is", "helpful", "to", "transfer", "knowledge", "from", "the", "text", "task", "to", "the", "speech", "task", ".", "The", "analysis", "also", "reveals", "that", "the", "modality", "representation", "difference", "at", "the", "top", "decoder", "layers", "is", "still", "not", "negligible", ",", "and", "those", "layers", "are", "critical", "for", "the", "translation", "quality", ".", "Inspired", "by", "these", "findings", ",", "we", "propose", "three", "methods", "to", "improve", "translation", "quality", ".", "First", ",", "a", "parameter", "sharing", "and", "initialization", "strategy", "is", "proposed", "to", "enhance", "information", "sharing", "between", "the", "tasks", ".", "Second", ",", "a", "novel", "attention-based", "regularization", "is", "proposed", "for", "the", "encoders", "and", "pulls", "the", "representations", "from", "different", "modalities", "closer", ".", "Third", ",", "an", "online", "knowledge", "distillation", "is", "proposed", "to", "enhance", "the", "knowledge", "transfer", "from", "the", "text", "to", "the", "speech", "task", ".", "Our", "experiments", "show", "that", "the", "proposed", "approach", "improves", "translation", "performance", "by", "more", "than", "2", "BLEU", "over", "a", "strong", "baseline", "and", "achieves", "state-of-the-art", "results", "on", "the", "MuST-C", "English-German", ",", "English-French", "and", "English-Spanish", "language", "pairs", "."], "entities": [{"type": "Operation", "start": 158, "end": 163, "text": "parameter sharing and initialization strategy", "sent_idx": 7}, {"type": "Effect", "start": 228, "end": 229, "text": "BLEU", "sent_idx": 10}, {"type": "Operation", "start": 177, "end": 179, "text": "attention-based regularization", "sent_idx": 8}, {"type": "Operation", "start": 196, "end": 199, "text": "online knowledge distillation", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 3, "tail": 1}], "id": "abstract-2021--acl-long--328"}
{"text": "In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.", "tokens": ["In", "this", "paper", ",", "with", "a", "belief", "that", "a", "language", "model", "that", "embraces", "a", "larger", "context", "provides", "better", "prediction", "ability", ",", "we", "present", "two", "extensions", "to", "standard", "n-gram", "language", "models", "in", "statistical", "machine", "translation", ":", "a", "backward", "language", "model", "that", "augments", "the", "conventional", "forward", "language", "model", ",", "and", "a", "mutual", "information", "trigger", "model", "which", "captures", "long-distance", "dependencies", "that", "go", "beyond", "the", "scope", "of", "standard", "n-gram", "language", "models", ".", "We", "integrate", "the", "two", "proposed", "models", "into", "phrase-based", "statistical", "machine", "translation", "and", "conduct", "experiments", "on", "large-scale", "training", "data", "to", "investigate", "their", "effectiveness", ".", "Our", "experimental", "results", "show", "that", "both", "models", "are", "able", "to", "significantly", "improve", "translation", "quality", "and", "collectively", "achieve", "up", "to", "1", "BLEU", "point", "over", "a", "competitive", "baseline", "."], "entities": [{"type": "Operation", "start": 36, "end": 39, "text": "backward language model", "sent_idx": 0}, {"type": "Effect", "start": 103, "end": 105, "text": "translation quality", "sent_idx": 2}, {"type": "Operation", "start": 49, "end": 53, "text": "mutual information trigger model", "sent_idx": 0}, {"type": "Effect", "start": 111, "end": 113, "text": "BLEU point", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "P11-1129"}
{"text": "Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisy-channel model. We show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance. \n \nOur approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model. We use large language models, introduce new features into this reranker and examine different optimisation strategies. We obtain a disfluency detection f-scores of 0.838 which improves upon the current state-of-the-art.", "tokens": ["Unrehearsed", "spoken", "language", "often", "contains", "disfluencies", ".", "In", "order", "to", "correctly", "interpret", "a", "spoken", "utterance", ",", "any", "such", "disfluencies", "must", "be", "identified", "and", "removed", "or", "otherwise", "dealt", "with", ".", "Operating", "on", "transcripts", "of", "speech", "which", "contain", "disfluencies", ",", "we", "study", "the", "effect", "of", "language", "model", "and", "loss", "function", "on", "the", "performance", "of", "a", "linear", "reranker", "that", "rescores", "the", "25-best", "output", "of", "a", "noisy-channel", "model", ".", "We", "show", "that", "language", "models", "trained", "on", "large", "amounts", "of", "non-speech", "data", "improve", "performance", "more", "than", "a", "language", "model", "trained", "on", "a", "more", "modest", "amount", "of", "speech", "data", ",", "and", "that", "optimising", "f-score", "rather", "than", "log", "loss", "improves", "disfluency", "detection", "performance", ".", "\n \n", "Our", "approach", "uses", "a", "log-linear", "reranker", ",", "operating", "on", "the", "top", "n", "analyses", "of", "a", "noisy", "channel", "model", ".", "We", "use", "large", "language", "models", ",", "introduce", "new", "features", "into", "this", "reranker", "and", "examine", "different", "optimisation", "strategies", ".", "We", "obtain", "a", "disfluency", "detection", "f-scores", "of", "0.838", "which", "improves", "upon", "the", "current", "state-of-the-art", "."], "entities": [{"type": "Operation", "start": 129, "end": 132, "text": "large language models", "sent_idx": 5}, {"type": "Effect", "start": 150, "end": 151, "text": "f-scores", "sent_idx": 6}, {"type": "Operation", "start": 112, "end": 114, "text": "log-linear reranker", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P11-1071"}
{"text": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model\u2019s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL\u2019s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.", "tokens": ["Pretrained", "masked", "language", "models", "(", "MLMs", ")", "require", "finetuning", "for", "most", "NLP", "tasks", ".", "Instead", ",", "we", "evaluate", "MLMs", "out", "of", "the", "box", "via", "their", "pseudo-log-likelihood", "scores", "(", "PLLs", ")", ",", "which", "are", "computed", "by", "masking", "tokens", "one", "by", "one", ".", "We", "show", "that", "PLLs", "outperform", "scores", "from", "autoregressive", "language", "models", "like", "GPT-2", "in", "a", "variety", "of", "tasks", ".", "By", "rescoring", "ASR", "and", "NMT", "hypotheses", ",", "RoBERTa", "reduces", "an", "end-to-end", "LibriSpeech", "model", "\u2019s", "WER", "by", "30", "%", "relative", "and", "adds", "up", "to", "+", "1.7", "BLEU", "on", "state-of-the-art", "baselines", "for", "low-resource", "translation", "pairs", ",", "with", "further", "gains", "from", "domain", "adaptation", ".", "We", "attribute", "this", "success", "to", "PLL", "\u2019s", "unsupervised", "expression", "of", "linguistic", "acceptability", "without", "a", "left-to-right", "bias", ",", "greatly", "improving", "on", "scores", "from", "GPT-2", "(", "+", "10", "points", "on", "island", "effects", ",", "NPI", "licensing", "in", "BLiMP", ")", ".", "One", "can", "finetune", "MLMs", "to", "give", "scores", "without", "masking", ",", "enabling", "computation", "in", "a", "single", "inference", "pass", ".", "In", "all", ",", "PLLs", "and", "their", "associated", "pseudo-perplexities", "(", "PPPLs", ")", "enable", "plug-and-play", "use", "of", "the", "growing", "number", "of", "pretrained", "MLMs", ";", "e.g.", ",", "we", "use", "a", "single", "cross-lingual", "model", "to", "rescore", "translations", "in", "multiple", "languages", ".", "We", "release", "our", "library", "for", "language", "model", "scoring", "at", "https://github.com/awslabs/mlm-scoring", "."], "entities": [{"type": "Operation", "start": 105, "end": 116, "text": "PLL\u2019s unsupervised expression of linguistic acceptability without a left-to-right bias", "sent_idx": 4}, {"type": "Effect", "start": 120, "end": 121, "text": "scores", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--240"}
{"text": "In this paper, we present an unsupervised framework that bootstraps a complete coreference resolution (CoRe) system from word associations mined from a large unlabeled corpus. We show that word associations are useful for CoRe -- e.g., the strong association between Obama and President is an indicator of likely coreference. Association information has so far not been used in CoRe because it is sparse and difficult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system has better CoRe performance than other learning approaches that do not use manually labeled data.", "tokens": ["In", "this", "paper", ",", "we", "present", "an", "unsupervised", "framework", "that", "bootstraps", "a", "complete", "coreference", "resolution", "(", "CoRe", ")", "system", "from", "word", "associations", "mined", "from", "a", "large", "unlabeled", "corpus", ".", "We", "show", "that", "word", "associations", "are", "useful", "for", "CoRe", "--", "e.g.", ",", "the", "strong", "association", "between", "Obama", "and", "President", "is", "an", "indicator", "of", "likely", "coreference", ".", "Association", "information", "has", "so", "far", "not", "been", "used", "in", "CoRe", "because", "it", "is", "sparse", "and", "difficult", "to", "learn", "from", "small", "labeled", "corpora", ".", "Since", "unlabeled", "text", "is", "readily", "available", ",", "our", "unsupervised", "approach", "addresses", "the", "sparseness", "problem", ".", "In", "a", "self-training", "framework", ",", "we", "train", "a", "decision", "tree", "on", "a", "corpus", "that", "is", "automatically", "labeled", "using", "word", "associations", ".", "We", "show", "that", "this", "unsupervised", "system", "has", "better", "CoRe", "performance", "than", "other", "learning", "approaches", "that", "do", "not", "use", "manually", "labeled", "data", "."], "entities": [{"type": "Operation", "start": 118, "end": 120, "text": "unsupervised system", "sent_idx": 5}, {"type": "Effect", "start": 123, "end": 124, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1079"}
{"text": "Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.", "tokens": ["Pre-trained", "language", "models", "like", "BERT", "are", "performant", "in", "a", "wide", "range", "of", "natural", "language", "tasks", ".", "However", ",", "they", "are", "resource", "exhaustive", "and", "computationally", "expensive", "for", "industrial", "scenarios", ".", "Thus", ",", "early", "exits", "are", "adopted", "at", "each", "layer", "of", "BERT", "to", "perform", "adaptive", "computation", "by", "predicting", "easier", "samples", "with", "the", "first", "few", "layers", "to", "speed", "up", "the", "inference", ".", "In", "this", "work", ",", "to", "improve", "efficiency", "without", "performance", "drop", ",", "we", "propose", "a", "novel", "training", "scheme", "called", "Learned", "Early", "Exit", "for", "BERT", "(", "LeeBERT", ")", ".", "First", ",", "we", "ask", "each", "exit", "to", "learn", "from", "each", "other", ",", "rather", "than", "learning", "only", "from", "the", "last", "layer", ".", "Second", ",", "the", "weights", "of", "different", "loss", "terms", "are", "learned", ",", "thus", "balancing", "off", "different", "objectives", ".", "We", "formulate", "the", "optimization", "of", "LeeBERT", "as", "a", "bi-level", "optimization", "problem", ",", "and", "we", "propose", "a", "novel", "cross-level", "optimization", "(", "CLO", ")", "algorithm", "to", "improve", "the", "optimization", "results", ".", "Experiments", "on", "the", "GLUE", "benchmark", "show", "that", "our", "proposed", "methods", "improve", "the", "performance", "of", "the", "state-of-the-art", "(", "SOTA", ")", "early", "exit", "methods", "for", "pre-trained", "models", "."], "entities": [{"type": "Operation", "start": 129, "end": 130, "text": "LeeBERT", "sent_idx": 6}, {"type": "Effect", "start": 165, "end": 166, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--231"}
