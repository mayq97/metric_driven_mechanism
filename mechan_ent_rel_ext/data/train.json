{"text": "In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children\u2019s ability to understand others\u2019 thoughts, feelings, and desires (or \u201cmindreading\u201d). We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20 - a new corpus of children\u2019s performance on tests of mindreading, consisting of 10,320 question-answer pairs. We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points. Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems. The task-specific augmentations generally outperform task-agnostic augmentations. Automatic augmentations based on vectors (GloVe, FastText) perform the worst. We find that systems trained on MIND-CA generalize well to UK-MIND-20. We demonstrate that data augmentation strategies also improve the performance on unseen data.", "tokens": ["In", "this", "paper", "we", "implement", "and", "compare", "7", "different", "data", "augmentation", "strategies", "for", "the", "task", "of", "automatic", "scoring", "of", "children", "\u2019s", "ability", "to", "understand", "others", "\u2019", "thoughts", ",", "feelings", ",", "and", "desires", "(", "or", "\u201c", "mindreading", "\u201d", ")", ".", "We", "recruit", "in-domain", "experts", "to", "re-annotate", "augmented", "samples", "and", "determine", "to", "what", "extent", "each", "strategy", "preserves", "the", "original", "rating", ".", "We", "also", "carry", "out", "multiple", "experiments", "to", "measure", "how", "much", "each", "augmentation", "strategy", "improves", "the", "performance", "of", "automatic", "scoring", "systems", ".", "To", "determine", "the", "capabilities", "of", "automatic", "systems", "to", "generalize", "to", "unseen", "data", ",", "we", "create", "UK-MIND-20", "-", "a", "new", "corpus", "of", "children", "\u2019s", "performance", "on", "tests", "of", "mindreading", ",", "consisting", "of", "10,320", "question-answer", "pairs", ".", "We", "obtain", "a", "new", "state-of-the-art", "performance", "on", "the", "MIND-CA", "corpus", ",", "improving", "macro-F1-score", "by", "6", "points", ".", "Results", "indicate", "that", "both", "the", "number", "of", "training", "examples", "and", "the", "quality", "of", "the", "augmentation", "strategies", "affect", "the", "performance", "of", "the", "systems", ".", "The", "task-specific", "augmentations", "generally", "outperform", "task-agnostic", "augmentations", ".", "Automatic", "augmentations", "based", "on", "vectors", "(", "GloVe", ",", "FastText", ")", "perform", "the", "worst", ".", "We", "find", "that", "systems", "trained", "on", "MIND-CA", "generalize", "well", "to", "UK-MIND-20", ".", "We", "demonstrate", "that", "data", "augmentation", "strategies", "also", "improve", "the", "performance", "on", "unseen", "data", "."], "entities": [{"type": "Operation", "start": 143, "end": 148, "text": "quality of the augmentation strategies", "sent_idx": 5}, {"type": "Effect", "start": 150, "end": 151, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 137, "end": 141, "text": "number of training examples", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--96"}
{"text": "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation.", "tokens": ["Multi-task", "learning", "(", "MTL", ")", "and", "transfer", "learning", "(", "TL", ")", "are", "techniques", "to", "overcome", "the", "issue", "of", "data", "scarcity", "when", "training", "state-of-the-art", "neural", "networks", ".", "However", ",", "finding", "beneficial", "auxiliary", "datasets", "for", "MTL", "or", "TL", "is", "a", "time-", "and", "resource-consuming", "trial-and-error", "approach", ".", "We", "propose", "new", "methods", "to", "automatically", "assess", "the", "similarity", "of", "sequence", "tagging", "datasets", "to", "identify", "beneficial", "auxiliary", "data", "for", "MTL", "or", "TL", "setups", ".", "Our", "methods", "can", "compute", "the", "similarity", "between", "any", "two", "sequence", "tagging", "datasets", ",", "they", "do", "not", "need", "to", "be", "annotated", "with", "the", "same", "tagset", "or", "multiple", "labels", "in", "parallel", ".", "Additionally", ",", "our", "methods", "take", "tokens", "and", "their", "labels", "into", "account", ",", "which", "is", "more", "robust", "than", "only", "using", "either", "of", "them", "as", "an", "information", "source", ",", "as", "conducted", "in", "prior", "work", ".", "We", "empirically", "show", "that", "our", "similarity", "measures", "correlate", "with", "the", "change", "in", "test", "score", "of", "neural", "networks", "that", "use", "the", "auxiliary", "dataset", "for", "MTL", "to", "increase", "the", "main", "task", "performance", ".", "We", "provide", "an", "efficient", ",", "open-source", "implementation", "."], "entities": [{"type": "Operation", "start": 149, "end": 155, "text": "use the auxiliary dataset for MTL", "sent_idx": 5}, {"type": "Effect", "start": 160, "end": 161, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--268"}
{"text": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).", "tokens": ["Natural", "Language", "Processing", "(", "NLP", ")", "has", "recently", "achieved", "great", "success", "by", "using", "huge", "pre-trained", "models", "with", "hundreds", "of", "millions", "of", "parameters", ".", "However", ",", "these", "models", "suffer", "from", "heavy", "model", "sizes", "and", "high", "latency", "such", "that", "they", "can", "not", "be", "deployed", "to", "resource-limited", "mobile", "devices", ".", "In", "this", "paper", ",", "we", "propose", "MobileBERT", "for", "compressing", "and", "accelerating", "the", "popular", "BERT", "model", ".", "Like", "the", "original", "BERT", ",", "MobileBERT", "is", "task-agnostic", ",", "that", "is", ",", "it", "can", "be", "generically", "applied", "to", "various", "downstream", "NLP", "tasks", "via", "simple", "fine-tuning", ".", "Basically", ",", "MobileBERT", "is", "a", "thin", "version", "of", "BERT_LARGE", ",", "while", "equipped", "with", "bottleneck", "structures", "and", "a", "carefully", "designed", "balance", "between", "self-attentions", "and", "feed-forward", "networks", ".", "To", "train", "MobileBERT", ",", "we", "first", "train", "a", "specially", "designed", "teacher", "model", ",", "an", "inverted-bottleneck", "incorporated", "BERT_LARGE", "model", ".", "Then", ",", "we", "conduct", "knowledge", "transfer", "from", "this", "teacher", "to", "MobileBERT", ".", "Empirical", "studies", "show", "that", "MobileBERT", "is", "4.3x", "smaller", "and", "5.5x", "faster", "than", "BERT_BASE", "while", "achieving", "competitive", "results", "on", "well-known", "benchmarks", ".", "On", "the", "natural", "language", "inference", "tasks", "of", "GLUE", ",", "MobileBERT", "achieves", "a", "GLUE", "score", "of", "77.7", "(", "0.6", "lower", "than", "BERT_BASE", ")", ",", "and", "62", "ms", "latency", "on", "a", "Pixel", "4", "phone", ".", "On", "the", "SQuAD", "v1.1/v2.0", "question", "answering", "task", ",", "MobileBERT", "achieves", "a", "dev", "F1", "score", "of", "90.0/79.2", "(", "1.5/2.1", "higher", "than", "BERT_BASE", ")", "."], "entities": [{"type": "Operation", "start": 176, "end": 177, "text": "MobileBERT", "sent_idx": 8}, {"type": "Effect", "start": 179, "end": 181, "text": "GLUE score", "sent_idx": 8}, {"type": "Effect", "start": 193, "end": 194, "text": "latency", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--195"}
{"text": "Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.", "tokens": ["Implicit", "discourse", "relation", "recognition", "is", "a", "challenging", "task", "due", "to", "the", "lack", "of", "connectives", "as", "strong", "linguistic", "clues", ".", "Previous", "methods", "primarily", "encode", "two", "arguments", "separately", "or", "extract", "the", "specific", "interaction", "patterns", "for", "the", "task", ",", "which", "have", "not", "fully", "exploited", "the", "annotated", "relation", "signal", ".", "Therefore", ",", "we", "propose", "a", "novel", "TransS-driven", "joint", "learning", "architecture", "to", "address", "the", "issues", ".", "Specifically", ",", "based", "on", "the", "multi-level", "encoder", ",", "we", "1", ")", "translate", "discourse", "relations", "in", "low-dimensional", "embedding", "space", "(", "called", "TransS", ")", ",", "which", "could", "mine", "the", "latent", "geometric", "structure", "information", "of", "argument-relation", "instances", ";", "2", ")", "further", "exploit", "the", "semantic", "features", "of", "arguments", "to", "assist", "discourse", "understanding", ";", "3", ")", "jointly", "learn", "1", ")", "and", "2", ")", "to", "mutually", "reinforce", "each", "other", "to", "obtain", "the", "better", "argument", "representations", ",", "so", "as", "to", "improve", "the", "performance", "of", "the", "task", ".", "Extensive", "experimental", "results", "on", "the", "Penn", "Discourse", "TreeBank", "(", "PDTB", ")", "show", "that", "our", "model", "achieves", "competitive", "results", "against", "several", "state-of-the-art", "systems", "."], "entities": [{"type": "Operation", "start": 125, "end": 130, "text": "obtain the better argument representations", "sent_idx": 3}, {"type": "Effect", "start": 136, "end": 137, "text": "performance", "sent_idx": 3}, {"type": "Operation", "start": 66, "end": 68, "text": "multi-level encoder", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--14"}
{"text": "Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.", "tokens": ["Document-level", "MT", "models", "are", "still", "far", "from", "satisfactory", ".", "Existing", "work", "extend", "translation", "unit", "from", "single", "sentence", "to", "multiple", "sentences", ".", "However", ",", "study", "shows", "that", "when", "we", "further", "enlarge", "the", "translation", "unit", "to", "a", "whole", "document", ",", "supervised", "training", "of", "Transformer", "can", "fail", ".", "In", "this", "paper", ",", "we", "find", "such", "failure", "is", "not", "caused", "by", "overfitting", ",", "but", "by", "sticking", "around", "local", "minima", "during", "training", ".", "Our", "analysis", "shows", "that", "the", "increased", "complexity", "of", "target-to-source", "attention", "is", "a", "reason", "for", "the", "failure", ".", "As", "a", "solution", ",", "we", "propose", "G-Transformer", ",", "introducing", "locality", "assumption", "as", "an", "inductive", "bias", "into", "Transformer", ",", "reducing", "the", "hypothesis", "space", "of", "the", "attention", "from", "target", "to", "source", ".", "Experiments", "show", "that", "G-Transformer", "converges", "faster", "and", "more", "stably", "than", "Transformer", ",", "achieving", "new", "state-of-the-art", "BLEU", "scores", "for", "both", "nonpretraining", "and", "pre-training", "settings", "on", "three", "benchmark", "datasets", "."], "entities": [{"type": "Operation", "start": 118, "end": 119, "text": "G-Transformer", "sent_idx": 6}, {"type": "Effect", "start": 130, "end": 132, "text": "BLEU scores", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--267"}
{"text": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.", "tokens": ["Named", "Entity", "Recognition", "(", "NER", ")", "performance", "often", "degrades", "rapidly", "when", "applied", "to", "target", "domains", "that", "differ", "from", "the", "texts", "observed", "during", "training", ".", "When", "in-domain", "labelled", "data", "is", "available", ",", "transfer", "learning", "techniques", "can", "be", "used", "to", "adapt", "existing", "NER", "models", "to", "the", "target", "domain", ".", "But", "what", "should", "one", "do", "when", "there", "is", "no", "hand-labelled", "data", "for", "the", "target", "domain", "?", "This", "paper", "presents", "a", "simple", "but", "powerful", "approach", "to", "learn", "NER", "models", "in", "the", "absence", "of", "labelled", "data", "through", "weak", "supervision", ".", "The", "approach", "relies", "on", "a", "broad", "spectrum", "of", "labelling", "functions", "to", "automatically", "annotate", "texts", "from", "the", "target", "domain", ".", "These", "annotations", "are", "then", "merged", "together", "using", "a", "hidden", "Markov", "model", "which", "captures", "the", "varying", "accuracies", "and", "confusions", "of", "the", "labelling", "functions", ".", "A", "sequence", "labelling", "model", "can", "finally", "be", "trained", "on", "the", "basis", "of", "this", "unified", "annotation", ".", "We", "evaluate", "the", "approach", "on", "two", "English", "datasets", "(", "CoNLL", "2003", "and", "news", "articles", "from", "Reuters", "and", "Bloomberg", ")", "and", "demonstrate", "an", "improvement", "of", "about", "7", "percentage", "points", "in", "entity-level", "F1", "scores", "compared", "to", "an", "out-of-domain", "neural", "NER", "model", "."], "entities": [{"type": "Operation", "start": 82, "end": 84, "text": "weak supervision", "sent_idx": 2}, {"type": "Effect", "start": 173, "end": 175, "text": "F1 scores", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--139"}
{"text": "We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest-scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.", "tokens": ["We", "propose", "a", "novel", "self-training", "method", "for", "a", "parser", "which", "uses", "a", "lexicalised", "grammar", "and", "supertagger", ",", "focusing", "on", "increasing", "the", "speed", "of", "the", "parser", "rather", "than", "its", "accuracy", ".", "The", "idea", "is", "to", "train", "the", "supertagger", "on", "large", "amounts", "of", "parser", "output", ",", "so", "that", "the", "supertagger", "can", "learn", "to", "supply", "the", "supertags", "that", "the", "parser", "will", "eventually", "choose", "as", "part", "of", "the", "highest-scoring", "derivation", ".", "Since", "the", "supertagger", "supplies", "fewer", "supertags", "overall", ",", "the", "parsing", "speed", "is", "increased", ".", "We", "demonstrate", "the", "effectiveness", "of", "the", "method", "using", "a", "CCG", "supertagger", "and", "parser", ",", "obtaining", "significant", "speed", "increases", "on", "newspaper", "text", "with", "no", "loss", "in", "accuracy", ".", "We", "also", "show", "that", "the", "method", "can", "be", "used", "to", "adapt", "the", "CCG", "parser", "to", "new", "domains", ",", "obtaining", "accuracy", "and", "speed", "improvements", "for", "Wikipedia", "and", "biomedical", "text", "."], "entities": [{"type": "Operation", "start": 4, "end": 6, "text": "self-training method", "sent_idx": 0}, {"type": "Effect", "start": 127, "end": 128, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 129, "end": 130, "text": "speed", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P10-1036"}
{"text": "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.", "tokens": ["Most", "existing", "work", "on", "automated", "fact", "checking", "is", "concerned", "with", "predicting", "the", "veracity", "of", "claims", "based", "on", "metadata", ",", "social", "network", "spread", ",", "language", "used", "in", "claims", ",", "and", ",", "more", "recently", ",", "evidence", "supporting", "or", "denying", "claims", ".", "A", "crucial", "piece", "of", "the", "puzzle", "that", "is", "still", "missing", "is", "to", "understand", "how", "to", "automate", "the", "most", "elaborate", "part", "of", "the", "process", "\u2013", "generating", "justifications", "for", "verdicts", "on", "claims", ".", "This", "paper", "provides", "the", "first", "study", "of", "how", "these", "explanations", "can", "be", "generated", "automatically", "based", "on", "available", "claim", "context", ",", "and", "how", "this", "task", "can", "be", "modelled", "jointly", "with", "veracity", "prediction", ".", "Our", "results", "indicate", "that", "optimising", "both", "objectives", "at", "the", "same", "time", ",", "rather", "than", "training", "them", "separately", ",", "improves", "the", "performance", "of", "a", "fact", "checking", "system", ".", "The", "results", "of", "a", "manual", "evaluation", "further", "suggest", "that", "the", "informativeness", ",", "coverage", "and", "overall", "quality", "of", "the", "generated", "explanations", "are", "also", "improved", "in", "the", "multi-task", "model", "."], "entities": [{"type": "Operation", "start": 154, "end": 156, "text": "multi-task model", "sent_idx": 4}, {"type": "Effect", "start": 139, "end": 140, "text": "informativeness", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 142, "text": "coverage", "sent_idx": 4}, {"type": "Effect", "start": 143, "end": 145, "text": "overall quality", "sent_idx": 4}, {"type": "Operation", "start": 106, "end": 113, "text": "optimising both objectives at the same time", "sent_idx": 3}, {"type": "Effect", "start": 122, "end": 128, "text": "performance of a fact checking system", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 4, "tail": 5}], "id": "abstract-2020--acl-main--656"}
{"text": "Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly \u201creorder\u201d the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.", "tokens": ["Paraphrasing", "natural", "language", "sentences", "is", "a", "multifaceted", "process", ":", "it", "might", "involve", "replacing", "individual", "words", "or", "short", "phrases", ",", "local", "rearrangement", "of", "content", ",", "or", "high-level", "restructuring", "like", "topicalization", "or", "passivization", ".", "Past", "approaches", "struggle", "to", "cover", "this", "space", "of", "paraphrase", "possibilities", "in", "an", "interpretable", "manner", ".", "Our", "work", ",", "inspired", "by", "pre-ordering", "literature", "in", "machine", "translation", ",", "uses", "syntactic", "transformations", "to", "softly", "\u201c", "reorder", "\u201d", "the", "source", "sentence", "and", "guide", "our", "neural", "paraphrasing", "model", ".", "First", ",", "given", "an", "input", "sentence", ",", "we", "derive", "a", "set", "of", "feasible", "syntactic", "rearrangements", "using", "an", "encoder-decoder", "model", ".", "This", "model", "operates", "over", "a", "partially", "lexical", ",", "partially", "syntactic", "view", "of", "the", "sentence", "and", "can", "reorder", "big", "chunks", ".", "Next", ",", "we", "use", "each", "proposed", "rearrangement", "to", "produce", "a", "sequence", "of", "position", "embeddings", ",", "which", "encourages", "our", "final", "encoder-decoder", "paraphrase", "model", "to", "attend", "to", "the", "source", "words", "in", "a", "particular", "order", ".", "Our", "evaluation", ",", "both", "automatic", "and", "human", ",", "shows", "that", "the", "proposed", "system", "retains", "the", "quality", "of", "the", "baseline", "approaches", "while", "giving", "a", "substantial", "increase", "in", "the", "diversity", "of", "the", "generated", "paraphrases", "."], "entities": [{"type": "Operation", "start": 122, "end": 130, "text": "rearrangement to produce a sequence of position embeddings", "sent_idx": 5}, {"type": "Effect", "start": 176, "end": 177, "text": "diversity", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--22"}
{"text": "This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.", "tokens": ["This", "work", "revisits", "the", "task", "of", "training", "sequence", "tagging", "models", "with", "limited", "resources", "using", "transfer", "learning", ".", "We", "investigate", "several", "proposed", "approaches", "introduced", "in", "recent", "works", "and", "suggest", "a", "new", "loss", "that", "relies", "on", "sentence", "reconstruction", "from", "normalized", "embeddings", ".", "Specifically", ",", "our", "method", "demonstrates", "how", "by", "adding", "a", "decoding", "layer", "for", "sentence", "reconstruction", ",", "we", "can", "improve", "the", "performance", "of", "various", "baselines", ".", "We", "show", "improved", "results", "on", "the", "CoNLL02", "NER", "and", "UD", "1.2", "POS", "datasets", "and", "demonstrate", "the", "power", "of", "the", "method", "for", "transfer", "learning", "with", "low-resources", "achieving", "0.6", "F1", "score", "in", "Dutch", "using", "only", "one", "sample", "from", "it", "."], "entities": [{"type": "Operation", "start": 47, "end": 54, "text": "adding a decoding layer for sentence reconstruction", "sent_idx": 2}, {"type": "Effect", "start": 59, "end": 60, "text": "performance", "sent_idx": 2}, {"type": "Effect", "start": 91, "end": 93, "text": "F1 score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--239"}
{"text": "In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions. In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output. Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models. We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa.", "tokens": ["In", "open-domain", "question", "answering", ",", "questions", "are", "highly", "likely", "to", "be", "ambiguous", "because", "users", "may", "not", "know", "the", "scope", "of", "relevant", "topics", "when", "formulating", "them", ".", "Therefore", ",", "a", "system", "needs", "to", "find", "possible", "interpretations", "of", "the", "question", ",", "and", "predict", "one", "or", "multiple", "plausible", "answers", ".", "When", "multiple", "plausible", "answers", "are", "found", ",", "the", "system", "should", "rewrite", "the", "question", "for", "each", "answer", "to", "resolve", "the", "ambiguity", ".", "In", "this", "paper", ",", "we", "present", "a", "model", "that", "aggregates", "and", "combines", "evidence", "from", "multiple", "passages", "to", "adaptively", "predict", "a", "single", "answer", "or", "a", "set", "of", "question-answer", "pairs", "for", "ambiguous", "questions", ".", "In", "addition", ",", "we", "propose", "a", "novel", "round-trip", "prediction", "approach", "to", "iteratively", "generate", "additional", "interpretations", "that", "our", "model", "fails", "to", "find", "in", "the", "first", "pass", ",", "and", "then", "verify", "and", "filter", "out", "the", "incorrect", "question-answer", "pairs", "to", "arrive", "at", "the", "final", "disambiguated", "output", ".", "Our", "model", ",", "named", "Refuel", ",", "achieves", "a", "new", "state-of-the-art", "performance", "on", "the", "AmbigQA", "dataset", ",", "and", "shows", "competitive", "performance", "on", "NQ-Open", "and", "TriviaQA", ".", "The", "proposed", "round-trip", "prediction", "is", "a", "model-agnostic", "general", "approach", "for", "answering", "ambiguous", "open-domain", "questions", ",", "which", "improves", "our", "Refuel", "as", "well", "as", "several", "baseline", "models", ".", "We", "release", "source", "code", "for", "our", "models", "and", "experiments", "at", "https://github.com/amzn/refuel-open-domain-qa", "."], "entities": [{"type": "Operation", "start": 148, "end": 149, "text": "Refuel", "sent_idx": 5}, {"type": "Effect", "start": 154, "end": 155, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--253"}
{"text": "Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.", "tokens": ["Deep", "and", "large", "pre-trained", "language", "models", "are", "the", "state-of-the-art", "for", "various", "natural", "language", "processing", "tasks", ".", "However", ",", "the", "huge", "size", "of", "these", "models", "could", "be", "a", "deterrent", "to", "using", "them", "in", "practice", ".", "Some", "recent", "works", "use", "knowledge", "distillation", "to", "compress", "these", "huge", "models", "into", "shallow", "ones", ".", "In", "this", "work", "we", "study", "knowledge", "distillation", "with", "a", "focus", "on", "multilingual", "Named", "Entity", "Recognition", "(", "NER", ")", ".", "In", "particular", ",", "we", "study", "several", "distillation", "strategies", "and", "propose", "a", "stage-wise", "optimization", "scheme", "leveraging", "teacher", "internal", "representations", ",", "that", "is", "agnostic", "of", "teacher", "architecture", ",", "and", "show", "that", "it", "outperforms", "strategies", "employed", "in", "prior", "works", ".", "Additionally", ",", "we", "investigate", "the", "role", "of", "several", "factors", "like", "the", "amount", "of", "unlabeled", "data", ",", "annotation", "resources", ",", "model", "architecture", "and", "inference", "latency", "to", "name", "a", "few", ".", "We", "show", "that", "our", "approach", "leads", "to", "massive", "compression", "of", "teacher", "models", "like", "mBERT", "by", "upto", "35x", "in", "terms", "of", "parameters", "and", "51x", "in", "terms", "of", "latency", "for", "batch", "inference", "while", "retaining", "95", "%", "of", "its", "F1-score", "for", "NER", "over", "41", "languages", "."], "entities": [{"type": "Operation", "start": 79, "end": 86, "text": "stage-wise optimization scheme leveraging teacher internal representations", "sent_idx": 4}, {"type": "Effect", "start": 154, "end": 155, "text": "parameters", "sent_idx": 6}, {"type": "Effect", "start": 160, "end": 161, "text": "latency", "sent_idx": 6}, {"type": "Effect", "start": 170, "end": 171, "text": "F1-score", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--202"}
{"text": "Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training the model with constraints provided as part of the input sequence. Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. Our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation.", "tokens": ["Lexically", "constrained", "machine", "translation", "allows", "the", "user", "to", "manipulate", "the", "output", "sentence", "by", "enforcing", "the", "presence", "or", "absence", "of", "certain", "words", "and", "phrases", ".", "Although", "current", "approaches", "can", "enforce", "terms", "to", "appear", "in", "the", "translation", ",", "they", "often", "struggle", "to", "make", "the", "constraint", "word", "form", "agree", "with", "the", "rest", "of", "the", "generated", "output", ".", "Our", "manual", "analysis", "shows", "that", "46", "%", "of", "the", "errors", "in", "the", "output", "of", "a", "baseline", "constrained", "model", "for", "English", "to", "Czech", "translation", "are", "related", "to", "agreement", ".", "We", "investigate", "mechanisms", "to", "allow", "neural", "machine", "translation", "to", "infer", "the", "correct", "word", "inflection", "given", "lemmatized", "constraints", ".", "In", "particular", ",", "we", "focus", "on", "methods", "based", "on", "training", "the", "model", "with", "constraints", "provided", "as", "part", "of", "the", "input", "sequence", ".", "Our", "experiments", "on", "English-Czech", "language", "pair", "show", "that", "this", "approach", "improves", "translation", "of", "constrained", "terms", "in", "both", "automatic", "and", "manual", "evaluation", "by", "reducing", "errors", "in", "agreement", ".", "Our", "approach", "thus", "eliminates", "inflection", "errors", ",", "without", "introducing", "new", "errors", "or", "decreasing", "overall", "quality", "of", "the", "translation", "."], "entities": [{"type": "Operation", "start": 113, "end": 121, "text": "constraints provided as part of the input sequence", "sent_idx": 4}, {"type": "Effect", "start": 145, "end": 148, "text": "errors in agreement", "sent_idx": 5}, {"type": "Effect", "start": 162, "end": 164, "text": "overall quality", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--311"}
{"text": "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.", "tokens": ["Zero-shot", "transfer", "learning", "for", "multi-domain", "dialogue", "state", "tracking", "can", "allow", "us", "to", "handle", "new", "domains", "without", "incurring", "the", "high", "cost", "of", "data", "acquisition", ".", "This", "paper", "proposes", "new", "zero-short", "transfer", "learning", "technique", "for", "dialogue", "state", "tracking", "where", "the", "in-domain", "training", "data", "are", "all", "synthesized", "from", "an", "abstract", "dialogue", "model", "and", "the", "ontology", "of", "the", "domain", ".", "We", "show", "that", "data", "augmentation", "through", "synthesized", "data", "can", "improve", "the", "accuracy", "of", "zero-shot", "learning", "for", "both", "the", "TRADE", "model", "and", "the", "BERT-based", "SUMBT", "model", "on", "the", "MultiWOZ", "2.1", "dataset", ".", "We", "show", "training", "with", "only", "synthesized", "in-domain", "data", "on", "the", "SUMBT", "model", "can", "reach", "about", "2/3", "of", "the", "accuracy", "obtained", "with", "the", "full", "training", "dataset", ".", "We", "improve", "the", "zero-shot", "learning", "state", "of", "the", "art", "on", "average", "across", "domains", "by", "21", "%", "."], "entities": [{"type": "Operation", "start": 92, "end": 99, "text": "synthesized in-domain data on the SUMBT model", "sent_idx": 3}, {"type": "Effect", "start": 105, "end": 106, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--12"}
{"text": "Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.", "tokens": ["Context", "gates", "are", "effective", "to", "control", "the", "contributions", "from", "the", "source", "and", "target", "contexts", "in", "the", "recurrent", "neural", "network", "(", "RNN", ")", "based", "neural", "machine", "translation", "(", "NMT", ")", ".", "However", ",", "it", "is", "challenging", "to", "extend", "them", "into", "the", "advanced", "Transformer", "architecture", ",", "which", "is", "more", "complicated", "than", "RNN", ".", "This", "paper", "first", "provides", "a", "method", "to", "identify", "source", "and", "target", "contexts", "and", "then", "introduce", "a", "gate", "mechanism", "to", "control", "the", "source", "and", "target", "contributions", "in", "Transformer", ".", "In", "addition", ",", "to", "further", "reduce", "the", "bias", "problem", "in", "the", "gate", "mechanism", ",", "this", "paper", "proposes", "a", "regularization", "method", "to", "guide", "the", "learning", "of", "the", "gates", "with", "supervision", "automatically", "generated", "using", "pointwise", "mutual", "information", ".", "Extensive", "experiments", "on", "4", "translation", "datasets", "demonstrate", "that", "the", "proposed", "model", "obtains", "an", "averaged", "gain", "of", "1.0", "BLEU", "score", "over", "a", "strong", "Transformer", "baseline", "."], "entities": [{"type": "Operation", "start": 65, "end": 78, "text": "introduce a gate mechanism to control the source and target contributions in Transformer", "sent_idx": 2}, {"type": "Effect", "start": 132, "end": 134, "text": "BLEU score", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--757"}
{"text": "While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++.", "tokens": ["While", "Inversion", "Transduction", "Grammar", "(", "ITG", ")", "has", "regained", "more", "and", "more", "attention", "in", "recent", "years", ",", "it", "still", "suffers", "from", "the", "major", "obstacle", "of", "speed", ".", "We", "propose", "a", "discriminative", "ITG", "pruning", "framework", "using", "Minimum", "Error", "Rate", "Training", "and", "various", "features", "from", "previous", "work", "on", "ITG", "alignment", ".", "Experiment", "results", "show", "that", "it", "is", "superior", "to", "all", "existing", "heuristics", "in", "ITG", "pruning", ".", "On", "top", "of", "the", "pruning", "framework", ",", "we", "also", "propose", "a", "discriminative", "ITG", "alignment", "model", "using", "hierarchical", "phrase", "pairs", ",", "which", "improves", "both", "F-score", "and", "Bleu", "score", "over", "the", "baseline", "alignment", "system", "of", "GIZA++", "."], "entities": [{"type": "Operation", "start": 75, "end": 79, "text": "discriminative ITG alignment model", "sent_idx": 3}, {"type": "Effect", "start": 87, "end": 88, "text": "F-score", "sent_idx": 3}, {"type": "Effect", "start": 89, "end": 91, "text": "Bleu score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P10-1033"}
{"text": "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.", "tokens": ["In", "this", "paper", ",", "we", "detail", "the", "relationship", "between", "convolutions", "and", "self-attention", "in", "natural", "language", "tasks", ".", "We", "show", "that", "relative", "position", "embeddings", "in", "self-attention", "layers", "are", "equivalent", "to", "recently-proposed", "dynamic", "lightweight", "convolutions", ",", "and", "we", "consider", "multiple", "new", "ways", "of", "integrating", "convolutions", "into", "Transformer", "self-attention", ".", "Specifically", ",", "we", "propose", "composite", "attention", ",", "which", "unites", "previous", "relative", "position", "encoding", "methods", "under", "a", "convolutional", "framework", ".", "We", "conduct", "experiments", "by", "training", "BERT", "with", "composite", "attention", ",", "finding", "that", "convolutions", "consistently", "improve", "performance", "on", "multiple", "downstream", "tasks", ",", "replacing", "absolute", "position", "embeddings", ".", "To", "inform", "future", "work", ",", "we", "present", "results", "comparing", "lightweight", "convolutions", ",", "dynamic", "convolutions", ",", "and", "depthwise-separable", "convolutions", "in", "language", "model", "pre-training", ",", "considering", "multiple", "injection", "points", "for", "convolutions", "in", "self-attention", "layers", "."], "entities": [{"type": "Operation", "start": 78, "end": 79, "text": "convolutions", "sent_idx": 3}, {"type": "Effect", "start": 81, "end": 82, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--333"}
{"text": "Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce \u201centity triggers,\u201d an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.", "tokens": ["Training", "neural", "models", "for", "named", "entity", "recognition", "(", "NER", ")", "in", "a", "new", "domain", "often", "requires", "additional", "human", "annotations", "(", "e.g.", ",", "tens", "of", "thousands", "of", "labeled", "instances", ")", "that", "are", "usually", "expensive", "and", "time-consuming", "to", "collect", ".", "Thus", ",", "a", "crucial", "research", "question", "is", "how", "to", "obtain", "supervision", "in", "a", "cost-effective", "way", ".", "In", "this", "paper", ",", "we", "introduce", "\u201c", "entity", "triggers", ",", "\u201d", "an", "effective", "proxy", "of", "human", "explanations", "for", "facilitating", "label-efficient", "learning", "of", "NER", "models", ".", "An", "entity", "trigger", "is", "defined", "as", "a", "group", "of", "words", "in", "a", "sentence", "that", "helps", "to", "explain", "why", "humans", "would", "recognize", "an", "entity", "in", "the", "sentence", ".", "We", "crowd-sourced", "14k", "entity", "triggers", "for", "two", "well-studied", "NER", "datasets", ".", "Our", "proposed", "model", ",", "Trigger", "Matching", "Network", ",", "jointly", "learns", "trigger", "representations", "and", "soft", "matching", "module", "with", "self-attention", "such", "that", "can", "generalize", "to", "unseen", "sentences", "easily", "for", "tagging", ".", "Our", "framework", "is", "significantly", "more", "cost-effective", "than", "the", "traditional", "neural", "NER", "frameworks", ".", "Experiments", "show", "that", "using", "only", "20", "%", "of", "the", "trigger-annotated", "sentences", "results", "in", "a", "comparable", "performance", "as", "using", "70", "%", "of", "conventional", "annotated", "sentences", "."], "entities": [{"type": "Operation", "start": 168, "end": 170, "text": "trigger-annotated sentences", "sent_idx": 7}, {"type": "Effect", "start": 174, "end": 175, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--752"}
{"text": "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.", "tokens": ["Personalization", "of", "natural", "language", "generation", "plays", "a", "vital", "role", "in", "a", "large", "spectrum", "of", "tasks", ",", "such", "as", "explainable", "recommendation", ",", "review", "summarization", "and", "dialog", "systems", ".", "In", "these", "tasks", ",", "user", "and", "item", "IDs", "are", "important", "identifiers", "for", "personalization", ".", "Transformer", ",", "which", "is", "demonstrated", "with", "strong", "language", "modeling", "capability", ",", "however", ",", "is", "not", "personalized", "and", "fails", "to", "make", "use", "of", "the", "user", "and", "item", "IDs", "since", "the", "ID", "tokens", "are", "not", "even", "in", "the", "same", "semantic", "space", "as", "the", "words", ".", "To", "address", "this", "problem", ",", "we", "present", "a", "PErsonalized", "Transformer", "for", "Explainable", "Recommendation", "(", "PETER", ")", ",", "on", "which", "we", "design", "a", "simple", "and", "effective", "learning", "objective", "that", "utilizes", "the", "IDs", "to", "predict", "the", "words", "in", "the", "target", "explanation", ",", "so", "as", "to", "endow", "the", "IDs", "with", "linguistic", "meanings", "and", "to", "achieve", "personalized", "Transformer", ".", "Besides", "generating", "explanations", ",", "PETER", "can", "also", "make", "recommendations", ",", "which", "makes", "it", "a", "unified", "model", "for", "the", "whole", "recommendation-explanation", "pipeline", ".", "Extensive", "experiments", "show", "that", "our", "small", "unpretrained", "model", "outperforms", "fine-tuned", "BERT", "on", "the", "generation", "task", ",", "in", "terms", "of", "both", "effectiveness", "and", "efficiency", ",", "which", "highlights", "the", "importance", "and", "the", "nice", "utility", "of", "our", "design", "."], "entities": [{"type": "Operation", "start": 166, "end": 169, "text": "small unpretrained model", "sent_idx": 5}, {"type": "Effect", "start": 181, "end": 182, "text": "effectiveness", "sent_idx": 5}, {"type": "Effect", "start": 183, "end": 184, "text": "efficiency", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--383"}
{"text": "Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts\u2019 semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses\u2019 relevance and coherence while maintaining their diversity and informativeness. SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group. Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets.", "tokens": ["Conditional", "Variational", "AutoEncoder", "(", "CVAE", ")", "effectively", "increases", "the", "diversity", "and", "informativeness", "of", "responses", "in", "open-ended", "dialogue", "generation", "tasks", "through", "enriching", "the", "context", "vector", "with", "sampled", "latent", "variables", ".", "However", ",", "due", "to", "the", "inherent", "one-to-many", "and", "many-to-one", "phenomena", "in", "human", "dialogues", ",", "the", "sampled", "latent", "variables", "may", "not", "correctly", "reflect", "the", "contexts", "\u2019", "semantics", ",", "leading", "to", "irrelevant", "and", "incoherent", "generated", "responses", ".", "To", "resolve", "this", "problem", ",", "we", "propose", "Self-separated", "Conditional", "Variational", "AutoEncoder", "(", "abbreviated", "as", "SepaCVAE", ")", "that", "introduces", "group", "information", "to", "regularize", "the", "latent", "variables", ",", "which", "enhances", "CVAE", "by", "improving", "the", "responses", "\u2019", "relevance", "and", "coherence", "while", "maintaining", "their", "diversity", "and", "informativeness", ".", "SepaCVAE", "actively", "divides", "the", "input", "data", "into", "groups", ",", "and", "then", "widens", "the", "absolute", "difference", "between", "data", "pairs", "from", "distinct", "groups", ",", "while", "narrowing", "the", "relative", "distance", "between", "data", "pairs", "in", "the", "same", "group", ".", "Empirical", "results", "from", "automatic", "evaluation", "and", "detailed", "analysis", "demonstrate", "that", "SepaCVAE", "can", "significantly", "boost", "responses", "in", "well-established", "open-domain", "dialogue", "datasets", "."], "entities": [{"type": "Operation", "start": 108, "end": 109, "text": "SepaCVAE", "sent_idx": 3}, {"type": "Effect", "start": 133, "end": 138, "text": "relative distance between data pairs", "sent_idx": 3}, {"type": "Operation", "start": 71, "end": 80, "text": "Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE)", "sent_idx": 2}, {"type": "Effect", "start": 104, "end": 105, "text": "diversity", "sent_idx": 2}, {"type": "Effect", "start": 106, "end": 107, "text": "informativeness", "sent_idx": 2}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}, {"type": "Affect", "head": 2, "tail": 4}], "id": "abstract-2021--acl-long--437"}
{"text": "Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.", "tokens": ["Knowing", "the", "Most", "Frequent", "Sense", "(", "MFS", ")", "of", "a", "word", "has", "been", "proved", "to", "help", "Word", "Sense", "Disambiguation", "(", "WSD", ")", "models", "significantly", ".", "However", ",", "the", "scarcity", "of", "sense-annotated", "data", "makes", "it", "difficult", "to", "induce", "a", "reliable", "and", "high-coverage", "distribution", "of", "the", "meanings", "in", "a", "language", "vocabulary", ".", "To", "address", "this", "issue", ",", "in", "this", "paper", "we", "present", "CluBERT", ",", "an", "automatic", "and", "multilingual", "approach", "for", "inducing", "the", "distributions", "of", "word", "senses", "from", "a", "corpus", "of", "raw", "sentences", ".", "Our", "experiments", "show", "that", "CluBERT", "learns", "distributions", "over", "English", "senses", "that", "are", "of", "higher", "quality", "than", "those", "extracted", "by", "alternative", "approaches", ".", "When", "used", "to", "induce", "the", "MFS", "of", "a", "lemma", ",", "CluBERT", "attains", "state-of-the-art", "results", "on", "the", "English", "Word", "Sense", "Disambiguation", "tasks", "and", "helps", "to", "improve", "the", "disambiguation", "performance", "of", "two", "off-the-shelf", "WSD", "models", ".", "Moreover", ",", "our", "distributions", "also", "prove", "to", "be", "effective", "in", "other", "languages", ",", "beating", "all", "their", "alternatives", "for", "computing", "the", "MFS", "on", "the", "multilingual", "WSD", "tasks", ".", "We", "release", "our", "sense", "distributions", "in", "five", "different", "languages", "at", "https://github.com/SapienzaNLP/clubert", "."], "entities": [{"type": "Operation", "start": 113, "end": 114, "text": "CluBERT", "sent_idx": 4}, {"type": "Effect", "start": 130, "end": 131, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--369"}
{"text": "Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.", "tokens": ["Model", "ensemble", "techniques", "often", "increase", "task", "performance", "in", "neural", "networks", ";", "however", ",", "they", "require", "increased", "time", ",", "memory", ",", "and", "management", "effort", ".", "In", "this", "study", ",", "we", "propose", "a", "novel", "method", "that", "replicates", "the", "effects", "of", "a", "model", "ensemble", "with", "a", "single", "model", ".", "Our", "approach", "creates", "K-virtual", "models", "within", "a", "single", "parameter", "space", "using", "K-distinct", "pseudo-tags", "and", "K-distinct", "vectors", ".", "Experiments", "on", "text", "classification", "and", "sequence", "labeling", "tasks", "on", "several", "datasets", "demonstrate", "that", "our", "method", "emulates", "or", "outperforms", "a", "traditional", "model", "ensemble", "with", "1/K-times", "fewer", "parameters", "."], "entities": [{"type": "Operation", "start": 34, "end": 45, "text": "replicates the effects of a model ensemble with a single model", "sent_idx": 1}, {"type": "Effect", "start": 88, "end": 89, "text": "parameters", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--271"}
{"text": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.", "tokens": ["Implicit", "Event", "Argument", "Extraction", "seeks", "to", "identify", "arguments", "that", "play", "direct", "or", "implicit", "roles", "in", "a", "given", "event", ".", "However", ",", "most", "prior", "works", "focus", "on", "capturing", "direct", "relations", "between", "arguments", "and", "the", "event", "trigger", ".", "The", "lack", "of", "reasoning", "ability", "brings", "many", "challenges", "to", "the", "extraction", "of", "implicit", "arguments", ".", "In", "this", "work", ",", "we", "present", "a", "Frame-aware", "Event", "Argument", "Extraction", "(", "FEAE", ")", "learning", "framework", "to", "tackle", "this", "issue", "through", "reasoning", "in", "event", "frame-level", "scope", ".", "The", "proposed", "method", "leverages", "related", "arguments", "of", "the", "expected", "one", "as", "clues", "to", "guide", "the", "reasoning", "process", ".", "To", "bridge", "the", "gap", "between", "oracle", "knowledge", "used", "in", "the", "training", "phase", "and", "the", "imperfect", "related", "arguments", "in", "the", "test", "stage", ",", "we", "further", "introduce", "a", "curriculum", "knowledge", "distillation", "strategy", "to", "drive", "a", "final", "model", "that", "could", "operate", "without", "extra", "inputs", "through", "mimicking", "the", "behavior", "of", "a", "well-informed", "teacher", "model", ".", "Experimental", "results", "demonstrate", "FEAE", "obtains", "new", "state-of-the-art", "performance", "on", "the", "RAMS", "dataset", "."], "entities": [{"type": "Operation", "start": 150, "end": 151, "text": "FEAE", "sent_idx": 6}, {"type": "Effect", "start": 154, "end": 155, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--360"}
{"text": "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU.", "tokens": ["Existing", "work", "in", "multilingual", "pretraining", "has", "demonstrated", "the", "potential", "of", "cross-lingual", "transferability", "by", "training", "a", "unified", "Transformer", "encoder", "for", "multiple", "languages", ".", "However", ",", "much", "of", "this", "work", "only", "relies", "on", "the", "shared", "vocabulary", "and", "bilingual", "contexts", "to", "encourage", "the", "correlation", "across", "languages", ",", "which", "is", "loose", "and", "implicit", "for", "aligning", "the", "contextual", "representations", "between", "languages", ".", "In", "this", "paper", ",", "we", "plug", "a", "cross-attention", "module", "into", "the", "Transformer", "encoder", "to", "explicitly", "build", "the", "interdependence", "between", "languages", ".", "It", "can", "effectively", "avoid", "the", "degeneration", "of", "predicting", "masked", "words", "only", "conditioned", "on", "the", "context", "in", "its", "own", "language", ".", "More", "importantly", ",", "when", "fine-tuning", "on", "downstream", "tasks", ",", "the", "cross-attention", "module", "can", "be", "plugged", "in", "or", "out", "on-demand", ",", "thus", "naturally", "benefiting", "a", "wider", "range", "of", "cross-lingual", "tasks", ",", "from", "language", "understanding", "to", "generation", ".", "As", "a", "result", ",", "the", "proposed", "cross-lingual", "model", "delivers", "new", "state-of-the-art", "results", "on", "various", "cross-lingual", "understanding", "tasks", "of", "the", "XTREME", "benchmark", ",", "covering", "text", "classification", ",", "sequence", "labeling", ",", "question", "answering", ",", "and", "sentence", "retrieval", ".", "For", "cross-lingual", "generation", "tasks", ",", "it", "also", "outperforms", "all", "existing", "cross-lingual", "models", "and", "state-of-the-art", "Transformer", "variants", "on", "WMT14", "English-to-German", "and", "English-to-French", "translation", "datasets", ",", "with", "gains", "of", "up", "to", "1", "2", "BLEU", "."], "entities": [{"type": "Operation", "start": 62, "end": 70, "text": "plug a cross-attention module into the Transformer encoder", "sent_idx": 2}, {"type": "Effect", "start": 201, "end": 202, "text": "BLEU", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--308"}
{"text": "We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.", "tokens": ["We", "combine", "two", "complementary", "ideas", "for", "learning", "supertaggers", "from", "highly", "ambiguous", "lexicons", ":", "grammar-informed", "tag", "transitions", "and", "models", "minimized", "via", "integer", "programming", ".", "Each", "strategy", "on", "its", "own", "greatly", "improves", "performance", "over", "basic", "expectation-maximization", "training", "with", "a", "bitag", "Hidden", "Markov", "Model", ",", "which", "we", "show", "on", "the", "CCGbank", "and", "CCG-TUT", "corpora", ".", "The", "strategies", "provide", "further", "error", "reductions", "when", "combined", ".", "We", "describe", "a", "new", "two-stage", "integer", "programming", "strategy", "that", "efficiently", "deals", "with", "the", "high", "degree", "of", "ambiguity", "on", "these", "datasets", "while", "obtaining", "the", "full", "effect", "of", "model", "minimization", "."], "entities": [{"type": "Operation", "start": 13, "end": 22, "text": "grammar-informed tag transitions and models minimized via integer programming", "sent_idx": 0}, {"type": "Effect", "start": 56, "end": 57, "text": "error", "sent_idx": 2}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1051"}
{"text": "Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.", "tokens": ["Document-level", "contextual", "information", "has", "shown", "benefits", "to", "text-based", "machine", "translation", ",", "but", "whether", "and", "how", "context", "helps", "end-to-end", "(", "E2E", ")", "speech", "translation", "(", "ST", ")", "is", "still", "under-studied", ".", "We", "fill", "this", "gap", "through", "extensive", "experiments", "using", "a", "simple", "concatenation-based", "context-aware", "ST", "model", ",", "paired", "with", "adaptive", "feature", "selection", "on", "speech", "encodings", "for", "computational", "efficiency", ".", "We", "investigate", "several", "decoding", "approaches", ",", "and", "introduce", "in-model", "ensemble", "decoding", "which", "jointly", "performs", "document-", "and", "sentence-level", "translation", "using", "the", "same", "model", ".", "Our", "results", "on", "the", "MuST-C", "benchmark", "with", "Transformer", "demonstrate", "the", "effectiveness", "of", "context", "to", "E2E", "ST", ".", "Compared", "to", "sentence-level", "ST", ",", "context-aware", "ST", "obtains", "better", "translation", "quality", "(", "+", "0.18", "-", "2.61", "BLEU", ")", ",", "improves", "pronoun", "and", "homophone", "translation", ",", "shows", "better", "robustness", "to", "(", "artificial", ")", "audio", "segmentation", "errors", ",", "and", "reduces", "latency", "and", "flicker", "to", "deliver", "higher", "quality", "for", "simultaneous", "translation", "."], "entities": [{"type": "Operation", "start": 102, "end": 104, "text": "context-aware ST", "sent_idx": 4}, {"type": "Effect", "start": 106, "end": 108, "text": "translation quality", "sent_idx": 4}, {"type": "Effect", "start": 113, "end": 114, "text": "BLEU", "sent_idx": 4}, {"type": "Effect", "start": 124, "end": 125, "text": "robustness", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 132, "text": "errors", "sent_idx": 4}, {"type": "Effect", "start": 135, "end": 136, "text": "latency", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Neg_Affect", "head": 0, "tail": 4}, {"type": "Neg_Affect", "head": 0, "tail": 5}], "id": "abstract-2021--acl-long--200"}
{"text": "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one\u2019s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.", "tokens": ["Most", "of", "the", "recent", "work", "on", "personality", "detection", "from", "online", "posts", "adopts", "multifarious", "deep", "neural", "networks", "to", "represent", "the", "posts", "and", "builds", "predictive", "models", "in", "a", "data-driven", "manner", ",", "without", "the", "exploitation", "of", "psycholinguistic", "knowledge", "that", "may", "unveil", "the", "connections", "between", "one", "\u2019s", "language", "use", "and", "his", "psychological", "traits", ".", "In", "this", "paper", ",", "we", "propose", "a", "psycholinguistic", "knowledge-based", "tripartite", "graph", "network", ",", "TrigNet", ",", "which", "consists", "of", "a", "tripartite", "graph", "network", "and", "a", "BERT-based", "graph", "initializer", ".", "The", "graph", "network", "injects", "structural", "psycholinguistic", "knowledge", "in", "LIWC", ",", "a", "computerized", "instrument", "for", "psycholinguistic", "analysis", ",", "by", "constructing", "a", "heterogeneous", "tripartite", "graph", ".", "The", "initializer", "is", "employed", "to", "provide", "initial", "embeddings", "for", "the", "graph", "nodes", ".", "To", "reduce", "the", "computational", "cost", "in", "graph", "learning", ",", "we", "further", "propose", "a", "novel", "flow", "graph", "attention", "network", "(", "GAT", ")", "that", "only", "transmits", "messages", "between", "neighboring", "parties", "in", "the", "tripartite", "graph", ".", "Benefiting", "from", "the", "tripartite", "graph", ",", "TrigNet", "can", "aggregate", "post", "information", "from", "a", "psychological", "perspective", ",", "which", "is", "a", "novel", "way", "of", "exploiting", "domain", "knowledge", ".", "Extensive", "experiments", "on", "two", "datasets", "show", "that", "TrigNet", "outperforms", "the", "existing", "state-of-art", "model", "by", "3.47", "and", "2.10", "points", "in", "average", "F1", ".", "Moreover", ",", "the", "flow", "GAT", "reduces", "the", "FLOPS", "and", "Memory", "measures", "by", "38", "%", "and", "32", "%", ",", "respectively", ",", "in", "comparison", "to", "the", "original", "GAT", "in", "our", "setting", "."], "entities": [{"type": "Operation", "start": 181, "end": 182, "text": "TrigNet", "sent_idx": 6}, {"type": "Effect", "start": 193, "end": 195, "text": "average F1", "sent_idx": 6}, {"type": "Operation", "start": 199, "end": 201, "text": "flow GAT", "sent_idx": 7}, {"type": "Effect", "start": 203, "end": 204, "text": "FLOPS", "sent_idx": 7}, {"type": "Effect", "start": 205, "end": 206, "text": "Memory", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 3}, {"type": "Neg_Affect", "head": 2, "tail": 4}], "id": "abstract-2021--acl-long--326"}
{"text": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.", "tokens": ["Keyphrase", "generation", "(", "KG", ")", "aims", "to", "summarize", "the", "main", "ideas", "of", "a", "document", "into", "a", "set", "of", "keyphrases", ".", "A", "new", "setting", "is", "recently", "introduced", "into", "this", "problem", ",", "in", "which", ",", "given", "a", "document", ",", "the", "model", "needs", "to", "predict", "a", "set", "of", "keyphrases", "and", "simultaneously", "determine", "the", "appropriate", "number", "of", "keyphrases", "to", "produce", ".", "Previous", "work", "in", "this", "setting", "employs", "a", "sequential", "decoding", "process", "to", "generate", "keyphrases", ".", "However", ",", "such", "a", "decoding", "method", "ignores", "the", "intrinsic", "hierarchical", "compositionality", "existing", "in", "the", "keyphrase", "set", "of", "a", "document", ".", "Moreover", ",", "previous", "work", "tends", "to", "generate", "duplicated", "keyphrases", ",", "which", "wastes", "time", "and", "computing", "resources", ".", "To", "overcome", "these", "limitations", ",", "we", "propose", "an", "exclusive", "hierarchical", "decoding", "framework", "that", "includes", "a", "hierarchical", "decoding", "process", "and", "either", "a", "soft", "or", "a", "hard", "exclusion", "mechanism", ".", "The", "hierarchical", "decoding", "process", "is", "to", "explicitly", "model", "the", "hierarchical", "compositionality", "of", "a", "keyphrase", "set", ".", "Both", "the", "soft", "and", "the", "hard", "exclusion", "mechanisms", "keep", "track", "of", "previously-predicted", "keyphrases", "within", "a", "window", "size", "to", "enhance", "the", "diversity", "of", "the", "generated", "keyphrases", ".", "Extensive", "experiments", "on", "multiple", "KG", "benchmark", "datasets", "demonstrate", "the", "effectiveness", "of", "our", "method", "to", "generate", "less", "duplicated", "and", "more", "accurate", "keyphrases", "."], "entities": [{"type": "Operation", "start": 154, "end": 160, "text": "soft and the hard exclusion mechanisms", "sent_idx": 7}, {"type": "Effect", "start": 172, "end": 173, "text": "diversity", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--103"}
{"text": "Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models\u2019 predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in  https://github.com/rabeehk/robust-nli .", "tokens": ["Several", "recent", "studies", "have", "shown", "that", "strong", "natural", "language", "understanding", "(", "NLU", ")", "models", "are", "prone", "to", "relying", "on", "unwanted", "dataset", "biases", "without", "learning", "the", "underlying", "task", ",", "resulting", "in", "models", "that", "fail", "to", "generalize", "to", "out-of-domain", "datasets", "and", "are", "likely", "to", "perform", "poorly", "in", "real-world", "scenarios", ".", "We", "propose", "two", "learning", "strategies", "to", "train", "neural", "models", ",", "which", "are", "more", "robust", "to", "such", "biases", "and", "transfer", "better", "to", "out-of-domain", "datasets", ".", "The", "biases", "are", "specified", "in", "terms", "of", "one", "or", "more", "bias-only", "models", ",", "which", "learn", "to", "leverage", "the", "dataset", "biases", ".", "During", "training", ",", "the", "bias-only", "models", "\u2019", "predictions", "are", "used", "to", "adjust", "the", "loss", "of", "the", "base", "model", "to", "reduce", "its", "reliance", "on", "biases", "by", "down-weighting", "the", "biased", "examples", "and", "focusing", "the", "training", "on", "the", "hard", "examples", ".", "We", "experiment", "on", "large-scale", "natural", "language", "inference", "and", "fact", "verification", "benchmarks", ",", "evaluating", "on", "out-of-domain", "datasets", "that", "are", "specifically", "designed", "to", "assess", "the", "robustness", "of", "models", "against", "known", "biases", "in", "the", "training", "data", ".", "Results", "show", "that", "our", "debiasing", "methods", "greatly", "improve", "robustness", "in", "all", "settings", "and", "better", "transfer", "to", "other", "textual", "entailment", "datasets", ".", "Our", "code", "and", "data", "are", "publicly", "available", "in", " ", "https://github.com/rabeehk/robust-nli", "."], "entities": [{"type": "Operation", "start": 169, "end": 171, "text": "debiasing methods", "sent_idx": 5}, {"type": "Effect", "start": 173, "end": 174, "text": "robustness", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--769"}
{"text": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to \u201cinterpret\u201d these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3\u201320x less labeled data and improves on the baseline by 3\u201310 F1 points with the same amount of labeled data.", "tokens": ["Suppose", "we", "want", "to", "specify", "the", "inductive", "bias", "that", "married", "couples", "typically", "go", "on", "honeymoons", "for", "the", "task", "of", "extracting", "pairs", "of", "spouses", "from", "text", ".", "In", "this", "paper", ",", "we", "allow", "model", "developers", "to", "specify", "these", "types", "of", "inductive", "biases", "as", "natural", "language", "explanations", ".", "We", "use", "BERT", "fine-tuned", "on", "MultiNLI", "to", "\u201c", "interpret", "\u201d", "these", "explanations", "with", "respect", "to", "the", "input", "sentence", ",", "producing", "explanation-guided", "representations", "of", "the", "input", ".", "Across", "three", "relation", "extraction", "tasks", ",", "our", "method", ",", "ExpBERT", ",", "matches", "a", "BERT", "baseline", "but", "with", "3\u201320x", "less", "labeled", "data", "and", "improves", "on", "the", "baseline", "by", "3\u201310", "F1", "points", "with", "the", "same", "amount", "of", "labeled", "data", "."], "entities": [{"type": "Operation", "start": 81, "end": 82, "text": "ExpBERT", "sent_idx": 3}, {"type": "Effect", "start": 91, "end": 93, "text": "labeled data", "sent_idx": 3}, {"type": "Effect", "start": 100, "end": 102, "text": "F1 points", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--190"}
{"text": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.", "tokens": ["Although", "deep", "neural", "networks", "are", "effective", "at", "extracting", "high-level", "features", ",", "classification", "methods", "usually", "encode", "an", "input", "into", "a", "vector", "representation", "via", "simple", "feature", "aggregation", "operations", "(", "e.g.", "pooling", ")", ".", "Such", "operations", "limit", "the", "performance", ".", "For", "instance", ",", "a", "multi-label", "document", "may", "contain", "several", "concepts", ".", "In", "this", "case", ",", "one", "vector", "can", "not", "sufficiently", "capture", "its", "salient", "and", "discriminative", "content", ".", "Thus", ",", "we", "propose", "Hyperbolic", "Capsule", "Networks", "(", "HyperCaps", ")", "for", "Multi-Label", "Classification", "(", "MLC", ")", ",", "which", "have", "two", "merits", ".", "First", ",", "hyperbolic", "capsules", "are", "designed", "to", "capture", "fine-grained", "document", "information", "for", "each", "label", ",", "which", "has", "the", "ability", "to", "characterize", "complicated", "structures", "among", "labels", "and", "documents", ".", "Second", ",", "Hyperbolic", "Dynamic", "Routing", "(", "HDR", ")", "is", "introduced", "to", "aggregate", "hyperbolic", "capsules", "in", "a", "label-aware", "manner", ",", "so", "that", "the", "label-level", "discriminative", "information", "can", "be", "preserved", "along", "the", "depth", "of", "neural", "networks", ".", "To", "efficiently", "handle", "large-scale", "MLC", "datasets", ",", "we", "additionally", "present", "a", "new", "routing", "method", "to", "adaptively", "adjust", "the", "capsule", "number", "during", "routing", ".", "Extensive", "experiments", "are", "conducted", "on", "four", "benchmark", "datasets", ".", "Compared", "with", "the", "state-of-the-art", "methods", ",", "HyperCaps", "significantly", "improves", "the", "performance", "of", "MLC", "especially", "on", "tail", "labels", "."], "entities": [{"type": "Operation", "start": 187, "end": 188, "text": "HyperCaps", "sent_idx": 9}, {"type": "Effect", "start": 191, "end": 192, "text": "performance", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--283"}
{"text": "This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.", "tokens": ["This", "paper", "introduces", "the", "task", "of", "factual", "error", "correction", ":", "performing", "edits", "to", "a", "claim", "so", "that", "the", "generated", "rewrite", "is", "better", "supported", "by", "evidence", ".", "This", "extends", "the", "well-studied", "task", "of", "fact", "verification", "by", "providing", "a", "mechanism", "to", "correct", "written", "texts", "that", "are", "refuted", "or", "only", "partially", "supported", "by", "evidence", ".", "We", "demonstrate", "that", "it", "is", "feasible", "to", "train", "factual", "error", "correction", "systems", "from", "existing", "fact", "checking", "datasets", "which", "only", "contain", "labeled", "claims", "accompanied", "by", "evidence", ",", "but", "not", "the", "correction", ".", "We", "achieve", "this", "by", "employing", "a", "two-stage", "distant", "supervision", "approach", "that", "incorporates", "evidence", "into", "masked", "claims", "when", "generating", "corrections", ".", "Our", "approach", ",", "based", "on", "the", "T5", "transformer", "and", "using", "retrieved", "evidence", ",", "achieved", "better", "results", "than", "existing", "work", "which", "used", "a", "pointer", "copy", "network", "and", "gold", "evidence", ",", "producing", "accurate", "factual", "error", "corrections", "for", "5x", "more", "instances", "in", "human", "evaluation", "and", "a", ".125", "increase", "in", "SARI", "score", ".", "The", "evaluation", "is", "conducted", "on", "a", "dataset", "of", "65,000", "instances", "based", "on", "a", "recent", "fact", "verification", "shared", "task", "and", "we", "release", "it", "to", "enable", "further", "work", "on", "the", "task", "."], "entities": [{"type": "Operation", "start": 112, "end": 115, "text": "using retrieved evidence", "sent_idx": 4}, {"type": "Effect", "start": 149, "end": 151, "text": "SARI score", "sent_idx": 4}, {"type": "Operation", "start": 106, "end": 111, "text": "based on the T5 transformer", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--256"}
{"text": "Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.", "tokens": ["Crowdsourcing", "is", "regarded", "as", "one", "prospective", "solution", "for", "effective", "supervised", "learning", ",", "aiming", "to", "build", "large-scale", "annotated", "training", "data", "by", "crowd", "workers", ".", "Previous", "studies", "focus", "on", "reducing", "the", "influences", "from", "the", "noises", "of", "the", "crowdsourced", "annotations", "for", "supervised", "models", ".", "We", "take", "a", "different", "point", "in", "this", "work", ",", "regarding", "all", "crowdsourced", "annotations", "as", "gold-standard", "with", "respect", "to", "the", "individual", "annotators", ".", "In", "this", "way", ",", "we", "find", "that", "crowdsourcing", "could", "be", "highly", "similar", "to", "domain", "adaptation", ",", "and", "then", "the", "recent", "advances", "of", "cross-domain", "methods", "can", "be", "almost", "directly", "applied", "to", "crowdsourcing", ".", "Here", "we", "take", "named", "entity", "recognition", "(", "NER", ")", "as", "a", "study", "case", ",", "suggesting", "an", "annotator-aware", "representation", "learning", "model", "that", "inspired", "by", "the", "domain", "adaptation", "methods", "which", "attempt", "to", "capture", "effective", "domain-aware", "features", ".", "We", "investigate", "both", "unsupervised", "and", "supervised", "crowdsourcing", "learning", ",", "assuming", "that", "no", "or", "only", "small-scale", "expert", "annotations", "are", "available", ".", "Experimental", "results", "on", "a", "benchmark", "crowdsourced", "NER", "dataset", "show", "that", "our", "method", "is", "highly", "effective", ",", "leading", "to", "a", "new", "state-of-the-art", "performance", ".", "In", "addition", ",", "under", "the", "supervised", "setting", ",", "we", "can", "achieve", "impressive", "performance", "gains", "with", "only", "a", "very", "small", "scale", "of", "expert", "annotations", "."], "entities": [{"type": "Operation", "start": 133, "end": 138, "text": "unsupervised and supervised crowdsourcing learning", "sent_idx": 5}, {"type": "Effect", "start": 171, "end": 172, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--432"}
{"text": "We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical 'jargon' names of the domain entities. In such cases, dialogue systems must be able to model the user's (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user's domain expertise.", "tokens": ["We", "present", "a", "data-driven", "approach", "to", "learn", "user-adaptive", "referring", "expression", "generation", "(", "REG", ")", "policies", "for", "spoken", "dialogue", "systems", ".", "Referring", "expressions", "can", "be", "difficult", "to", "understand", "in", "technical", "domains", "where", "users", "may", "not", "know", "the", "technical", "'", "jargon", "'", "names", "of", "the", "domain", "entities", ".", "In", "such", "cases", ",", "dialogue", "systems", "must", "be", "able", "to", "model", "the", "user", "'s", "(", "lexical", ")", "domain", "knowledge", "and", "use", "appropriate", "referring", "expressions", ".", "We", "present", "a", "reinforcement", "learning", "(", "RL", ")", "framework", "in", "which", "the", "system", "learns", "REG", "policies", "which", "can", "adapt", "to", "unknown", "users", "online", ".", "Furthermore", ",", "unlike", "supervised", "learning", "methods", "which", "require", "a", "large", "corpus", "of", "expert", "adaptive", "behaviour", "to", "train", "on", ",", "we", "show", "that", "effective", "adaptive", "policies", "can", "be", "learned", "from", "a", "small", "dialogue", "corpus", "of", "non-adaptive", "human-machine", "interaction", ",", "by", "using", "a", "RL", "framework", "and", "a", "statistical", "user", "simulation", ".", "We", "show", "that", "in", "comparison", "to", "adaptive", "hand-coded", "baseline", "policies", ",", "the", "learned", "policy", "performs", "significantly", "better", ",", "with", "an", "18.6", "%", "average", "increase", "in", "adaptation", "accuracy", ".", "The", "best", "learned", "policy", "also", "takes", "less", "dialogue", "time", "(", "average", "1.07", "min", "less", ")", "than", "the", "best", "hand-coded", "policy", ".", "This", "is", "because", "the", "learned", "policies", "can", "adapt", "online", "to", "changing", "evidence", "about", "the", "user", "'s", "domain", "expertise", "."], "entities": [{"type": "Operation", "start": 74, "end": 80, "text": "reinforcement learning (RL) framework", "sent_idx": 3}, {"type": "Effect", "start": 179, "end": 181, "text": "dialogue time", "sent_idx": 6}, {"type": "Effect", "start": 169, "end": 171, "text": "adaptation accuracy", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P10-1008"}
{"text": "Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ traces to improve generation quality and controllability is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task. Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.", "tokens": ["Generating", "image", "captions", "with", "user", "intention", "is", "an", "emerging", "need", ".", "The", "recently", "published", "Localized", "Narratives", "dataset", "takes", "mouse", "traces", "as", "another", "input", "to", "the", "image", "captioning", "task", ",", "which", "is", "an", "intuitive", "and", "efficient", "way", "for", "a", "user", "to", "control", "what", "to", "describe", "in", "the", "image", ".", "However", ",", "how", "to", "effectively", "employ", "traces", "to", "improve", "generation", "quality", "and", "controllability", "is", "still", "under", "exploration", ".", "This", "paper", "aims", "to", "solve", "this", "problem", "by", "proposing", "a", "novel", "model", "called", "LoopCAG", ",", "which", "connects", "Contrastive", "constraints", "and", "Attention", "Guidance", "in", "a", "Loop", "manner", ",", "engaged", "explicit", "spatial", "and", "temporal", "constraints", "to", "the", "generating", "process", ".", "Precisely", ",", "each", "generated", "sentence", "is", "temporally", "aligned", "to", "the", "corresponding", "trace", "sequence", "through", "a", "contrastive", "learning", "strategy", ".", "Besides", ",", "each", "generated", "text", "token", "is", "supervised", "to", "attend", "to", "the", "correct", "visual", "objects", "under", "heuristic", "spatial", "attention", "guidance", ".", "Comprehensive", "experimental", "results", "demonstrate", "that", "our", "LoopCAG", "model", "learns", "better", "correspondence", "among", "the", "three", "modalities", "(", "vision", ",", "language", ",", "and", "traces", ")", "and", "achieves", "SOTA", "performance", "on", "trace-controlled", "image", "captioning", "task", ".", "Moreover", ",", "the", "controllability", "and", "explainability", "of", "LoopCAG", "are", "validated", "by", "analyzing", "spatial", "and", "temporal", "sensitivity", "during", "the", "generation", "process", "."], "entities": [{"type": "Operation", "start": 150, "end": 152, "text": "LoopCAG model", "sent_idx": 6}, {"type": "Effect", "start": 170, "end": 171, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--157"}
{"text": "The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of  both  multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.", "tokens": ["The", "task", "of", "Dialogue", "Act", "Classification", "(", "DAC", ")", "that", "purports", "to", "capture", "communicative", "intent", "has", "been", "studied", "extensively", ".", "But", "these", "studies", "limit", "themselves", "to", "text", ".", "Non-verbal", "features", "(", "change", "of", "tone", ",", "facial", "expressions", "etc", ".", ")", "can", "provide", "cues", "to", "identify", "DAs", ",", "thus", "stressing", "the", "benefit", "of", "incorporating", "multi-modal", "inputs", "in", "the", "task", ".", "Also", ",", "the", "emotional", "state", "of", "the", "speaker", "has", "a", "substantial", "effect", "on", "the", "choice", "of", "the", "dialogue", "act", ",", "since", "conversations", "are", "often", "influenced", "by", "emotions", ".", "Hence", ",", "the", "effect", "of", "emotion", "too", "on", "automatic", "identification", "of", "DAs", "needs", "to", "be", "studied", ".", "In", "this", "work", ",", "we", "address", "the", "role", "of", " ", "both", " ", "multi-modality", "and", "emotion", "recognition", "(", "ER", ")", "in", "DAC", ".", "DAC", "and", "ER", "help", "each", "other", "by", "way", "of", "multi-task", "learning", ".", "One", "of", "the", "major", "contributions", "of", "this", "work", "is", "a", "new", "dataset-", "multimodal", "Emotion", "aware", "Dialogue", "Act", "dataset", "called", "EMOTyDA", ",", "collected", "from", "open-sourced", "dialogue", "datasets", ".", "To", "demonstrate", "the", "utility", "of", "EMOTyDA", ",", "we", "build", "an", "attention", "based", "(", "self", ",", "inter-modal", ",", "inter-task", ")", "multi-modal", ",", "multi-task", "Deep", "Neural", "Network", "(", "DNN", ")", "for", "joint", "learning", "of", "DAs", "and", "emotions", ".", "We", "show", "empirically", "that", "multi-modality", "and", "multi-tasking", "achieve", "better", "performance", "of", "DAC", "compared", "to", "uni-modal", "and", "single", "task", "DAC", "variants", "."], "entities": [{"type": "Operation", "start": 207, "end": 208, "text": "multi-tasking", "sent_idx": 9}, {"type": "Effect", "start": 210, "end": 211, "text": "performance", "sent_idx": 9}, {"type": "Operation", "start": 205, "end": 206, "text": "multi-modality", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--402"}
{"text": "Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.", "tokens": ["Semantic", "similarity", "detection", "is", "a", "fundamental", "task", "in", "natural", "language", "understanding", ".", "Adding", "topic", "information", "has", "been", "useful", "for", "previous", "feature-engineered", "semantic", "similarity", "models", "as", "well", "as", "neural", "models", "for", "other", "tasks", ".", "There", "is", "currently", "no", "standard", "way", "of", "combining", "topics", "with", "pretrained", "contextual", "representations", "such", "as", "BERT", ".", "We", "propose", "a", "novel", "topic-informed", "BERT-based", "architecture", "for", "pairwise", "semantic", "similarity", "detection", "and", "show", "that", "our", "model", "improves", "performance", "over", "strong", "neural", "baselines", "across", "a", "variety", "of", "English", "language", "datasets", ".", "We", "find", "that", "the", "addition", "of", "topics", "to", "BERT", "helps", "particularly", "with", "resolving", "domain-specific", "cases", "."], "entities": [{"type": "Operation", "start": 54, "end": 57, "text": "topic-informed BERT-based architecture", "sent_idx": 3}, {"type": "Effect", "start": 68, "end": 69, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--630"}
{"text": "We present a method for automatically generating focused and accurate topic-specific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system.", "tokens": ["We", "present", "a", "method", "for", "automatically", "generating", "focused", "and", "accurate", "topic-specific", "subjectivity", "lexicons", "from", "a", "general", "purpose", "polarity", "lexicon", "that", "allow", "users", "to", "pin-point", "subjective", "on-topic", "information", "in", "a", "set", "of", "relevant", "documents", ".", "We", "motivate", "the", "need", "for", "such", "lexicons", "in", "the", "field", "of", "media", "analysis", ",", "describe", "a", "bootstrapping", "method", "for", "generating", "a", "topic-specific", "lexicon", "from", "a", "general", "purpose", "polarity", "lexicon", ",", "and", "evaluate", "the", "quality", "of", "the", "generated", "lexicons", "both", "manually", "and", "using", "a", "TREC", "Blog", "track", "test", "set", "for", "opinionated", "blog", "post", "retrieval", ".", "Although", "the", "generated", "lexicons", "can", "be", "an", "order", "of", "magnitude", "more", "selective", "than", "the", "general", "purpose", "lexicon", ",", "they", "maintain", ",", "or", "even", "improve", ",", "the", "performance", "of", "an", "opinion", "retrieval", "system", "."], "entities": [{"type": "Operation", "start": 90, "end": 92, "text": "generated lexicons", "sent_idx": 2}, {"type": "Effect", "start": 114, "end": 115, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1060"}
{"text": "Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.", "tokens": ["Intelligent", "features", "in", "email", "service", "applications", "aim", "to", "increase", "productivity", "by", "helping", "people", "organize", "their", "folders", ",", "compose", "their", "emails", "and", "respond", "to", "pending", "tasks", ".", "In", "this", "work", ",", "we", "explore", "a", "new", "application", ",", "Smart-To-Do", ",", "that", "helps", "users", "with", "task", "management", "over", "emails", ".", "We", "introduce", "a", "new", "task", "and", "dataset", "for", "automatically", "generating", "To-Do", "items", "from", "emails", "where", "the", "sender", "has", "promised", "to", "perform", "an", "action", ".", "We", "design", "a", "two-stage", "process", "leveraging", "recent", "advances", "in", "neural", "text", "generation", "and", "sequence-to-sequence", "learning", ",", "obtaining", "BLEU", "and", "ROUGE", "scores", "of", "0.23", "and", "0.63", "for", "this", "task", ".", "To", "the", "best", "of", "our", "knowledge", ",", "this", "is", "the", "first", "work", "to", "address", "the", "problem", "of", "composing", "To-Do", "items", "from", "emails", "."], "entities": [{"type": "Operation", "start": 76, "end": 86, "text": "leveraging recent advances in neural text generation and sequence-to-sequence learning", "sent_idx": 3}, {"type": "Effect", "start": 88, "end": 89, "text": "BLEU", "sent_idx": 3}, {"type": "Effect", "start": 90, "end": 92, "text": "ROUGE scores", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--767"}
{"text": "Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.", "tokens": ["Neural", "language", "models", "are", "usually", "trained", "to", "match", "the", "distributional", "properties", "of", "large-scale", "corpora", "by", "minimizing", "the", "log", "loss", ".", "While", "straightforward", "to", "optimize", ",", "this", "approach", "forces", "the", "model", "to", "reproduce", "all", "variations", "in", "the", "dataset", ",", "including", "noisy", "and", "invalid", "references", "(", "e.g.", ",", "misannotations", "and", "hallucinated", "facts", ")", ".", "Even", "a", "small", "fraction", "of", "noisy", "data", "can", "degrade", "the", "performance", "of", "log", "loss", ".", "As", "an", "alternative", ",", "prior", "work", "has", "shown", "that", "minimizing", "the", "distinguishability", "of", "generated", "samples", "is", "a", "principled", "and", "robust", "loss", "that", "can", "handle", "invalid", "references", ".", "However", ",", "distinguishability", "has", "not", "been", "used", "in", "practice", "due", "to", "challenges", "in", "optimization", "and", "estimation", ".", "We", "propose", "loss", "truncation", ":", "a", "simple", "and", "scalable", "procedure", "which", "adaptively", "removes", "high", "log", "loss", "examples", "as", "a", "way", "to", "optimize", "for", "distinguishability", ".", "Empirically", ",", "we", "demonstrate", "that", "loss", "truncation", "outperforms", "existing", "baselines", "on", "distinguishability", "on", "a", "summarization", "task", ".", "Furthermore", ",", "we", "show", "that", "samples", "generated", "by", "the", "loss", "truncation", "model", "have", "factual", "accuracy", "ratings", "that", "exceed", "those", "of", "baselines", "and", "match", "human", "references", "."], "entities": [{"type": "Operation", "start": 141, "end": 143, "text": "loss truncation", "sent_idx": 6}, {"type": "Effect", "start": 147, "end": 148, "text": "distinguishability", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--66"}
{"text": "Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99% evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount.", "tokens": ["Conversational", "dialogue", "systems", "(", "CDSs", ")", "are", "hard", "to", "evaluate", "due", "to", "the", "complexity", "of", "natural", "language", ".", "Automatic", "evaluation", "of", "dialogues", "often", "shows", "insufficient", "correlation", "with", "human", "judgements", ".", "Human", "evaluation", "is", "reliable", "but", "labor-intensive", ".", "We", "introduce", "a", "human-machine", "collaborative", "framework", ",", "HMCEval", ",", "that", "can", "guarantee", "reliability", "of", "the", "evaluation", "outcomes", "with", "reduced", "human", "effort", ".", "HMCEval", "casts", "dialogue", "evaluation", "as", "a", "sample", "assignment", "problem", ",", "where", "we", "need", "to", "decide", "to", "assign", "a", "sample", "to", "a", "human", "or", "a", "machine", "for", "evaluation", ".", "HMCEval", "includes", "a", "model", "confidence", "estimation", "module", "to", "estimate", "the", "confidence", "of", "the", "predicted", "sample", "assignment", ",", "and", "a", "human", "effort", "estimation", "module", "to", "estimate", "the", "human", "effort", "should", "the", "sample", "be", "assigned", "to", "human", "evaluation", ",", "as", "well", "as", "a", "sample", "assignment", "execution", "module", "that", "finds", "the", "optimum", "assignment", "solution", "based", "on", "the", "estimated", "confidence", "and", "effort", ".", "We", "assess", "the", "performance", "of", "HMCEval", "on", "the", "task", "of", "evaluating", "malevolence", "in", "dialogues", ".", "The", "experimental", "results", "show", "that", "HMCEval", "achieves", "around", "99", "%", "evaluation", "accuracy", "with", "half", "of", "the", "human", "effort", "spared", ",", "showing", "that", "HMCEval", "provides", "reliable", "evaluation", "outcomes", "while", "reducing", "human", "effort", "by", "a", "large", "amount", "."], "entities": [{"type": "Operation", "start": 166, "end": 167, "text": "HMCEval", "sent_idx": 7}, {"type": "Effect", "start": 172, "end": 173, "text": "accuracy", "sent_idx": 7}, {"type": "Effect", "start": 177, "end": 180, "text": "human effort spared", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--436"}
{"text": "We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is statistically significant.", "tokens": ["We", "propose", "a", "novel", "technique", "of", "learning", "how", "to", "transform", "the", "source", "parse", "trees", "to", "improve", "the", "translation", "qualities", "of", "syntax-based", "translation", "models", "using", "synchronous", "context-free", "grammars", ".", "We", "transform", "the", "source", "tree", "phrasal", "structure", "into", "a", "set", "of", "simpler", "structures", ",", "expose", "such", "decisions", "to", "the", "decoding", "process", ",", "and", "find", "the", "least", "expensive", "transformation", "operation", "to", "better", "model", "word", "reordering", ".", "In", "particular", ",", "we", "integrate", "synchronous", "binarizations", ",", "verb", "regrouping", ",", "removal", "of", "redundant", "parse", "nodes", ",", "and", "incorporate", "a", "few", "important", "features", "such", "as", "translation", "boundaries", ".", "We", "learn", "the", "structural", "preferences", "from", "the", "data", "in", "a", "generative", "framework", ".", "The", "syntax-based", "translation", "system", "integrating", "the", "proposed", "techniques", "outperforms", "the", "best", "Arabic-English", "unconstrained", "system", "in", "NIST-08", "evaluations", "by", "1.3", "absolute", "BLEU", ",", "which", "is", "statistically", "significant", "."], "entities": [{"type": "Operation", "start": 29, "end": 41, "text": "transform the source tree phrasal structure into a set of simpler structures", "sent_idx": 1}, {"type": "Effect", "start": 124, "end": 125, "text": "BLEU", "sent_idx": 4}, {"type": "Operation", "start": 104, "end": 112, "text": "The syntax-based translation system integrating the proposed techniques", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P11-1085"}
{"text": "Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the \u201cfuture\u201d information, to guide question generation. However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.", "tokens": ["Generating", "some", "appealing", "questions", "in", "open-domain", "conversations", "is", "an", "effective", "way", "to", "improve", "human-machine", "interactions", "and", "lead", "the", "topic", "to", "a", "broader", "or", "deeper", "direction", ".", "To", "avoid", "dull", "or", "deviated", "questions", ",", "some", "researchers", "tried", "to", "utilize", "answer", ",", "the", "\u201c", "future", "\u201d", "information", ",", "to", "guide", "question", "generation", ".", "However", ",", "they", "separate", "a", "post-question-answer", "(", "PQA", ")", "triple", "into", "two", "parts", ":", "post-question", "(", "PQ", ")", "and", "question-answer", "(", "QA", ")", "pairs", ",", "which", "may", "hurt", "the", "overall", "coherence", ".", "Besides", ",", "the", "QA", "relationship", "is", "modeled", "as", "a", "one-to-one", "mapping", "that", "is", "not", "reasonable", "in", "open-domain", "conversations", ".", "To", "tackle", "these", "problems", ",", "we", "propose", "a", "generative", "triple-wise", "model", "with", "hierarchical", "variations", "for", "open-domain", "conversational", "question", "generation", "(", "CQG", ")", ".", "Latent", "variables", "in", "three", "hierarchies", "are", "used", "to", "represent", "the", "shared", "background", "of", "a", "triple", "and", "one-to-many", "semantic", "mappings", "in", "both", "PQ", "and", "QA", "pairs", ".", "Experimental", "results", "on", "a", "large-scale", "CQG", "dataset", "show", "that", "our", "method", "significantly", "improves", "the", "quality", "of", "questions", "in", "terms", "of", "fluency", ",", "coherence", "and", "diversity", "over", "competitive", "baselines", "."], "entities": [{"type": "Operation", "start": 110, "end": 116, "text": "generative triple-wise model with hierarchical variations", "sent_idx": 4}, {"type": "Effect", "start": 171, "end": 172, "text": "fluency", "sent_idx": 6}, {"type": "Effect", "start": 173, "end": 174, "text": "coherence", "sent_idx": 6}, {"type": "Effect", "start": 175, "end": 176, "text": "diversity", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--271"}
{"text": "Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies\u2014top-_k_, nucleus sampling, and untruncated random sampling\u2014and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.", "tokens": ["Recent", "advancements", "in", "neural", "language", "modelling", "make", "it", "possible", "to", "rapidly", "generate", "vast", "amounts", "of", "human-sounding", "text", ".", "The", "capabilities", "of", "humans", "and", "automatic", "discriminators", "to", "detect", "machine-generated", "text", "have", "been", "a", "large", "source", "of", "research", "interest", ",", "but", "humans", "and", "machines", "rely", "on", "different", "cues", "to", "make", "their", "decisions", ".", "Here", ",", "we", "perform", "careful", "benchmarking", "and", "analysis", "of", "three", "popular", "sampling-based", "decoding", "strategies", "\u2014", "top-_k", "_", ",", "nucleus", "sampling", ",", "and", "untruncated", "random", "sampling", "\u2014", "and", "show", "that", "improvements", "in", "decoding", "methods", "have", "primarily", "optimized", "for", "fooling", "humans", ".", "This", "comes", "at", "the", "expense", "of", "introducing", "statistical", "abnormalities", "that", "make", "detection", "easy", "for", "automatic", "systems", ".", "We", "also", "show", "that", "though", "both", "human", "and", "automatic", "detector", "performance", "improve", "with", "longer", "excerpt", "length", ",", "even", "multi-sentence", "excerpts", "can", "fool", "expert", "human", "raters", "over", "30", "%", "of", "the", "time", ".", "Our", "findings", "reveal", "the", "importance", "of", "using", "both", "human", "and", "automatic", "detectors", "to", "assess", "the", "humanness", "of", "text", "generation", "systems", "."], "entities": [{"type": "Operation", "start": 121, "end": 124, "text": "longer excerpt length", "sent_idx": 4}, {"type": "Effect", "start": 118, "end": 119, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--164"}
{"text": "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.", "tokens": ["Incremental", "parsing", "techniques", "such", "as", "shift-reduce", "have", "gained", "popularity", "thanks", "to", "their", "efficiency", ",", "but", "there", "remains", "a", "major", "problem", ":", "the", "search", "is", "greedy", "and", "only", "explores", "a", "tiny", "fraction", "of", "the", "whole", "space", "(", "even", "with", "beam", "search", ")", "as", "opposed", "to", "dynamic", "programming", ".", "We", "show", "that", ",", "surprisingly", ",", "dynamic", "programming", "is", "in", "fact", "possible", "for", "many", "shift-reduce", "parsers", ",", "by", "merging", "\"", "equivalent", "\"", "stacks", "based", "on", "feature", "values", ".", "Empirically", ",", "our", "algorithm", "yields", "up", "to", "a", "five-fold", "speedup", "over", "a", "state-of-the-art", "shift-reduce", "dependency", "parser", "with", "no", "loss", "in", "accuracy", ".", "Better", "search", "also", "leads", "to", "better", "learning", ",", "and", "our", "final", "parser", "outperforms", "all", "previously", "reported", "dependency", "parsers", "for", "English", "and", "Chinese", ",", "yet", "is", "much", "faster", "."], "entities": [{"type": "Operation", "start": 65, "end": 74, "text": "merging \"equivalent\" stacks based on feature values", "sent_idx": 1}, {"type": "Effect", "start": 95, "end": 96, "text": "accuracy", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1110"}
{"text": "Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.", "tokens": ["Pre-trained", "language", "models", "like", "BERT", "have", "proven", "to", "be", "highly", "performant", ".", "However", ",", "they", "are", "often", "computationally", "expensive", "in", "many", "practical", "scenarios", ",", "for", "such", "heavy", "models", "can", "hardly", "be", "readily", "implemented", "with", "limited", "resources", ".", "To", "improve", "their", "efficiency", "with", "an", "assured", "model", "performance", ",", "we", "propose", "a", "novel", "speed-tunable", "FastBERT", "with", "adaptive", "inference", "time", ".", "The", "speed", "at", "inference", "can", "be", "flexibly", "adjusted", "under", "varying", "demands", ",", "while", "redundant", "calculation", "of", "samples", "is", "avoided", ".", "Moreover", ",", "this", "model", "adopts", "a", "unique", "self-distillation", "mechanism", "at", "fine-tuning", ",", "further", "enabling", "a", "greater", "computational", "efficacy", "with", "minimal", "loss", "in", "performance", ".", "Our", "model", "achieves", "promising", "results", "in", "twelve", "English", "and", "Chinese", "datasets", ".", "It", "is", "able", "to", "speed", "up", "by", "a", "wide", "range", "from", "1", "to", "12", "times", "than", "BERT", "if", "given", "different", "speedup", "thresholds", "to", "make", "a", "speed-performance", "tradeoff", "."], "entities": [{"type": "Operation", "start": 52, "end": 53, "text": "FastBERT", "sent_idx": 2}, {"type": "Effect", "start": 118, "end": 119, "text": "speed", "sent_idx": 6}, {"type": "Operation", "start": 82, "end": 87, "text": "adopts a unique self-distillation mechanism", "sent_idx": 4}, {"type": "Effect", "start": 94, "end": 96, "text": "computational efficacy", "sent_idx": 4}, {"type": "Effect", "start": 100, "end": 101, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Neg_Affect", "head": 2, "tail": 4}], "id": "abstract-2020--acl-main--537"}
{"text": "Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present c ascaded head-c o lli d ing a ttention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.", "tokens": ["Transformers", "have", "advanced", "the", "field", "of", "natural", "language", "processing", "(", "NLP", ")", "on", "a", "variety", "of", "important", "tasks", ".", "At", "the", "cornerstone", "of", "the", "Transformer", "architecture", "is", "the", "multi-head", "attention", "(", "MHA", ")", "mechanism", "which", "models", "pairwise", "interactions", "between", "the", "elements", "of", "the", "sequence", ".", "Despite", "its", "massive", "success", ",", "the", "current", "framework", "ignores", "interactions", "among", "different", "heads", ",", "leading", "to", "the", "problem", "that", "many", "of", "the", "heads", "are", "redundant", "in", "practice", ",", "which", "greatly", "wastes", "the", "capacity", "of", "the", "model", ".", "To", "improve", "parameter", "efficiency", ",", "we", "re-formulate", "the", "MHA", "as", "a", "latent", "variable", "model", "from", "a", "probabilistic", "perspective", ".", "We", "present", "c", "ascaded", "head-c", "o", "lli", "d", "ing", "a", "ttention", "(", "CODA", ")", "which", "explicitly", "models", "the", "interactions", "between", "attention", "heads", "through", "a", "hierarchical", "variational", "distribution", ".", "We", "conduct", "extensive", "experiments", "and", "demonstrate", "that", "CODA", "outperforms", "the", "transformer", "baseline", ",", "by", "0.6", "perplexity", "on", "Wikitext-103", "in", "language", "modeling", ",", "and", "by", "0.6", "BLEU", "on", "WMT14", "EN-DE", "in", "machine", "translation", ",", "due", "to", "its", "improvements", "on", "the", "parameter", "efficiency", "."], "entities": [{"type": "Operation", "start": 136, "end": 137, "text": "CODA", "sent_idx": 5}, {"type": "Effect", "start": 144, "end": 145, "text": "perplexity", "sent_idx": 5}, {"type": "Effect", "start": 154, "end": 155, "text": "BLEU", "sent_idx": 5}, {"type": "Effect", "start": 168, "end": 170, "text": "parameter efficiency", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--45"}
{"text": "The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples. Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances. Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5\u00d710\u02c64 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model.", "tokens": ["The", "importance", "of", "explaining", "the", "outcome", "of", "a", "machine", "learning", "model", ",", "especially", "a", "black-box", "model", ",", "is", "widely", "acknowledged", ".", "Recent", "approaches", "explain", "an", "outcome", "by", "identifying", "the", "contributions", "of", "input", "features", "to", "this", "outcome", ".", "In", "environments", "involving", "large", "black-box", "models", "or", "complex", "inputs", ",", "this", "leads", "to", "computationally", "demanding", "algorithms", ".", "Further", ",", "these", "algorithms", "often", "suffer", "from", "low", "stability", ",", "with", "explanations", "varying", "significantly", "across", "similar", "examples", ".", "In", "this", "paper", ",", "we", "propose", "a", "Learning", "to", "Explain", "(", "L2E", ")", "approach", "that", "learns", "the", "behaviour", "of", "an", "underlying", "explanation", "algorithm", "simultaneously", "from", "all", "training", "examples", ".", "Once", "the", "explanation", "algorithm", "is", "distilled", "into", "an", "explainer", "network", ",", "it", "can", "be", "used", "to", "explain", "new", "instances", ".", "Our", "experiments", "on", "three", "classification", "tasks", ",", "which", "compare", "our", "approach", "to", "six", "explanation", "algorithms", ",", "show", "that", "L2E", "is", "between", "5", "and", "7.5", "\u00d7", "10\u02c64", "times", "faster", "than", "these", "algorithms", ",", "while", "generating", "more", "stable", "explanations", ",", "and", "having", "comparable", "faithfulness", "to", "the", "black-box", "model", "."], "entities": [{"type": "Operation", "start": 139, "end": 140, "text": "L2E", "sent_idx": 6}, {"type": "Effect", "start": 162, "end": 163, "text": "faithfulness", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--415"}
{"text": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.", "tokens": ["Recently", ",", "many", "methods", "discover", "effective", "evidence", "from", "reliable", "sources", "by", "appropriate", "neural", "networks", "for", "explainable", "claim", "verification", ",", "which", "has", "been", "widely", "recognized", ".", "However", ",", "in", "these", "methods", ",", "the", "discovery", "process", "of", "evidence", "is", "nontransparent", "and", "unexplained", ".", "Simultaneously", ",", "the", "discovered", "evidence", "is", "aimed", "at", "the", "interpretability", "of", "the", "whole", "sequence", "of", "claims", "but", "insufficient", "to", "focus", "on", "the", "false", "parts", "of", "claims", ".", "In", "this", "paper", ",", "we", "propose", "a", "Decision", "Tree-based", "Co-Attention", "model", "(", "DTCA", ")", "to", "discover", "evidence", "for", "explainable", "claim", "verification", ".", "Specifically", ",", "we", "first", "construct", "Decision", "Tree-based", "Evidence", "model", "(", "DTE", ")", "to", "select", "comments", "with", "high", "credibility", "as", "evidence", "in", "a", "transparent", "and", "interpretable", "way", ".", "Then", "we", "design", "Co-attention", "Self-attention", "networks", "(", "CaSa", ")", "to", "make", "the", "selected", "evidence", "interact", "with", "claims", ",", "which", "is", "for", "1", ")", "training", "DTE", "to", "determine", "the", "optimal", "decision", "thresholds", "and", "obtain", "more", "powerful", "evidence", ";", "and", "2", ")", "utilizing", "the", "evidence", "to", "find", "the", "false", "parts", "in", "the", "claim", ".", "Experiments", "on", "two", "public", "datasets", ",", "RumourEval", "and", "PHEME", ",", "demonstrate", "that", "DTCA", "not", "only", "provides", "explanations", "for", "the", "results", "of", "claim", "verification", "but", "also", "achieves", "the", "state-of-the-art", "performance", ",", "boosting", "the", "F1-score", "by", "more", "than", "3.11", "%", ",", "2.41", "%", ",", "respectively", "."], "entities": [{"type": "Operation", "start": 181, "end": 182, "text": "DTCA", "sent_idx": 6}, {"type": "Effect", "start": 197, "end": 198, "text": "performance", "sent_idx": 6}, {"type": "Effect", "start": 201, "end": 202, "text": "F1-score", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--97"}
{"text": "In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER. (2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness. For further research, the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA .", "tokens": ["In", "this", "paper", ",", "we", "aim", "to", "explore", "an", "uncharted", "territory", ",", "which", "is", "Chinese", "multimodal", "named", "entity", "recognition", "(", "NER", ")", "with", "both", "textual", "and", "acoustic", "contents", ".", "To", "achieve", "this", ",", "we", "construct", "a", "large-scale", "human-annotated", "Chinese", "multimodal", "NER", "dataset", ",", "named", "CNERTA", ".", "Our", "corpus", "totally", "contains", "42,987", "annotated", "sentences", "accompanying", "by", "71", "hours", "of", "speech", "data", ".", "Based", "on", "this", "dataset", ",", "we", "propose", "a", "family", "of", "strong", "and", "representative", "baseline", "models", ",", "which", "can", "leverage", "textual", "features", "or", "multimodal", "features", ".", "Upon", "these", "baselines", ",", "to", "capture", "the", "natural", "monotonic", "alignment", "between", "the", "textual", "modality", "and", "the", "acoustic", "modality", ",", "we", "further", "propose", "a", "simple", "multimodal", "multitask", "model", "by", "introducing", "a", "speech-to-text", "alignment", "auxiliary", "task", ".", "Through", "extensive", "experiments", ",", "we", "observe", "that", ":", "(", "1", ")", "Progressive", "performance", "boosts", "as", "we", "move", "from", "unimodal", "to", "multimodal", ",", "verifying", "the", "necessity", "of", "integrating", "speech", "clues", "into", "Chinese", "NER", ".", "(", "2", ")", "Our", "proposed", "model", "yields", "state-of-the-art", "(", "SoTA", ")", "results", "on", "CNERTA", ",", "demonstrating", "its", "effectiveness", ".", "For", "further", "research", ",", "the", "annotated", "dataset", "is", "publicly", "available", "at", "http://github.com/DianboWork/CNERTA", "."], "entities": [{"type": "Operation", "start": 137, "end": 142, "text": "move from unimodal to multimodal", "sent_idx": 5}, {"type": "Effect", "start": 132, "end": 134, "text": "Progressive performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--218"}
{"text": "Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like \u201cgay\u201d or \u201cblack\u201d are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models\u2019 inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.", "tokens": ["Hate", "speech", "classifiers", "trained", "on", "imbalanced", "datasets", "struggle", "to", "determine", "if", "group", "identifiers", "like", "\u201c", "gay", "\u201d", "or", "\u201c", "black", "\u201d", "are", "used", "in", "offensive", "or", "prejudiced", "ways", ".", "Such", "biases", "manifest", "in", "false", "positives", "when", "these", "identifiers", "are", "present", ",", "due", "to", "models", "\u2019", "inability", "to", "learn", "the", "contexts", "which", "constitute", "a", "hateful", "usage", "of", "identifiers", ".", "We", "extract", "post-hoc", "explanations", "from", "fine-tuned", "BERT", "classifiers", "to", "detect", "bias", "towards", "identity", "terms", ".", "Then", ",", "we", "propose", "a", "novel", "regularization", "technique", "based", "on", "these", "explanations", "that", "encourages", "models", "to", "learn", "from", "the", "context", "of", "group", "identifiers", "in", "addition", "to", "the", "identifiers", "themselves", ".", "Our", "approach", "improved", "over", "baselines", "in", "limiting", "false", "positives", "on", "out-of-domain", "data", "while", "maintaining", "and", "in", "cases", "improving", "in-domain", "performance", "."], "entities": [{"type": "Operation", "start": 59, "end": 66, "text": "extract post-hoc explanations from fine-tuned BERT classifiers", "sent_idx": 2}, {"type": "Effect", "start": 122, "end": 123, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 79, "end": 81, "text": "regularization technique", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--483"}
{"text": "Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. In our approach, each dialogue model consists of a shared module, a gating module, and a private module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.", "tokens": ["Training", "the", "generative", "models", "with", "minimal", "corpus", "is", "one", "of", "the", "critical", "challenges", "for", "building", "open-domain", "dialogue", "systems", ".", "Existing", "methods", "tend", "to", "use", "the", "meta-learning", "framework", "which", "pre-trains", "the", "parameters", "on", "all", "non-target", "tasks", "then", "fine-tunes", "on", "the", "target", "task", ".", "However", ",", "fine-tuning", "distinguishes", "tasks", "from", "the", "parameter", "perspective", "but", "ignores", "the", "model-structure", "perspective", ",", "resulting", "in", "similar", "dialogue", "models", "for", "different", "tasks", ".", "In", "this", "paper", ",", "we", "propose", "an", "algorithm", "that", "can", "customize", "a", "unique", "dialogue", "model", "for", "each", "task", "in", "the", "few-shot", "setting", ".", "In", "our", "approach", ",", "each", "dialogue", "model", "consists", "of", "a", "shared", "module", ",", "a", "gating", "module", ",", "and", "a", "private", "module", ".", "The", "first", "two", "modules", "are", "shared", "among", "all", "the", "tasks", ",", "while", "the", "third", "one", "will", "differentiate", "into", "different", "network", "structures", "to", "better", "capture", "the", "characteristics", "of", "the", "corresponding", "task", ".", "The", "extensive", "experiments", "on", "two", "datasets", "show", "that", "our", "method", "outperforms", "all", "the", "baselines", "in", "terms", "of", "task", "consistency", ",", "response", "quality", ",", "and", "diversity", "."], "entities": [{"type": "Operation", "start": 150, "end": 152, "text": "our method", "sent_idx": 6}, {"type": "Effect", "start": 159, "end": 161, "text": "task consistency", "sent_idx": 6}, {"type": "Effect", "start": 162, "end": 164, "text": "response quality", "sent_idx": 6}, {"type": "Effect", "start": 166, "end": 167, "text": "diversity", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--517"}
{"text": "With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user\u2019s latent preference factors which cause her clicks on different news. In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.", "tokens": ["With", "the", "explosion", "of", "news", "information", ",", "personalized", "news", "recommendation", "has", "become", "very", "important", "for", "users", "to", "quickly", "find", "their", "interested", "contents", ".", "Most", "existing", "methods", "usually", "learn", "the", "representations", "of", "users", "and", "news", "from", "news", "contents", "for", "recommendation", ".", "However", ",", "they", "seldom", "consider", "high-order", "connectivity", "underlying", "the", "user-news", "interactions", ".", "Moreover", ",", "existing", "methods", "failed", "to", "disentangle", "a", "user", "\u2019s", "latent", "preference", "factors", "which", "cause", "her", "clicks", "on", "different", "news", ".", "In", "this", "paper", ",", "we", "model", "the", "user-news", "interactions", "as", "a", "bipartite", "graph", "and", "propose", "a", "novel", "Graph", "Neural", "News", "Recommendation", "model", "with", "Unsupervised", "Preference", "Disentanglement", ",", "named", "GNUD", ".", "Our", "model", "can", "encode", "high-order", "relationships", "into", "user", "and", "news", "representations", "by", "information", "propagation", "along", "the", "graph", ".", "Furthermore", ",", "the", "learned", "representations", "are", "disentangled", "with", "latent", "preference", "factors", "by", "a", "neighborhood", "routing", "algorithm", ",", "which", "can", "enhance", "expressiveness", "and", "interpretability", ".", "A", "preference", "regularizer", "is", "also", "designed", "to", "force", "each", "disentangled", "subspace", "to", "independently", "reflect", "an", "isolated", "preference", ",", "improving", "the", "quality", "of", "the", "disentangled", "representations", ".", "Experimental", "results", "on", "real-world", "news", "datasets", "demonstrate", "that", "our", "proposed", "model", "can", "effectively", "improve", "the", "performance", "of", "news", "recommendation", "and", "outperform", "state-of-the-art", "news", "recommendation", "methods", "."], "entities": [{"type": "Operation", "start": 134, "end": 137, "text": "neighborhood routing algorithm", "sent_idx": 6}, {"type": "Effect", "start": 141, "end": 142, "text": "expressiveness", "sent_idx": 6}, {"type": "Effect", "start": 143, "end": 144, "text": "interpretability", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--392"}
{"text": "Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. However, individuals\u2019 cognition of social things is not always able to truly reflect the objective. There may be one-sided or biased semantics in their opinions on a claim. The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance. In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification. From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence. From the view of individual cognition, we select the top- k articles with high degree of difference and interact with the claim to explore the local key evidence fragments. To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both. Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance.", "tokens": ["Recent", "studies", "constructing", "direct", "interactions", "between", "the", "claim", "and", "each", "single", "user", "response", "(", "a", "comment", "or", "a", "relevant", "article", ")", "to", "capture", "evidence", "have", "shown", "remarkable", "success", "in", "interpretable", "claim", "verification", ".", "Owing", "to", "different", "single", "responses", "convey", "different", "cognition", "of", "individual", "users", "(", "i.e.", ",", "audiences", ")", ",", "the", "captured", "evidence", "belongs", "to", "the", "perspective", "of", "individual", "cognition", ".", "However", ",", "individuals", "\u2019", "cognition", "of", "social", "things", "is", "not", "always", "able", "to", "truly", "reflect", "the", "objective", ".", "There", "may", "be", "one-sided", "or", "biased", "semantics", "in", "their", "opinions", "on", "a", "claim", ".", "The", "captured", "evidence", "correspondingly", "contains", "some", "unobjective", "and", "biased", "evidence", "fragments", ",", "deteriorating", "task", "performance", ".", "In", "this", "paper", ",", "we", "propose", "a", "Dual-view", "model", "based", "on", "the", "views", "of", "Collective", "and", "Individual", "Cognition", "(", "CICD", ")", "for", "interpretable", "claim", "verification", ".", "From", "the", "view", "of", "the", "collective", "cognition", ",", "we", "not", "only", "capture", "the", "word-level", "semantics", "based", "on", "individual", "users", ",", "but", "also", "focus", "on", "sentence-level", "semantics", "(", "i.e.", ",", "the", "overall", "responses", ")", "among", "all", "users", "and", "adjust", "the", "proportion", "between", "them", "to", "generate", "global", "evidence", ".", "From", "the", "view", "of", "individual", "cognition", ",", "we", "select", "the", "top-", "k", "articles", "with", "high", "degree", "of", "difference", "and", "interact", "with", "the", "claim", "to", "explore", "the", "local", "key", "evidence", "fragments", ".", "To", "weaken", "the", "bias", "of", "individual", "cognition-view", "evidence", ",", "we", "devise", "inconsistent", "loss", "to", "suppress", "the", "divergence", "between", "global", "and", "local", "evidence", "for", "strengthening", "the", "consistent", "shared", "evidence", "between", "the", "both", ".", "Experiments", "on", "three", "benchmark", "datasets", "confirm", "that", "CICD", "achieves", "state-of-the-art", "performance", "."], "entities": [{"type": "Operation", "start": 252, "end": 253, "text": "CICD", "sent_idx": 9}, {"type": "Effect", "start": 255, "end": 256, "text": "performance", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--5"}
{"text": "Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.", "tokens": ["Interpretable", "rationales", "for", "model", "predictions", "play", "a", "critical", "role", "in", "practical", "applications", ".", "In", "this", "study", ",", "we", "develop", "models", "possessing", "interpretable", "inference", "process", "for", "structured", "prediction", ".", "Specifically", ",", "we", "present", "a", "method", "of", "instance-based", "learning", "that", "learns", "similarities", "between", "spans", ".", "At", "inference", "time", ",", "each", "span", "is", "assigned", "a", "class", "label", "based", "on", "its", "similar", "spans", "in", "the", "training", "set", ",", "where", "it", "is", "easy", "to", "understand", "how", "much", "each", "training", "instance", "contributes", "to", "the", "predictions", ".", "Through", "empirical", "analysis", "on", "named", "entity", "recognition", ",", "we", "demonstrate", "that", "our", "method", "enables", "to", "build", "models", "that", "have", "high", "interpretability", "without", "sacrificing", "performance", "."], "entities": [{"type": "Operation", "start": 32, "end": 37, "text": "a method of instance-based learning", "sent_idx": 2}, {"type": "Effect", "start": 100, "end": 101, "text": "interpretability", "sent_idx": 4}, {"type": "Effect", "start": 103, "end": 104, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--575"}
{"text": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT.", "tokens": ["Heavily", "overparameterized", "language", "models", "such", "as", "BERT", ",", "XLNet", "and", "T5", "have", "achieved", "impressive", "success", "in", "many", "NLP", "tasks", ".", "However", ",", "their", "high", "model", "complexity", "requires", "enormous", "computation", "resources", "and", "extremely", "long", "training", "time", "for", "both", "pre-training", "and", "fine-tuning", ".", "Many", "works", "have", "studied", "model", "compression", "on", "large", "NLP", "models", ",", "but", "only", "focusing", "on", "reducing", "inference", "time", "while", "still", "requiring", "an", "expensive", "training", "process", ".", "Other", "works", "use", "extremely", "large", "batch", "sizes", "to", "shorten", "the", "pre-training", "time", ",", "at", "the", "expense", "of", "higher", "computational", "resource", "demands", ".", "In", "this", "paper", ",", "inspired", "by", "the", "Early-Bird", "Lottery", "Tickets", "recently", "studied", "for", "computer", "vision", "tasks", ",", "we", "propose", "EarlyBERT", ",", "a", "general", "computationally-efficient", "training", "algorithm", "applicable", "to", "both", "pre-training", "and", "fine-tuning", "of", "large-scale", "language", "models", ".", "By", "slimming", "the", "self-attention", "and", "fully-connected", "sub-layers", "inside", "a", "transformer", ",", "we", "are", "the", "first", "to", "identify", "structured", "winning", "tickets", "in", "the", "early", "stage", "of", "BERT", "training", ".", "We", "apply", "those", "tickets", "towards", "efficient", "BERT", "training", ",", "and", "conduct", "comprehensive", "pre-training", "and", "fine-tuning", "experiments", "on", "GLUE", "and", "SQuAD", "downstream", "tasks", ".", "Our", "results", "show", "that", "EarlyBERT", "achieves", "comparable", "performance", "to", "standard", "BERT", ",", "with", "35", "45", "%", "less", "training", "time", ".", "Code", "is", "available", "at", "https://github.com/VITA-Group/EarlyBERT", "."], "entities": [{"type": "Operation", "start": 181, "end": 182, "text": "EarlyBERT", "sent_idx": 7}, {"type": "Effect", "start": 184, "end": 185, "text": "performance", "sent_idx": 7}, {"type": "Effect", "start": 194, "end": 196, "text": "training time", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--171"}
{"text": "Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accuracies of 63--91% in one-against-others classification and 80--94% in pairwise classification (baseline = 50% in both cases).", "tokens": ["Argumentation", "schemes", "are", "structures", "or", "templates", "for", "various", "kinds", "of", "arguments", ".", "Given", "the", "text", "of", "an", "argument", "with", "premises", "and", "conclusion", "identified", ",", "we", "classify", "it", "as", "an", "instance", "of", "one", "of", "five", "common", "schemes", ",", "using", "features", "specific", "to", "each", "scheme", ".", "We", "achieve", "accuracies", "of", "63", "-", "-91", "%", "in", "one-against-others", "classification", "and", "80", "-", "-94", "%", "in", "pairwise", "classification", "(", "baseline", "=", "50", "%", "in", "both", "cases", ")", "."], "entities": [{"type": "Operation", "start": 37, "end": 43, "text": "using features specific to each scheme", "sent_idx": 1}, {"type": "Effect", "start": 46, "end": 47, "text": "accuracies", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1099"}
{"text": "The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS. It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35% of questions are context-independent, and 28% of SQL queries are easy. We experiment on Chase with three state-of-the-art XDTS approaches. The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that Chase highlights the challenging problems of XDTS. We believe that XDTS can provide fertile soil for addressing the problems.", "tokens": ["The", "cross-database", "context-dependent", "Text-to-SQL", "(", "XDTS", ")", "problem", "has", "attracted", "considerable", "attention", "in", "recent", "years", "due", "to", "its", "wide", "range", "of", "potential", "applications", ".", "However", ",", "we", "identify", "two", "biases", "in", "existing", "datasets", "for", "XDTS", ":", "(", "1", ")", "a", "high", "proportion", "of", "context-independent", "questions", "and", "(", "2", ")", "a", "high", "proportion", "of", "easy", "SQL", "queries", ".", "These", "biases", "conceal", "the", "major", "challenges", "in", "XDTS", "to", "some", "extent", ".", "In", "this", "work", ",", "we", "present", "Chase", ",", "a", "large-scale", "and", "pragmatic", "Chinese", "dataset", "for", "XDTS", ".", "It", "consists", "of", "5,459", "coherent", "question", "sequences", "(", "17,940", "questions", "with", "their", "SQL", "queries", "annotated", ")", "over", "280", "databases", ",", "in", "which", "only", "35", "%", "of", "questions", "are", "context-independent", ",", "and", "28", "%", "of", "SQL", "queries", "are", "easy", ".", "We", "experiment", "on", "Chase", "with", "three", "state-of-the-art", "XDTS", "approaches", ".", "The", "best", "approach", "only", "achieves", "an", "exact", "match", "accuracy", "of", "40", "%", "over", "all", "questions", "and", "16", "%", "over", "all", "question", "sequences", ",", "indicating", "that", "Chase", "highlights", "the", "challenging", "problems", "of", "XDTS", ".", "We", "believe", "that", "XDTS", "can", "provide", "fertile", "soil", "for", "addressing", "the", "problems", "."], "entities": [{"type": "Operation", "start": 132, "end": 133, "text": "XDTS", "sent_idx": 5}, {"type": "Effect", "start": 141, "end": 144, "text": "exact match accuracy", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--180"}
{"text": "Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.", "tokens": ["Generating", "open-domain", "conversational", "responses", "in", "the", "desired", "style", "usually", "suffers", "from", "the", "lack", "of", "parallel", "data", "in", "the", "style", ".", "Meanwhile", ",", "using", "monolingual", "stylistic", "data", "to", "increase", "style", "intensity", "often", "leads", "to", "the", "expense", "of", "decreasing", "content", "relevance", ".", "In", "this", "paper", ",", "we", "propose", "to", "disentangle", "the", "content", "and", "style", "in", "latent", "space", "by", "diluting", "sentence-level", "information", "in", "style", "representations", ".", "Combining", "the", "desired", "style", "representation", "and", "a", "response", "content", "representation", "will", "then", "obtain", "a", "stylistic", "response", ".", "Our", "approach", "achieves", "a", "higher", "BERT-based", "style", "intensity", "score", "and", "comparable", "BLEU", "scores", ",", "compared", "with", "baselines", ".", "Human", "evaluation", "results", "show", "that", "our", "approach", "significantly", "improves", "style", "intensity", "and", "maintains", "content", "relevance", "."], "entities": [{"type": "Operation", "start": 56, "end": 62, "text": "diluting sentence-level information in style representations", "sent_idx": 2}, {"type": "Effect", "start": 85, "end": 89, "text": "BERT-based style intensity score", "sent_idx": 4}, {"type": "Effect", "start": 91, "end": 93, "text": "BLEU scores", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--339"}
{"text": "Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. Our code is available at https://github.com/shizhediao/T-DNA.", "tokens": ["Large", "pre-trained", "models", "such", "as", "BERT", "are", "known", "to", "improve", "different", "downstream", "NLP", "tasks", ",", "even", "when", "such", "a", "model", "is", "trained", "on", "a", "generic", "domain", ".", "Moreover", ",", "recent", "studies", "have", "shown", "that", "when", "large", "domain-specific", "corpora", "are", "available", ",", "continued", "pre-training", "on", "domain-specific", "data", "can", "further", "improve", "the", "performance", "of", "in-domain", "tasks", ".", "However", ",", "this", "practice", "requires", "significant", "domain-specific", "data", "and", "computational", "resources", "which", "may", "not", "always", "be", "available", ".", "In", "this", "paper", ",", "we", "aim", "to", "adapt", "a", "generic", "pretrained", "model", "with", "a", "relatively", "small", "amount", "of", "domain-specific", "data", ".", "We", "demonstrate", "that", "by", "explicitly", "incorporating", "multi-granularity", "information", "of", "unseen", "and", "domain-specific", "words", "via", "the", "adaptation", "of", "(", "word", "based", ")", "n-grams", ",", "the", "performance", "of", "a", "generic", "pretrained", "model", "can", "be", "greatly", "improved", ".", "Specifically", ",", "we", "introduce", "a", "Transformer-based", "Domain-aware", "N-gram", "Adaptor", ",", "T-DNA", ",", "to", "effectively", "learn", "and", "incorporate", "the", "semantic", "representation", "of", "different", "combinations", "of", "words", "in", "the", "new", "domain", ".", "Experimental", "results", "illustrate", "the", "effectiveness", "of", "T-DNA", "on", "eight", "low-resource", "downstream", "tasks", "from", "four", "domains", ".", "We", "show", "that", "T-DNA", "is", "able", "to", "achieve", "significant", "improvements", "compared", "to", "existing", "methods", "on", "most", "tasks", "using", "limited", "data", "with", "lower", "computational", "costs", ".", "Moreover", ",", "further", "analyses", "demonstrate", "the", "importance", "and", "effectiveness", "of", "both", "unseen", "words", "and", "the", "information", "of", "different", "granularities", ".", "Our", "code", "is", "available", "at", "https://github.com/shizhediao/T-DNA", "."], "entities": [{"type": "Operation", "start": 178, "end": 179, "text": "T-DNA", "sent_idx": 7}, {"type": "Effect", "start": 197, "end": 199, "text": "computational costs", "sent_idx": 7}, {"type": "Operation", "start": 98, "end": 107, "text": "explicitly incorporating multi-granularity information of unseen and domain-specific words", "sent_idx": 4}, {"type": "Effect", "start": 118, "end": 119, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--259"}
{"text": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.", "tokens": ["Counts", "from", "large", "corpora", "(", "like", "the", "web", ")", "can", "be", "powerful", "syntactic", "cues", ".", "Past", "work", "has", "used", "web", "counts", "to", "help", "resolve", "isolated", "ambiguities", ",", "such", "as", "binary", "noun-verb", "PP", "attachments", "and", "noun", "compound", "bracketings", ".", "In", "this", "work", ",", "we", "first", "present", "a", "method", "for", "generating", "web", "count", "features", "that", "address", "the", "full", "range", "of", "syntactic", "attachments", ".", "These", "features", "encode", "both", "surface", "evidence", "of", "lexical", "affinities", "as", "well", "as", "paraphrase-based", "cues", "to", "syntactic", "structure", ".", "We", "then", "integrate", "our", "features", "into", "full-scale", "dependency", "and", "constituent", "parsers", ".", "We", "show", "relative", "error", "reductions", "of", "7.0", "%", "over", "the", "second-order", "dependency", "parser", "of", "McDonald", "and", "Pereira", "(", "2006", ")", ",", "9.2", "%", "over", "the", "constituent", "parser", "of", "Petrov", "et", "al.", "(", "2006", ")", ",", "and", "3.4", "%", "over", "a", "non-local", "constituent", "reranker", "."], "entities": [{"type": "Operation", "start": 65, "end": 70, "text": "surface evidence of lexical affinities", "sent_idx": 3}, {"type": "Effect", "start": 93, "end": 95, "text": "relative error", "sent_idx": 5}, {"type": "Operation", "start": 73, "end": 75, "text": "paraphrase-based cues", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "P11-1070"}
{"text": "In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG.", "tokens": ["In", "modular", "dialogue", "systems", ",", "natural", "language", "understanding", "(", "NLU", ")", "and", "natural", "language", "generation", "(", "NLG", ")", "are", "two", "critical", "components", ",", "where", "NLU", "extracts", "the", "semantics", "from", "the", "given", "texts", "and", "NLG", "is", "to", "construct", "corresponding", "natural", "language", "sentences", "based", "on", "the", "input", "semantic", "representations", ".", "However", ",", "the", "dual", "property", "between", "understanding", "and", "generation", "has", "been", "rarely", "explored", ".", "The", "prior", "work", "is", "the", "first", "attempt", "that", "utilized", "the", "duality", "between", "NLU", "and", "NLG", "to", "improve", "the", "performance", "via", "a", "dual", "supervised", "learning", "framework", ".", "However", ",", "the", "prior", "work", "still", "learned", "both", "components", "in", "a", "supervised", "manner", ";", "instead", ",", "this", "paper", "introduces", "a", "general", "learning", "framework", "to", "effectively", "exploit", "such", "duality", ",", "providing", "flexibility", "of", "incorporating", "both", "supervised", "and", "unsupervised", "learning", "algorithms", "to", "train", "language", "understanding", "and", "generation", "models", "in", "a", "joint", "fashion", ".", "The", "benchmark", "experiments", "demonstrate", "that", "the", "proposed", "approach", "is", "capable", "of", "boosting", "the", "performance", "of", "both", "NLU", "and", "NLG", ".", "The", "source", "code", "is", "available", "at", ":", "https://github.com/MiuLab/DuaLUG", "."], "entities": [{"type": "Operation", "start": 106, "end": 111, "text": "introduces a general learning framework", "sent_idx": 3}, {"type": "Effect", "start": 152, "end": 153, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--63"}
{"text": "The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.", "tokens": ["The", "state-of-the-art", "system", "combination", "method", "for", "machine", "translation", "(", "MT", ")", "is", "based", "on", "confusion", "networks", "constructed", "by", "aligning", "hypotheses", "with", "regard", "to", "word", "similarities", ".", "We", "introduce", "a", "novel", "system", "combination", "framework", "in", "which", "hypotheses", "are", "encoded", "as", "a", "confusion", "forest", ",", "a", "packed", "forest", "representing", "alternative", "trees", ".", "The", "forest", "is", "generated", "using", "syntactic", "consensus", "among", "parsed", "hypotheses", ":", "First", ",", "MT", "outputs", "are", "parsed", ".", "Second", ",", "a", "context", "free", "grammar", "is", "learned", "by", "extracting", "a", "set", "of", "rules", "that", "constitute", "the", "parse", "trees", ".", "Third", ",", "a", "packed", "forest", "is", "generated", "starting", "from", "the", "root", "symbol", "of", "the", "extracted", "grammar", "through", "non-terminal", "rewriting", ".", "The", "new", "hypothesis", "is", "produced", "by", "searching", "the", "best", "derivation", "in", "the", "forest", ".", "Experimental", "results", "on", "the", "WMT10", "system", "combination", "shared", "task", "yield", "comparable", "performance", "to", "the", "conventional", "confusion", "network", "based", "method", "with", "smaller", "space", "."], "entities": [{"type": "Operation", "start": 29, "end": 33, "text": "novel system combination framework", "sent_idx": 1}, {"type": "Effect", "start": 133, "end": 134, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1125"}
{"text": "This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.", "tokens": ["This", "paper", "explores", "the", "use", "of", "bilingual", "parallel", "corpora", "as", "a", "source", "of", "lexical", "knowledge", "for", "cross-lingual", "textual", "entailment", ".", "We", "claim", "that", ",", "in", "spite", "of", "the", "inherent", "difficulties", "of", "the", "task", ",", "phrase", "tables", "extracted", "from", "parallel", "data", "allow", "to", "capture", "both", "lexical", "relations", "between", "single", "words", ",", "and", "contextual", "information", "useful", "for", "inference", ".", "We", "experiment", "with", "a", "phrasal", "matching", "method", "in", "order", "to", ":", "i", ")", "build", "a", "system", "portable", "across", "languages", ",", "and", "ii", ")", "evaluate", "the", "contribution", "of", "lexical", "knowledge", "in", "isolation", ",", "without", "interaction", "with", "other", "inference", "mechanisms", ".", "Results", "achieved", "on", "an", "English-Spanish", "corpus", "obtained", "from", "the", "RTE3", "dataset", "support", "our", "claim", ",", "with", "an", "overall", "accuracy", "above", "average", "scores", "reported", "by", "RTE", "participants", "on", "monolingual", "data", ".", "Finally", ",", "we", "show", "that", "using", "parallel", "corpora", "to", "extract", "paraphrase", "tables", "reveals", "their", "potential", "also", "in", "the", "monolingual", "setting", ",", "improving", "the", "results", "achieved", "with", "other", "sources", "of", "lexical", "knowledge", "."], "entities": [{"type": "Operation", "start": 4, "end": 15, "text": "use of bilingual parallel corpora as a source of lexical knowledge", "sent_idx": 0}, {"type": "Effect", "start": 114, "end": 115, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1134"}
{"text": "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.", "tokens": ["Simultaneous", "translation", "has", "many", "important", "application", "scenarios", "and", "attracts", "much", "attention", "from", "both", "academia", "and", "industry", "recently", ".", "Most", "existing", "frameworks", ",", "however", ",", "have", "difficulties", "in", "balancing", "between", "the", "translation", "quality", "and", "latency", ",", "i.e.", ",", "the", "decoding", "policy", "is", "usually", "either", "too", "aggressive", "or", "too", "conservative", ".", "We", "propose", "an", "opportunistic", "decoding", "technique", "with", "timely", "correction", "ability", ",", "which", "always", "(over-)generates", "a", "certain", "mount", "of", "extra", "words", "at", "each", "step", "to", "keep", "the", "audience", "on", "track", "with", "the", "latest", "information", ".", "At", "the", "same", "time", ",", "it", "also", "corrects", ",", "in", "a", "timely", "fashion", ",", "the", "mistakes", "in", "the", "former", "overgenerated", "words", "when", "observing", "more", "source", "context", "to", "ensure", "high", "translation", "quality", ".", "Experiments", "show", "our", "technique", "achieves", "substantial", "reduction", "in", "latency", "and", "up", "to", "+", "3.1", "increase", "in", "BLEU", ",", "with", "revision", "rate", "under", "8", "%", "in", "Chinese-to-English", "and", "English-to-Chinese", "translation", "."], "entities": [{"type": "Operation", "start": 52, "end": 55, "text": "opportunistic decoding technique", "sent_idx": 2}, {"type": "Effect", "start": 123, "end": 124, "text": "latency", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 132, "text": "BLEU", "sent_idx": 4}, {"type": "Effect", "start": 134, "end": 136, "text": "revision rate", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--42"}
{"text": "To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations.", "tokens": ["To", "defend", "against", "machine-generated", "fake", "news", ",", "an", "effective", "mechanism", "is", "urgently", "needed", ".", "We", "contribute", "a", "novel", "benchmark", "for", "fake", "news", "detection", "at", "the", "knowledge", "element", "level", ",", "as", "well", "as", "a", "solution", "for", "this", "task", "which", "incorporates", "cross-media", "consistency", "checking", "to", "detect", "the", "fine-grained", "knowledge", "elements", "making", "news", "articles", "misinformative", ".", "Due", "to", "training", "data", "scarcity", ",", "we", "also", "formulate", "a", "novel", "data", "synthesis", "method", "by", "manipulating", "knowledge", "elements", "within", "the", "knowledge", "graph", "to", "generate", "noisy", "training", "data", "with", "specific", ",", "hard", "to", "detect", ",", "known", "inconsistencies", ".", "Our", "detection", "approach", "outperforms", "the", "state-of-the-art", "(", "up", "to", "16.8", "%", "accuracy", "gain", ")", ",", "and", "more", "critically", ",", "yields", "fine-grained", "explanations", "."], "entities": [{"type": "Operation", "start": 90, "end": 93, "text": "Our detection approach", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 102, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--133"}
{"text": "We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user\u2019s goal instructions, which are the user context and task constraints in natural language, and (2) system\u2019s API call results, which is a list of possible query responses for user requests from the given knowledge base. Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results. We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking. In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset.", "tokens": ["We", "propose", "NeuralWOZ", ",", "a", "novel", "dialogue", "collection", "framework", "that", "uses", "model-based", "dialogue", "simulation", ".", "NeuralWOZ", "has", "two", "pipelined", "models", ",", "Collector", "and", "Labeler", ".", "Collector", "generates", "dialogues", "from", "(", "1", ")", "user", "\u2019s", "goal", "instructions", ",", "which", "are", "the", "user", "context", "and", "task", "constraints", "in", "natural", "language", ",", "and", "(", "2", ")", "system", "\u2019s", "API", "call", "results", ",", "which", "is", "a", "list", "of", "possible", "query", "responses", "for", "user", "requests", "from", "the", "given", "knowledge", "base", ".", "Labeler", "annotates", "the", "generated", "dialogue", "by", "formulating", "the", "annotation", "as", "a", "multiple-choice", "problem", ",", "in", "which", "the", "candidate", "labels", "are", "extracted", "from", "goal", "instructions", "and", "API", "call", "results", ".", "We", "demonstrate", "the", "effectiveness", "of", "the", "proposed", "method", "in", "the", "zero-shot", "domain", "transfer", "learning", "for", "dialogue", "state", "tracking", ".", "In", "the", "evaluation", ",", "the", "synthetic", "dialogue", "corpus", "generated", "from", "NeuralWOZ", "achieves", "a", "new", "state-of-the-art", "with", "improvements", "of", "4.4", "%", "point", "joint", "goal", "accuracy", "on", "average", "across", "domains", ",", "and", "improvements", "of", "5.7", "%", "point", "of", "zero-shot", "coverage", "against", "the", "MultiWOZ", "2.1", "dataset", "."], "entities": [{"type": "Operation", "start": 134, "end": 135, "text": "NeuralWOZ", "sent_idx": 5}, {"type": "Effect", "start": 145, "end": 148, "text": "joint goal accuracy", "sent_idx": 5}, {"type": "Effect", "start": 161, "end": 162, "text": "coverage", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--287"}
{"text": "Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.", "tokens": ["Task", "variance", "regularization", ",", "which", "can", "be", "used", "to", "improve", "the", "generalization", "of", "Multi-task", "Learning", "(", "MTL", ")", "models", ",", "remains", "unexplored", "in", "multi-task", "text", "classification", ".", "Accordingly", ",", "to", "fill", "this", "gap", ",", "this", "paper", "investigates", "how", "the", "task", "might", "be", "effectively", "regularized", ",", "and", "consequently", "proposes", "a", "multi-task", "learning", "method", "based", "on", "adversarial", "multi-armed", "bandit", ".", "The", "proposed", "method", ",", "named", "BanditMTL", ",", "regularizes", "the", "task", "variance", "by", "means", "of", "a", "mirror", "gradient", "ascent-descent", "algorithm", ".", "Adopting", "BanditMTL", "in", "the", "multi-task", "text", "classification", "context", "is", "found", "to", "achieve", "state-of-the-art", "performance", ".", "The", "results", "of", "extensive", "experiments", "back", "up", "our", "theoretical", "analysis", "and", "validate", "the", "superiority", "of", "our", "proposals", "."], "entities": [{"type": "Operation", "start": 78, "end": 80, "text": "Adopting BanditMTL", "sent_idx": 3}, {"type": "Effect", "start": 91, "end": 92, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--428"}
{"text": "This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in  https://github.com/maszhongming/MatchSum .", "tokens": ["This", "paper", "creates", "a", "paradigm", "shift", "with", "regard", "to", "the", "way", "we", "build", "neural", "extractive", "summarization", "systems", ".", "Instead", "of", "following", "the", "commonly", "used", "framework", "of", "extracting", "sentences", "individually", "and", "modeling", "the", "relationship", "between", "sentences", ",", "we", "formulate", "the", "extractive", "summarization", "task", "as", "a", "semantic", "text", "matching", "problem", ",", "in", "which", "a", "source", "document", "and", "candidate", "summaries", "will", "be", "(", "extracted", "from", "the", "original", "text", ")", "matched", "in", "a", "semantic", "space", ".", "Notably", ",", "this", "paradigm", "shift", "to", "semantic", "matching", "framework", "is", "well-grounded", "in", "our", "comprehensive", "analysis", "of", "the", "inherent", "gap", "between", "sentence-level", "and", "summary-level", "extractors", "based", "on", "the", "property", "of", "the", "dataset", ".", "Besides", ",", "even", "instantiating", "the", "framework", "with", "a", "simple", "form", "of", "a", "matching", "model", ",", "we", "have", "driven", "the", "state-of-the-art", "extractive", "result", "on", "CNN/DailyMail", "to", "a", "new", "level", "(", "44.41", "in", "ROUGE-1", ")", ".", "Experiments", "on", "the", "other", "five", "datasets", "also", "show", "the", "effectiveness", "of", "the", "matching", "framework", ".", "We", "believe", "the", "power", "of", "this", "matching-based", "summarization", "framework", "has", "not", "been", "fully", "exploited", ".", "To", "encourage", "more", "instantiations", "in", "the", "future", ",", "we", "have", "released", "our", "codes", ",", "processed", "dataset", ",", "as", "well", "as", "generated", "summaries", "in", " ", "https://github.com/maszhongming/MatchSum", "."], "entities": [{"type": "Operation", "start": 107, "end": 118, "text": "instantiating the framework with a simple form of a matching model", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 136, "text": "ROUGE-1", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--552"}
{"text": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.", "tokens": ["Named", "Entity", "Recognition", "(", "NER", ")", "is", "a", "fundamental", "task", "in", "Natural", "Language", "Processing", ",", "concerned", "with", "identifying", "spans", "of", "text", "expressing", "references", "to", "entities", ".", "NER", "research", "is", "often", "focused", "on", "flat", "entities", "only", "(", "flat", "NER", ")", ",", "ignoring", "the", "fact", "that", "entity", "references", "can", "be", "nested", ",", "as", "in", "[", "Bank", "of", "[", "China", "]", "]", "(", "Finkel", "and", "Manning", ",", "2009", ")", ".", "In", "this", "paper", ",", "we", "use", "ideas", "from", "graph-based", "dependency", "parsing", "to", "provide", "our", "model", "a", "global", "view", "on", "the", "input", "via", "a", "biaffine", "model", "(", "Dozat", "and", "Manning", ",", "2017", ")", ".", "The", "biaffine", "model", "scores", "pairs", "of", "start", "and", "end", "tokens", "in", "a", "sentence", "which", "we", "use", "to", "explore", "all", "spans", ",", "so", "that", "the", "model", "is", "able", "to", "predict", "named", "entities", "accurately", ".", "We", "show", "that", "the", "model", "works", "well", "for", "both", "nested", "and", "flat", "NER", "through", "evaluation", "on", "8", "corpora", "and", "achieving", "SoTA", "performance", "on", "all", "of", "them", ",", "with", "accuracy", "gains", "of", "up", "to", "2.2", "percentage", "points", "."], "entities": [{"type": "Operation", "start": 101, "end": 103, "text": "biaffine model", "sent_idx": 3}, {"type": "Effect", "start": 154, "end": 155, "text": "performance", "sent_idx": 4}, {"type": "Effect", "start": 161, "end": 162, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--577"}
{"text": "The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.", "tokens": ["The", "advent", "of", "context-aware", "NMT", "has", "resulted", "in", "promising", "improvements", "in", "the", "overall", "translation", "quality", "and", "specifically", "in", "the", "translation", "of", "discourse", "phenomena", "such", "as", "pronouns", ".", "Previous", "works", "have", "mainly", "focused", "on", "the", "use", "of", "past", "sentences", "as", "context", "with", "a", "focus", "on", "anaphora", "translation", ".", "In", "this", "work", ",", "we", "investigate", "the", "effect", "of", "future", "sentences", "as", "context", "by", "comparing", "the", "performance", "of", "a", "contextual", "NMT", "model", "trained", "with", "the", "future", "context", "to", "the", "one", "trained", "with", "the", "past", "context", ".", "Our", "experiments", "and", "evaluation", ",", "using", "generic", "and", "pronoun-focused", "automatic", "metrics", ",", "show", "that", "the", "use", "of", "future", "context", "not", "only", "achieves", "significant", "improvements", "over", "the", "context-agnostic", "Transformer", ",", "but", "also", "demonstrates", "comparable", "and", "in", "some", "cases", "improved", "performance", "over", "its", "counterpart", "trained", "on", "past", "context", ".", "We", "also", "perform", "an", "evaluation", "on", "a", "targeted", "cataphora", "test", "suite", "and", "report", "significant", "gains", "over", "the", "context-agnostic", "Transformer", "in", "terms", "of", "BLEU", "."], "entities": [{"type": "Operation", "start": 98, "end": 102, "text": "use of future context", "sent_idx": 3}, {"type": "Effect", "start": 121, "end": 122, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--530"}
{"text": "We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance.", "tokens": ["We", "present", "a", "novel", "approach", "to", "discovering", "relations", "and", "their", "instantiations", "from", "a", "collection", "of", "documents", "in", "a", "single", "domain", ".", "Our", "approach", "learns", "relation", "types", "by", "exploiting", "meta-constraints", "that", "characterize", "the", "general", "qualities", "of", "a", "good", "relation", "in", "any", "domain", ".", "These", "constraints", "state", "that", "instances", "of", "a", "single", "relation", "should", "exhibit", "regularities", "at", "multiple", "levels", "of", "linguistic", "structure", ",", "including", "lexicography", ",", "syntax", ",", "and", "document-level", "context", ".", "We", "capture", "these", "regularities", "via", "the", "structure", "of", "our", "probabilistic", "model", "as", "well", "as", "a", "set", "of", "declaratively-specified", "constraints", "enforced", "during", "posterior", "inference", ".", "Across", "two", "domains", "our", "approach", "successfully", "recovers", "hidden", "relation", "structure", ",", "comparable", "to", "or", "outperforming", "previous", "state-of-the-art", "approaches", ".", "Furthermore", ",", "we", "find", "that", "a", "small", "set", "of", "constraints", "is", "applicable", "across", "the", "domains", ",", "and", "that", "using", "domain-specific", "constraints", "can", "further", "improve", "performance", "."], "entities": [{"type": "Operation", "start": 131, "end": 134, "text": "using domain-specific constraints", "sent_idx": 5}, {"type": "Effect", "start": 137, "end": 138, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1054"}
{"text": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.", "tokens": ["Although", "pretrained", "language", "models", "can", "be", "fine-tuned", "to", "produce", "state-of-the-art", "results", "for", "a", "very", "wide", "range", "of", "language", "understanding", "tasks", ",", "the", "dynamics", "of", "this", "process", "are", "not", "well", "understood", ",", "especially", "in", "the", "low", "data", "regime", ".", "Why", "can", "we", "use", "relatively", "vanilla", "gradient", "descent", "algorithms", "(", "e.g.", ",", "without", "strong", "regularization", ")", "to", "tune", "a", "model", "with", "hundreds", "of", "millions", "of", "parameters", "on", "datasets", "with", "only", "hundreds", "or", "thousands", "of", "labeled", "examples", "?", "In", "this", "paper", ",", "we", "argue", "that", "analyzing", "fine-tuning", "through", "the", "lens", "of", "intrinsic", "dimension", "provides", "us", "with", "empirical", "and", "theoretical", "intuitions", "to", "explain", "this", "remarkable", "phenomenon", ".", "We", "empirically", "show", "that", "common", "pre-trained", "models", "have", "a", "very", "low", "intrinsic", "dimension", ";", "in", "other", "words", ",", "there", "exists", "a", "low", "dimension", "reparameterization", "that", "is", "as", "effective", "for", "fine-tuning", "as", "the", "full", "parameter", "space", ".", "For", "example", ",", "by", "optimizing", "only", "200", "trainable", "parameters", "randomly", "projected", "back", "into", "the", "full", "space", ",", "we", "can", "tune", "a", "RoBERTa", "model", "to", "achieve", "90", "%", "of", "the", "full", "parameter", "performance", "levels", "on", "MRPC", ".", "Furthermore", ",", "we", "empirically", "show", "that", "pre-training", "implicitly", "minimizes", "intrinsic", "dimension", "and", ",", "perhaps", "surprisingly", ",", "larger", "models", "tend", "to", "have", "lower", "intrinsic", "dimension", "after", "a", "fixed", "number", "of", "pre-training", "updates", ",", "at", "least", "in", "part", "explaining", "their", "extreme", "effectiveness", ".", "Lastly", ",", "we", "connect", "intrinsic", "dimensionality", "with", "low", "dimensional", "task", "representations", "and", "compression", "based", "generalization", "bounds", "to", "provide", "intrinsic-dimension-based", "generalization", "bounds", "that", "are", "independent", "of", "the", "full", "parameter", "count", "."], "entities": [{"type": "Operation", "start": 158, "end": 162, "text": "tune a RoBERTa model", "sent_idx": 4}, {"type": "Effect", "start": 170, "end": 171, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--568"}
{"text": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.", "tokens": ["We", "study", "the", "problem", "of", "leveraging", "the", "syntactic", "structure", "of", "text", "to", "enhance", "pre-trained", "models", "such", "as", "BERT", "and", "RoBERTa", ".", "Existing", "methods", "utilize", "syntax", "of", "text", "either", "in", "the", "pre-training", "stage", "or", "in", "the", "fine-tuning", "stage", ",", "so", "that", "they", "suffer", "from", "discrepancy", "between", "the", "two", "stages", ".", "Such", "a", "problem", "would", "lead", "to", "the", "necessity", "of", "having", "human-annotated", "syntactic", "information", ",", "which", "limits", "the", "application", "of", "existing", "methods", "to", "broader", "scenarios", ".", "To", "address", "this", ",", "we", "present", "a", "model", "that", "utilizes", "the", "syntax", "of", "text", "in", "both", "pre-training", "and", "fine-tuning", "stages", ".", "Our", "model", "is", "based", "on", "Transformer", "with", "a", "syntax-aware", "attention", "layer", "that", "considers", "the", "dependency", "tree", "of", "the", "text", ".", "We", "further", "introduce", "a", "new", "pre-training", "task", "of", "predicting", "the", "syntactic", "distance", "among", "tokens", "in", "the", "dependency", "tree", ".", "We", "evaluate", "the", "model", "on", "three", "downstream", "tasks", ",", "including", "relation", "classification", ",", "entity", "typing", ",", "and", "question", "answering", ".", "Results", "show", "that", "our", "model", "achieves", "state-of-the-art", "performance", "on", "six", "public", "benchmark", "datasets", ".", "We", "have", "two", "major", "findings", ".", "First", ",", "we", "demonstrate", "that", "infusing", "automatically", "produced", "syntax", "of", "text", "improves", "pre-trained", "models", ".", "Second", ",", "global", "syntactic", "distances", "among", "tokens", "bring", "larger", "performance", "gains", "compared", "to", "local", "head", "relations", "between", "contiguous", "tokens", "."], "entities": [{"type": "Operation", "start": 191, "end": 196, "text": "global syntactic distances among tokens", "sent_idx": 10}, {"type": "Effect", "start": 198, "end": 199, "text": "performance", "sent_idx": 10}, {"type": "Operation", "start": 100, "end": 101, "text": "Transformer", "sent_idx": 4}, {"type": "Effect", "start": 161, "end": 162, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--420"}
{"text": "Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available.", "tokens": ["Image", "captioning", "is", "a", "multimodal", "problem", "that", "has", "drawn", "extensive", "attention", "in", "both", "the", "natural", "language", "processing", "and", "computer", "vision", "community", ".", "In", "this", "paper", ",", "we", "present", "a", "novel", "image", "captioning", "architecture", "to", "better", "explore", "semantics", "available", "in", "captions", "and", "leverage", "that", "to", "enhance", "both", "image", "representation", "and", "caption", "generation", ".", "Our", "models", "first", "construct", "caption-guided", "visual", "relationship", "graphs", "that", "introduce", "beneficial", "inductive", "bias", "using", "weakly", "supervised", "multi-instance", "learning", ".", "The", "representation", "is", "then", "enhanced", "with", "neighbouring", "and", "contextual", "nodes", "with", "their", "textual", "and", "visual", "features", ".", "During", "generation", ",", "the", "model", "further", "incorporates", "visual", "relationships", "using", "multi-task", "learning", "for", "jointly", "predicting", "word", "and", "object/predicate", "tag", "sequences", ".", "We", "perform", "extensive", "experiments", "on", "the", "MSCOCO", "dataset", ",", "showing", "that", "the", "proposed", "framework", "significantly", "outperforms", "the", "baselines", ",", "resulting", "in", "the", "state-of-the-art", "performance", "under", "a", "wide", "range", "of", "evaluation", "metrics", ".", "The", "code", "of", "our", "paper", "has", "been", "made", "publicly", "available", "."], "entities": [{"type": "Operation", "start": 35, "end": 40, "text": "explore semantics available in captions", "sent_idx": 1}, {"type": "Effect", "start": 132, "end": 133, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 30, "end": 33, "text": "image captioning architecture", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--664"}
{"text": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.", "tokens": ["We", "introduce", "a", "new", "task", ",", "MultiMedia", "Event", "Extraction", ",", "which", "aims", "to", "extract", "events", "and", "their", "arguments", "from", "multimedia", "documents", ".", "We", "develop", "the", "first", "benchmark", "and", "collect", "a", "dataset", "of", "245", "multimedia", "news", "articles", "with", "extensively", "annotated", "events", "and", "arguments", ".", "We", "propose", "a", "novel", "method", ",", "Weakly", "Aligned", "Structured", "Embedding", "(", "WASE", ")", ",", "that", "encodes", "structured", "representations", "of", "semantic", "information", "from", "textual", "and", "visual", "data", "into", "a", "common", "embedding", "space", ".", "The", "structures", "are", "aligned", "across", "modalities", "by", "employing", "a", "weakly", "supervised", "training", "strategy", ",", "which", "enables", "exploiting", "available", "resources", "without", "explicit", "cross-media", "annotation", ".", "Compared", "to", "uni-modal", "state-of-the-art", "methods", ",", "our", "approach", "achieves", "4.0", "%", "and", "9.8", "%", "absolute", "F-score", "gains", "on", "text", "event", "argument", "role", "labeling", "and", "visual", "event", "extraction", ".", "Compared", "to", "state-of-the-art", "multimedia", "unstructured", "representations", ",", "we", "achieve", "8.3", "%", "and", "5.0", "%", "absolute", "F-score", "gains", "on", "multimedia", "event", "extraction", "and", "argument", "role", "labeling", ",", "respectively", ".", "By", "utilizing", "images", ",", "we", "extract", "21.4", "%", "more", "event", "mentions", "than", "traditional", "text-only", "methods", "."], "entities": [{"type": "Operation", "start": 49, "end": 56, "text": "Weakly Aligned Structured Embedding (WASE)", "sent_idx": 2}, {"type": "Effect", "start": 114, "end": 115, "text": "F-score", "sent_idx": 4}, {"type": "Effect", "start": 142, "end": 143, "text": "F-score", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--230"}
{"text": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the \u201choneypot\u201d concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to \u201cbait and catch\u201d potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger\u2019s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers\u2019 varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.", "tokens": ["The", "Universal", "Trigger", "(", "UniTrigger", ")", "is", "a", "recently-proposed", "powerful", "adversarial", "textual", "attack", "method", ".", "Utilizing", "a", "learning-based", "mechanism", ",", "UniTrigger", "generates", "a", "fixed", "phrase", "that", ",", "when", "added", "to", "any", "benign", "inputs", ",", "can", "drop", "the", "prediction", "accuracy", "of", "a", "textual", "neural", "network", "(", "NN", ")", "model", "to", "near", "zero", "on", "a", "target", "class", ".", "To", "defend", "against", "this", "attack", "that", "can", "cause", "significant", "harm", ",", "in", "this", "paper", ",", "we", "borrow", "the", "\u201c", "honeypot", "\u201d", "concept", "from", "the", "cybersecurity", "community", "and", "propose", "DARCY", ",", "a", "honeypot-based", "defense", "framework", "against", "UniTrigger", ".", "DARCY", "greedily", "searches", "and", "injects", "multiple", "trapdoors", "into", "an", "NN", "model", "to", "\u201c", "bait", "and", "catch", "\u201d", "potential", "attacks", ".", "Through", "comprehensive", "experiments", "across", "four", "public", "datasets", ",", "we", "show", "that", "DARCY", "detects", "UniTrigger", "\u2019s", "adversarial", "attacks", "with", "up", "to", "99", "%", "TPR", "and", "less", "than", "2", "%", "FPR", "in", "most", "cases", ",", "while", "maintaining", "the", "prediction", "accuracy", "(", "in", "F1", ")", "for", "clean", "inputs", "within", "a", "1", "%", "margin", ".", "We", "also", "demonstrate", "that", "DARCY", "with", "multiple", "trapdoors", "is", "also", "robust", "to", "a", "diverse", "set", "of", "attack", "scenarios", "with", "attackers", "\u2019", "varying", "levels", "of", "knowledge", "and", "skills", ".", "We", "release", "the", "source", "code", "of", "DARCY", "at", ":", "https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP", "."], "entities": [{"type": "Operation", "start": 124, "end": 125, "text": "DARCY", "sent_idx": 4}, {"type": "Effect", "start": 135, "end": 136, "text": "TPR", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 142, "text": "FPR", "sent_idx": 4}, {"type": "Effect", "start": 150, "end": 151, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--296"}
{"text": "Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages\u2014namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion. We train our model directly on romanized data from two languages: Egyptian Arabic and Russian. We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model\u2019s performance on both languages, yielding results much closer to the supervised skyline. Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.", "tokens": ["Informal", "romanization", "is", "an", "idiosyncratic", "process", "used", "by", "humans", "in", "informal", "digital", "communication", "to", "encode", "non-Latin", "script", "languages", "into", "Latin", "character", "sets", "found", "on", "common", "keyboards", ".", "Character", "substitution", "choices", "differ", "between", "users", "but", "have", "been", "shown", "to", "be", "governed", "by", "the", "same", "main", "principles", "observed", "across", "a", "variety", "of", "languages", "\u2014", "namely", ",", "character", "pairs", "are", "often", "associated", "through", "phonetic", "or", "visual", "similarity", ".", "We", "propose", "a", "noisy-channel", "WFST", "cascade", "model", "for", "deciphering", "the", "original", "non-Latin", "script", "from", "observed", "romanized", "text", "in", "an", "unsupervised", "fashion", ".", "We", "train", "our", "model", "directly", "on", "romanized", "data", "from", "two", "languages", ":", "Egyptian", "Arabic", "and", "Russian", ".", "We", "demonstrate", "that", "adding", "inductive", "bias", "through", "phonetic", "and", "visual", "priors", "on", "character", "mappings", "substantially", "improves", "the", "model", "\u2019s", "performance", "on", "both", "languages", ",", "yielding", "results", "much", "closer", "to", "the", "supervised", "skyline", ".", "Finally", ",", "we", "introduce", "a", "new", "dataset", "of", "romanized", "Russian", ",", "collected", "from", "a", "Russian", "social", "network", "website", "and", "partially", "annotated", "for", "our", "experiments", "."], "entities": [{"type": "Operation", "start": 107, "end": 118, "text": "adding inductive bias through phonetic and visual priors on character mappings", "sent_idx": 4}, {"type": "Effect", "start": 123, "end": 124, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--737"}
{"text": "Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.", "tokens": ["Adversarial", "attacks", "are", "carried", "out", "to", "reveal", "the", "vulnerability", "of", "deep", "neural", "networks", ".", "Textual", "adversarial", "attacking", "is", "challenging", "because", "text", "is", "discrete", "and", "a", "small", "perturbation", "can", "bring", "significant", "change", "to", "the", "original", "input", ".", "Word-level", "attacking", ",", "which", "can", "be", "regarded", "as", "a", "combinatorial", "optimization", "problem", ",", "is", "a", "well-studied", "class", "of", "textual", "attack", "methods", ".", "However", ",", "existing", "word-level", "attack", "models", "are", "far", "from", "perfect", ",", "largely", "because", "unsuitable", "search", "space", "reduction", "methods", "and", "inefficient", "optimization", "algorithms", "are", "employed", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "attack", "model", ",", "which", "incorporates", "the", "sememe-based", "word", "substitution", "method", "and", "particle", "swarm", "optimization-based", "search", "algorithm", "to", "solve", "the", "two", "problems", "separately", ".", "We", "conduct", "exhaustive", "experiments", "to", "evaluate", "our", "attack", "model", "by", "attacking", "BiLSTM", "and", "BERT", "on", "three", "benchmark", "datasets", ".", "Experimental", "results", "demonstrate", "that", "our", "model", "consistently", "achieves", "much", "higher", "attack", "success", "rates", "and", "crafts", "more", "high-quality", "adversarial", "examples", "as", "compared", "to", "baseline", "methods", ".", "Also", ",", "further", "experiments", "show", "our", "model", "has", "higher", "transferability", "and", "can", "bring", "more", "robustness", "enhancement", "to", "victim", "models", "by", "adversarial", "training", ".", "All", "the", "code", "and", "data", "of", "this", "paper", "can", "be", "obtained", "on", "https://github.com/thunlp/SememePSO-Attack", "."], "entities": [{"type": "Operation", "start": 178, "end": 180, "text": "adversarial training", "sent_idx": 7}, {"type": "Effect", "start": 172, "end": 173, "text": "robustness", "sent_idx": 7}, {"type": "Operation", "start": 95, "end": 107, "text": "incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm", "sent_idx": 4}, {"type": "Effect", "start": 143, "end": 146, "text": "attack success rates", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--540"}
{"text": "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English<->German) and a self-collected bilingual dialogue corpus, named BMELD (English<->Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community.", "tokens": ["Neural", "chat", "translation", "aims", "to", "translate", "bilingual", "conversational", "text", ",", "which", "has", "a", "broad", "application", "in", "international", "exchanges", "and", "cooperation", ".", "Despite", "the", "impressive", "performance", "of", "sentence-level", "and", "context-aware", "Neural", "Machine", "Translation", "(", "NMT", ")", ",", "there", "still", "remain", "challenges", "to", "translate", "bilingual", "conversational", "text", "due", "to", "its", "inherent", "characteristics", "such", "as", "role", "preference", ",", "dialogue", "coherence", ",", "and", "translation", "consistency", ".", "In", "this", "paper", ",", "we", "aim", "to", "promote", "the", "translation", "quality", "of", "conversational", "text", "by", "modeling", "the", "above", "properties", ".", "Specifically", ",", "we", "design", "three", "latent", "variational", "modules", "to", "learn", "the", "distributions", "of", "bilingual", "conversational", "characteristics", ".", "Through", "sampling", "from", "these", "learned", "distributions", ",", "the", "latent", "variables", ",", "tailored", "for", "role", "preference", ",", "dialogue", "coherence", ",", "and", "translation", "consistency", ",", "are", "incorporated", "into", "the", "NMT", "model", "for", "better", "translation", ".", "We", "evaluate", "our", "approach", "on", "the", "benchmark", "dataset", "BConTrasT", "(", "English<->German", ")", "and", "a", "self-collected", "bilingual", "dialogue", "corpus", ",", "named", "BMELD", "(", "English<->Chinese", ")", ".", "Extensive", "experiments", "show", "that", "our", "approach", "notably", "boosts", "the", "performance", "over", "strong", "baselines", "by", "a", "large", "margin", "and", "significantly", "surpasses", "some", "state-of-the-art", "context-aware", "NMT", "models", "in", "terms", "of", "BLEU", "and", "TER", ".", "Additionally", ",", "we", "make", "the", "BMELD", "dataset", "publicly", "available", "for", "the", "research", "community", "."], "entities": [{"type": "Operation", "start": 85, "end": 90, "text": "design three latent variational modules", "sent_idx": 3}, {"type": "Effect", "start": 166, "end": 167, "text": "performance", "sent_idx": 6}, {"type": "Effect", "start": 185, "end": 186, "text": "BLEU", "sent_idx": 6}, {"type": "Effect", "start": 187, "end": 188, "text": "TER", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--444"}
{"text": "We present an unsupervised model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.", "tokens": ["We", "present", "an", "unsupervised", "model", "for", "joint", "phrase", "alignment", "and", "extraction", "using", "non-parametric", "Bayesian", "methods", "and", "inversion", "transduction", "grammars", "(", "ITGs", ")", ".", "The", "key", "contribution", "is", "that", "phrases", "of", "many", "granularities", "are", "included", "directly", "in", "the", "model", "through", "the", "use", "of", "a", "novel", "formulation", "that", "memorizes", "phrases", "generated", "not", "only", "by", "terminal", ",", "but", "also", "non-terminal", "symbols", ".", "This", "allows", "for", "a", "completely", "probabilistic", "model", "that", "is", "able", "to", "create", "a", "phrase", "table", "that", "achieves", "competitive", "accuracy", "on", "phrase-based", "machine", "translation", "tasks", "directly", "from", "unaligned", "sentence", "pairs", ".", "Experiments", "on", "several", "language", "pairs", "demonstrate", "that", "the", "proposed", "model", "matches", "the", "accuracy", "of", "traditional", "two-step", "word", "alignment/phrase", "extraction", "approach", "while", "reducing", "the", "phrase", "table", "to", "a", "fraction", "of", "the", "original", "size", "."], "entities": [{"type": "Operation", "start": 63, "end": 66, "text": "completely probabilistic model", "sent_idx": 2}, {"type": "Effect", "start": 77, "end": 78, "text": "accuracy", "sent_idx": 2}, {"type": "Effect", "start": 101, "end": 102, "text": "accuracy", "sent_idx": 3}, {"type": "Effect", "start": 112, "end": 114, "text": "phrase table", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Neg_Affect", "head": 0, "tail": 3}], "id": "P11-1064"}
{"text": "The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.", "tokens": ["The", "commonly", "used", "framework", "for", "unsupervised", "machine", "translation", "builds", "initial", "translation", "models", "of", "both", "translation", "directions", ",", "and", "then", "performs", "iterative", "back-translation", "to", "jointly", "boost", "their", "translation", "performance", ".", "The", "initialization", "stage", "is", "very", "important", "since", "bad", "initialization", "may", "wrongly", "squeeze", "the", "search", "space", ",", "and", "too", "much", "noise", "introduced", "in", "this", "stage", "may", "hurt", "the", "final", "performance", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "retrieval", "and", "rewriting", "based", "method", "to", "better", "initialize", "unsupervised", "translation", "models", ".", "We", "first", "retrieve", "semantically", "comparable", "sentences", "from", "monolingual", "corpora", "of", "two", "languages", "and", "then", "rewrite", "the", "target", "side", "to", "minimize", "the", "semantic", "gap", "between", "the", "source", "and", "retrieved", "targets", "with", "a", "designed", "rewriting", "model", ".", "The", "rewritten", "sentence", "pairs", "are", "used", "to", "initialize", "SMT", "models", "which", "are", "used", "to", "generate", "pseudo", "data", "for", "two", "NMT", "models", ",", "followed", "by", "the", "iterative", "back-translation", ".", "Experiments", "show", "that", "our", "method", "can", "build", "better", "initial", "unsupervised", "translation", "models", "and", "improve", "the", "final", "translation", "performance", "by", "over", "4", "BLEU", "scores", ".", "Our", "code", "is", "released", "at", "https://github.com/Imagist-Shuo/RRforUNMT.git", "."], "entities": [{"type": "Operation", "start": 67, "end": 72, "text": "retrieval and rewriting based method", "sent_idx": 2}, {"type": "Effect", "start": 159, "end": 160, "text": "performance", "sent_idx": 5}, {"type": "Effect", "start": 163, "end": 165, "text": "BLEU scores", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--320"}
{"text": "Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.", "tokens": ["Discovering", "the", "stances", "of", "media", "outlets", "and", "influential", "people", "on", "current", ",", "debatable", "topics", "is", "important", "for", "social", "statisticians", "and", "policy", "makers", ".", "Many", "supervised", "solutions", "exist", "for", "determining", "viewpoints", ",", "but", "manually", "annotating", "training", "data", "is", "costly", ".", "In", "this", "paper", ",", "we", "propose", "a", "cascaded", "method", "that", "uses", "unsupervised", "learning", "to", "ascertain", "the", "stance", "of", "Twitter", "users", "with", "respect", "to", "a", "polarizing", "topic", "by", "leveraging", "their", "retweet", "behavior", ";", "then", ",", "it", "uses", "supervised", "learning", "based", "on", "user", "labels", "to", "characterize", "both", "the", "general", "political", "leaning", "of", "online", "media", "and", "of", "popular", "Twitter", "users", ",", "as", "well", "as", "their", "stance", "with", "respect", "to", "the", "target", "polarizing", "topic", ".", "We", "evaluate", "the", "model", "by", "comparing", "its", "predictions", "to", "gold", "labels", "from", "the", "Media", "Bias/Fact", "Check", "website", ",", "achieving", "82.6", "%", "accuracy", "."], "entities": [{"type": "Operation", "start": 46, "end": 48, "text": "cascaded method", "sent_idx": 2}, {"type": "Effect", "start": 131, "end": 132, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--50"}
{"text": "Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a \"crime\" event can cause a \"investigation\" event, which can lead to an \"arrest\" event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP'09 shared task with a F1 score of 53.5% in development and 48.6% in testing.", "tokens": ["Nested", "event", "structures", "are", "a", "common", "occurrence", "in", "both", "open", "domain", "and", "domain", "specific", "extraction", "tasks", ",", "e.g.", ",", "a", "\"", "crime", "\"", "event", "can", "cause", "a", "\"", "investigation", "\"", "event", ",", "which", "can", "lead", "to", "an", "\"", "arrest", "\"", "event", ".", "However", ",", "most", "current", "approaches", "address", "event", "extraction", "with", "highly", "local", "models", "that", "extract", "each", "event", "and", "argument", "independently", ".", "We", "propose", "a", "simple", "approach", "for", "the", "extraction", "of", "such", "structures", "by", "taking", "the", "tree", "of", "event-argument", "relations", "and", "using", "it", "directly", "as", "the", "representation", "in", "a", "reranking", "dependency", "parser", ".", "This", "provides", "a", "simple", "framework", "that", "captures", "global", "properties", "of", "both", "nested", "and", "flat", "event", "structures", ".", "We", "explore", "a", "rich", "feature", "space", "that", "models", "both", "the", "events", "to", "be", "parsed", "and", "context", "from", "the", "original", "supporting", "text", ".", "Our", "approach", "obtains", "competitive", "results", "in", "the", "extraction", "of", "biomedical", "events", "from", "the", "BioNLP'09", "shared", "task", "with", "a", "F1", "score", "of", "53.5", "%", "in", "development", "and", "48.6", "%", "in", "testing", "."], "entities": [{"type": "Operation", "start": 74, "end": 80, "text": "taking the tree of event-argument relations", "sent_idx": 2}, {"type": "Effect", "start": 150, "end": 152, "text": "F1 score", "sent_idx": 5}, {"type": "Operation", "start": 81, "end": 92, "text": "using it directly as the representation in a reranking dependency parser", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "P11-1163"}
{"text": "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.", "tokens": ["Parody", "is", "a", "figurative", "device", "used", "to", "imitate", "an", "entity", "for", "comedic", "or", "critical", "purposes", "and", "represents", "a", "widespread", "phenomenon", "in", "social", "media", "through", "many", "popular", "parody", "accounts", ".", "In", "this", "paper", ",", "we", "present", "the", "first", "computational", "study", "of", "parody", ".", "We", "introduce", "a", "new", "publicly", "available", "data", "set", "of", "tweets", "from", "real", "politicians", "and", "their", "corresponding", "parody", "accounts", ".", "We", "run", "a", "battery", "of", "supervised", "machine", "learning", "models", "for", "automatically", "detecting", "parody", "tweets", "with", "an", "emphasis", "on", "robustness", "by", "testing", "on", "tweets", "from", "accounts", "unseen", "in", "training", ",", "across", "different", "genders", "and", "across", "countries", ".", "Our", "results", "show", "that", "political", "parody", "tweets", "can", "be", "predicted", "with", "an", "accuracy", "up", "to", "90", "%", ".", "Finally", ",", "we", "identify", "the", "markers", "of", "parody", "through", "a", "linguistic", "analysis", ".", "Beyond", "research", "in", "linguistics", "and", "political", "communication", ",", "accurately", "and", "automatically", "detecting", "parody", "is", "important", "to", "improving", "fact", "checking", "for", "journalists", "and", "analytics", "such", "as", "sentiment", "analysis", "through", "filtering", "out", "parodical", "utterances", "."], "entities": [{"type": "Operation", "start": 66, "end": 70, "text": "supervised machine learning models", "sent_idx": 3}, {"type": "Effect", "start": 109, "end": 110, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--403"}
{"text": "Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.", "tokens": ["Based", "on", "the", "recently", "proposed", "transferable", "dialogue", "state", "generator", "(", "TRADE", ")", "that", "predicts", "dialogue", "states", "from", "utterance-concatenated", "dialogue", "context", ",", "we", "propose", "a", "multi-task", "learning", "model", "with", "a", "simple", "yet", "effective", "utterance", "tagging", "technique", "and", "a", "bidirectional", "language", "model", "as", "an", "auxiliary", "task", "for", "task-oriented", "dialogue", "state", "generation", ".", "By", "enabling", "the", "model", "to", "learn", "a", "better", "representation", "of", "the", "long", "dialogue", "context", ",", "our", "approaches", "attempt", "to", "solve", "the", "problem", "that", "the", "performance", "of", "the", "baseline", "significantly", "drops", "when", "the", "input", "dialogue", "context", "sequence", "is", "long", ".", "In", "our", "experiments", ",", "our", "proposed", "model", "achieves", "a", "7.03", "%", "relative", "improvement", "over", "the", "baseline", ",", "establishing", "a", "new", "state-of-the-art", "joint", "goal", "accuracy", "of", "52.04", "%", "on", "the", "MultiWOZ", "2.0", "dataset", "."], "entities": [{"type": "Operation", "start": 37, "end": 40, "text": "bidirectional language model", "sent_idx": 0}, {"type": "Effect", "start": 110, "end": 113, "text": "joint goal accuracy", "sent_idx": 2}, {"type": "Operation", "start": 24, "end": 27, "text": "multi-task learning model", "sent_idx": 0}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--637"}
{"text": "Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human\u2019s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder. On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG. Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge.", "tokens": ["Given", "a", "sentence", "and", "its", "relevant", "answer", ",", "how", "to", "ask", "good", "questions", "is", "a", "challenging", "task", ",", "which", "has", "many", "real", "applications", ".", "Inspired", "by", "human", "\u2019s", "paraphrasing", "capability", "to", "ask", "questions", "of", "the", "same", "meaning", "but", "with", "diverse", "expressions", ",", "we", "propose", "to", "incorporate", "paraphrase", "knowledge", "into", "question", "generation(QG", ")", "to", "generate", "human-like", "questions", ".", "Specifically", ",", "we", "present", "a", "two-hand", "hybrid", "model", "leveraging", "a", "self-built", "paraphrase", "resource", ",", "which", "is", "automatically", "conducted", "by", "a", "simple", "back-translation", "method", ".", "On", "the", "one", "hand", ",", "we", "conduct", "multi-task", "learning", "with", "sentence-level", "paraphrase", "generation", "(", "PG", ")", "as", "an", "auxiliary", "task", "to", "supplement", "paraphrase", "knowledge", "to", "the", "task-share", "encoder", ".", "On", "the", "other", "hand", ",", "we", "adopt", "a", "new", "loss", "function", "for", "diversity", "training", "to", "introduce", "more", "question", "patterns", "to", "QG", ".", "Extensive", "experimental", "results", "show", "that", "our", "proposed", "model", "obtains", "obvious", "performance", "gain", "over", "several", "strong", "baselines", ",", "and", "further", "human", "evaluation", "validates", "that", "our", "model", "can", "ask", "questions", "of", "high", "quality", "by", "leveraging", "paraphrase", "knowledge", "."], "entities": [{"type": "Operation", "start": 62, "end": 70, "text": "two-hand hybrid model leveraging a self-built paraphrase resource", "sent_idx": 2}, {"type": "Effect", "start": 142, "end": 143, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--545"}
{"text": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.", "tokens": ["We", "propose", "a", "method", "for", "program", "generation", "based", "on", "semantic", "scaffolds", ",", "lightweight", "structures", "representing", "the", "high-level", "semantic", "and", "syntactic", "composition", "of", "a", "program", ".", "By", "first", "searching", "over", "plausible", "scaffolds", "then", "using", "these", "as", "constraints", "for", "a", "beam", "search", "over", "programs", ",", "we", "achieve", "better", "coverage", "of", "the", "search", "space", "when", "compared", "with", "existing", "techniques", ".", "We", "apply", "our", "hierarchical", "search", "method", "to", "the", "SPoC", "dataset", "for", "pseudocode-to-code", "generation", ",", "in", "which", "we", "are", "given", "line-level", "natural", "language", "pseudocode", "annotations", "and", "aim", "to", "produce", "a", "program", "satisfying", "execution-based", "test", "cases", ".", "By", "using", "semantic", "scaffolds", "during", "inference", ",", "we", "achieve", "a", "10", "%", "absolute", "improvement", "in", "top-100", "accuracy", "over", "the", "previous", "state-of-the-art", ".", "Additionally", ",", "we", "require", "only", "11", "candidates", "to", "reach", "the", "top-3000", "performance", "of", "the", "previous", "best", "approach", "when", "tested", "against", "unseen", "problems", ",", "demonstrating", "a", "substantial", "improvement", "in", "efficiency", "."], "entities": [{"type": "Operation", "start": 93, "end": 98, "text": "using semantic scaffolds during inference", "sent_idx": 3}, {"type": "Effect", "start": 107, "end": 109, "text": "top-100 accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--208"}
{"text": "Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.", "tokens": ["Monolingual", "word", "alignment", "is", "important", "for", "studying", "fine-grained", "editing", "operations", "(", "i.e.", ",", "deletion", ",", "addition", ",", "and", "substitution", ")", "in", "text-to-text", "generation", "tasks", ",", "such", "as", "paraphrase", "generation", ",", "text", "simplification", ",", "neutralizing", "biased", "language", ",", "etc", ".", "In", "this", "paper", ",", "we", "present", "a", "novel", "neural", "semi-Markov", "CRF", "alignment", "model", ",", "which", "unifies", "word", "and", "phrase", "alignments", "through", "variable-length", "spans", ".", "We", "also", "create", "a", "new", "benchmark", "with", "human", "annotations", "that", "cover", "four", "different", "text", "genres", "to", "evaluate", "monolingual", "word", "alignment", "models", "in", "more", "realistic", "settings", ".", "Experimental", "results", "show", "that", "our", "proposed", "model", "outperforms", "all", "previous", "approaches", "for", "monolingual", "word", "alignment", "as", "well", "as", "a", "competitive", "QA-based", "baseline", ",", "which", "was", "previously", "only", "applied", "to", "bilingual", "data", ".", "Our", "model", "demonstrates", "good", "generalizability", "to", "three", "out-of-domain", "datasets", "and", "shows", "great", "utility", "in", "two", "downstream", "applications", ":", "automatic", "text", "simplification", "and", "sentence", "pair", "classification", "tasks", "."], "entities": [{"type": "Operation", "start": 47, "end": 52, "text": "neural semi-Markov CRF alignment model", "sent_idx": 1}, {"type": "Effect", "start": 125, "end": 126, "text": "generalizability", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--531"}
{"text": "This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.", "tokens": ["This", "paper", "presents", "a", "supervised", "pronoun", "anaphora", "resolution", "system", "based", "on", "factorial", "hidden", "Markov", "models", "(", "FHMMs", ")", ".", "The", "basic", "idea", "is", "that", "the", "hidden", "states", "of", "FHMMs", "are", "an", "explicit", "short-term", "memory", "with", "an", "antecedent", "buffer", "containing", "recently", "described", "referents", ".", "Thus", "an", "observed", "pronoun", "can", "find", "its", "antecedent", "from", "the", "hidden", "buffer", ",", "or", "in", "terms", "of", "a", "generative", "model", ",", "the", "entries", "in", "the", "hidden", "buffer", "generate", "the", "corresponding", "pronouns", ".", "A", "system", "implementing", "this", "model", "is", "evaluated", "on", "the", "ACE", "corpus", "with", "promising", "performance", "."], "entities": [{"type": "Operation", "start": 4, "end": 9, "text": "supervised pronoun anaphora resolution system", "sent_idx": 0}, {"type": "Effect", "start": 88, "end": 89, "text": "performance", "sent_idx": 3}, {"type": "Operation", "start": 11, "end": 18, "text": "factorial hidden Markov models (FHMMs)", "sent_idx": 0}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "P11-1117"}
{"text": "The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.", "tokens": ["The", "Transformer", "translation", "model", "employs", "residual", "connection", "and", "layer", "normalization", "to", "ease", "the", "optimization", "difficulties", "caused", "by", "its", "multi-layer", "encoder/decoder", "structure", ".", "Previous", "research", "shows", "that", "even", "with", "residual", "connection", "and", "layer", "normalization", ",", "deep", "Transformers", "still", "have", "difficulty", "in", "training", ",", "and", "particularly", "Transformer", "models", "with", "more", "than", "12", "encoder/decoder", "layers", "fail", "to", "converge", ".", "In", "this", "paper", ",", "we", "first", "empirically", "demonstrate", "that", "a", "simple", "modification", "made", "in", "the", "official", "implementation", ",", "which", "changes", "the", "computation", "order", "of", "residual", "connection", "and", "layer", "normalization", ",", "can", "significantly", "ease", "the", "optimization", "of", "deep", "Transformers", ".", "We", "then", "compare", "the", "subtle", "differences", "in", "computation", "order", "in", "considerable", "detail", ",", "and", "present", "a", "parameter", "initialization", "method", "that", "leverages", "the", "Lipschitz", "constraint", "on", "the", "initialization", "of", "Transformer", "parameters", "that", "effectively", "ensures", "training", "convergence", ".", "In", "contrast", "to", "findings", "in", "previous", "research", "we", "further", "demonstrate", "that", "with", "Lipschitz", "parameter", "initialization", ",", "deep", "Transformers", "with", "the", "original", "computation", "order", "can", "converge", ",", "and", "obtain", "significant", "BLEU", "improvements", "with", "up", "to", "24", "layers", ".", "In", "contrast", "to", "previous", "research", "which", "focuses", "on", "deep", "encoders", ",", "our", "approach", "additionally", "enables", "Transformers", "to", "also", "benefit", "from", "deep", "decoders", "."], "entities": [{"type": "Operation", "start": 143, "end": 146, "text": "Lipschitz parameter initialization", "sent_idx": 4}, {"type": "Effect", "start": 160, "end": 161, "text": "BLEU", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--38"}
{"text": "Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making.", "tokens": ["Nowadays", ",", "the", "interpretability", "of", "machine", "learning", "models", "is", "becoming", "increasingly", "important", ",", "especially", "in", "the", "medical", "domain", ".", "Aiming", "to", "shed", "some", "light", "on", "how", "to", "rationalize", "medical", "relation", "prediction", ",", "we", "present", "a", "new", "interpretable", "framework", "inspired", "by", "existing", "theories", "on", "how", "human", "memory", "works", ",", "e.g.", ",", "theories", "of", "recall", "and", "recognition", ".", "Given", "the", "corpus-level", "statistics", ",", "i.e.", ",", "a", "global", "co-occurrence", "graph", "of", "a", "clinical", "text", "corpus", ",", "to", "predict", "the", "relations", "between", "two", "entities", ",", "we", "first", "recall", "rich", "contexts", "associated", "with", "the", "target", "entities", ",", "and", "then", "recognize", "relational", "interactions", "between", "these", "contexts", "to", "form", "model", "rationales", ",", "which", "will", "contribute", "to", "the", "final", "prediction", ".", "We", "conduct", "experiments", "on", "a", "real-world", "public", "clinical", "dataset", "and", "show", "that", "our", "framework", "can", "not", "only", "achieve", "competitive", "predictive", "performance", "against", "a", "comprehensive", "list", "of", "neural", "baseline", "models", ",", "but", "also", "present", "rationales", "to", "justify", "its", "prediction", ".", "We", "further", "collaborate", "with", "medical", "experts", "deeply", "to", "verify", "the", "usefulness", "of", "our", "model", "rationales", "for", "clinical", "decision", "making", "."], "entities": [{"type": "Operation", "start": 125, "end": 127, "text": "our framework", "sent_idx": 3}, {"type": "Effect", "start": 132, "end": 134, "text": "predictive performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--719"}
{"text": "Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model\u2019s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT\u2019s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.", "tokens": ["Pretrained", "neural", "models", "such", "as", "BERT", ",", "when", "fine-tuned", "to", "perform", "natural", "language", "inference", "(", "NLI", ")", ",", "often", "show", "high", "accuracy", "on", "standard", "datasets", ",", "but", "display", "a", "surprising", "lack", "of", "sensitivity", "to", "word", "order", "on", "controlled", "challenge", "sets", ".", "We", "hypothesize", "that", "this", "issue", "is", "not", "primarily", "caused", "by", "the", "pretrained", "model", "\u2019s", "limitations", ",", "but", "rather", "by", "the", "paucity", "of", "crowdsourced", "NLI", "examples", "that", "might", "convey", "the", "importance", "of", "syntactic", "structure", "at", "the", "fine-tuning", "stage", ".", "We", "explore", "several", "methods", "to", "augment", "standard", "training", "sets", "with", "syntactically", "informative", "examples", ",", "generated", "by", "applying", "syntactic", "transformations", "to", "sentences", "from", "the", "MNLI", "corpus", ".", "The", "best-performing", "augmentation", "method", ",", "subject/object", "inversion", ",", "improved", "BERT", "\u2019s", "accuracy", "on", "controlled", "examples", "that", "diagnose", "sensitivity", "to", "word", "order", "from", "0.28", "to", "0.73", ",", "without", "affecting", "performance", "on", "the", "MNLI", "test", "set", ".", "This", "improvement", "generalized", "beyond", "the", "particular", "construction", "used", "for", "data", "augmentation", ",", "suggesting", "that", "augmentation", "causes", "BERT", "to", "recruit", "abstract", "syntactic", "representations", "."], "entities": [{"type": "Operation", "start": 110, "end": 112, "text": "subject/object inversion", "sent_idx": 3}, {"type": "Effect", "start": 114, "end": 117, "text": "BERT\u2019s accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--212"}
{"text": "Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs. To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit. The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels. To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks. Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.", "tokens": ["Opinion", "entity", "extraction", "is", "a", "fundamental", "task", "in", "fine-grained", "opinion", "mining", ".", "Related", "studies", "generally", "extract", "aspects", "and/or", "opinion", "expressions", "without", "recognizing", "the", "relations", "between", "them", ".", "However", ",", "the", "relations", "are", "crucial", "for", "downstream", "tasks", ",", "including", "sentiment", "classification", ",", "opinion", "summarization", ",", "etc", ".", "In", "this", "paper", ",", "we", "explore", "Aspect-Opinion", "Pair", "Extraction", "(", "AOPE", ")", "task", ",", "which", "aims", "at", "extracting", "aspects", "and", "opinion", "expressions", "in", "pairs", ".", "To", "deal", "with", "this", "task", ",", "we", "propose", "Synchronous", "Double-channel", "Recurrent", "Network", "(", "SDRN", ")", "mainly", "consisting", "of", "an", "opinion", "entity", "extraction", "unit", ",", "a", "relation", "detection", "unit", ",", "and", "a", "synchronization", "unit", ".", "The", "opinion", "entity", "extraction", "unit", "and", "the", "relation", "detection", "unit", "are", "developed", "as", "two", "channels", "to", "extract", "opinion", "entities", "and", "relations", "simultaneously", ".", "Furthermore", ",", "within", "the", "synchronization", "unit", ",", "we", "design", "Entity", "Synchronization", "Mechanism", "(", "ESM", ")", "and", "Relation", "Synchronization", "Mechanism", "(", "RSM", ")", "to", "enhance", "the", "mutual", "benefit", "on", "the", "above", "two", "channels", ".", "To", "verify", "the", "performance", "of", "SDRN", ",", "we", "manually", "build", "three", "datasets", "based", "on", "SemEval", "2014", "and", "2015", "benchmarks", ".", "Extensive", "experiments", "demonstrate", "that", "SDRN", "achieves", "state-of-the-art", "performances", "."], "entities": [{"type": "Operation", "start": 185, "end": 186, "text": "SDRN", "sent_idx": 8}, {"type": "Effect", "start": 188, "end": 189, "text": "performances", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--582"}
{"text": "This paper studies a new problem setting of entity alignment for knowledge graphs (KGs). Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. The framework can opt to abstain from predicting alignment for the detected dangling entities. We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking. After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. Comprehensive experiments and analyses demonstrate the effectiveness of our framework. We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. The contributed resource is publicly available to foster further research.", "tokens": ["This", "paper", "studies", "a", "new", "problem", "setting", "of", "entity", "alignment", "for", "knowledge", "graphs", "(", "KGs", ")", ".", "Since", "KGs", "possess", "different", "sets", "of", "entities", ",", "there", "could", "be", "entities", "that", "can", "not", "find", "alignment", "across", "them", ",", "leading", "to", "the", "problem", "of", "dangling", "entities", ".", "As", "the", "first", "attempt", "to", "this", "problem", ",", "we", "construct", "a", "new", "dataset", "and", "design", "a", "multi-task", "learning", "framework", "for", "both", "entity", "alignment", "and", "dangling", "entity", "detection", ".", "The", "framework", "can", "opt", "to", "abstain", "from", "predicting", "alignment", "for", "the", "detected", "dangling", "entities", ".", "We", "propose", "three", "techniques", "for", "dangling", "entity", "detection", "that", "are", "based", "on", "the", "distribution", "of", "nearest-neighbor", "distances", ",", "i.e.", ",", "nearest", "neighbor", "classification", ",", "marginal", "ranking", "and", "background", "ranking", ".", "After", "detecting", "and", "removing", "dangling", "entities", ",", "an", "incorporated", "entity", "alignment", "model", "in", "our", "framework", "can", "provide", "more", "robust", "alignment", "for", "remaining", "entities", ".", "Comprehensive", "experiments", "and", "analyses", "demonstrate", "the", "effectiveness", "of", "our", "framework", ".", "We", "further", "discover", "that", "the", "dangling", "entity", "detection", "module", "can", ",", "in", "turn", ",", "improve", "alignment", "learning", "and", "the", "final", "performance", ".", "The", "contributed", "resource", "is", "publicly", "available", "to", "foster", "further", "research", "."], "entities": [{"type": "Operation", "start": 158, "end": 162, "text": "dangling entity detection module", "sent_idx": 7}, {"type": "Effect", "start": 173, "end": 174, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--278"}
{"text": "Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation.", "tokens": ["Evaluating", "image", "captions", "is", "very", "challenging", "partially", "due", "to", "the", "fact", "that", "there", "are", "multiple", "correct", "captions", "for", "every", "single", "image", ".", "Most", "of", "the", "existing", "one-to-one", "metrics", "operate", "by", "penalizing", "mismatches", "between", "reference", "and", "generative", "caption", "without", "considering", "the", "intrinsic", "variance", "between", "ground", "truth", "captions", ".", "It", "usually", "leads", "to", "over-penalization", "and", "thus", "a", "bad", "correlation", "to", "human", "judgment", ".", "Recently", ",", "the", "latest", "one-to-one", "metric", "BERTScore", "can", "achieve", "high", "human", "correlation", "in", "system-level", "tasks", "while", "some", "issues", "can", "be", "fixed", "for", "better", "performance", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "metric", "based", "on", "BERTScore", "that", "could", "handle", "such", "a", "challenge", "and", "extend", "BERTScore", "with", "a", "few", "new", "features", "appropriately", "for", "image", "captioning", "evaluation", ".", "The", "experimental", "results", "show", "that", "our", "metric", "achieves", "state-of-the-art", "human", "judgment", "correlation", "."], "entities": [{"type": "Operation", "start": 65, "end": 68, "text": "one-to-one metric BERTScore", "sent_idx": 3}, {"type": "Effect", "start": 71, "end": 73, "text": "human correlation", "sent_idx": 3}, {"type": "Effect", "start": 84, "end": 85, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--93"}
{"text": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.", "tokens": ["We", "propose", "Generation-Augmented", "Retrieval", "(", "GAR", ")", "for", "answering", "open-domain", "questions", ",", "which", "augments", "a", "query", "through", "text", "generation", "of", "heuristically", "discovered", "relevant", "contexts", "without", "external", "resources", "as", "supervision", ".", "We", "demonstrate", "that", "the", "generated", "contexts", "substantially", "enrich", "the", "semantics", "of", "the", "queries", "and", "GAR", "with", "sparse", "representations", "(", "BM25", ")", "achieves", "comparable", "or", "better", "performance", "than", "state-of-the-art", "dense", "retrieval", "methods", "such", "as", "DPR", ".", "We", "show", "that", "generating", "diverse", "contexts", "for", "a", "query", "is", "beneficial", "as", "fusing", "their", "results", "consistently", "yields", "better", "retrieval", "accuracy", ".", "Moreover", ",", "as", "sparse", "and", "dense", "representations", "are", "often", "complementary", ",", "GAR", "can", "be", "easily", "combined", "with", "DPR", "to", "achieve", "even", "better", "performance", ".", "GAR", "achieves", "state-of-the-art", "performance", "on", "Natural", "Questions", "and", "TriviaQA", "datasets", "under", "the", "extractive", "QA", "setup", "when", "equipped", "with", "an", "extractive", "reader", ",", "and", "consistently", "outperforms", "other", "retrieval", "methods", "when", "the", "same", "generative", "reader", "is", "used", "."], "entities": [{"type": "Operation", "start": 97, "end": 98, "text": "GAR", "sent_idx": 3}, {"type": "Effect", "start": 108, "end": 109, "text": "performance", "sent_idx": 3}, {"type": "Operation", "start": 110, "end": 111, "text": "GAR", "sent_idx": 4}, {"type": "Effect", "start": 113, "end": 114, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--316"}
{"text": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model\u2019s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.", "tokens": ["Biomedical", "Information", "Extraction", "from", "scientific", "literature", "presents", "two", "unique", "and", "non-trivial", "challenges", ".", "First", ",", "compared", "with", "general", "natural", "language", "texts", ",", "sentences", "from", "scientific", "papers", "usually", "possess", "wider", "contexts", "between", "knowledge", "elements", ".", "Moreover", ",", "comprehending", "the", "fine-grained", "scientific", "entities", "and", "events", "urgently", "requires", "domain-specific", "background", "knowledge", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "biomedical", "Information", "Extraction", "(", "IE", ")", "model", "to", "tackle", "these", "two", "challenges", "and", "extract", "scientific", "entities", "and", "events", "from", "English", "research", "papers", ".", "We", "perform", "Abstract", "Meaning", "Representation", "(", "AMR", ")", "to", "compress", "the", "wide", "context", "to", "uncover", "a", "clear", "semantic", "structure", "for", "each", "complex", "sentence", ".", "Besides", ",", "we", "construct", "the", "sentence-level", "knowledge", "graph", "from", "an", "external", "knowledge", "base", "and", "use", "it", "to", "enrich", "the", "AMR", "graph", "to", "improve", "the", "model", "\u2019s", "understanding", "of", "complex", "scientific", "concepts", ".", "We", "use", "an", "edge-conditioned", "graph", "attention", "network", "to", "encode", "the", "knowledge-enriched", "AMR", "graph", "for", "biomedical", "IE", "tasks", ".", "Experiments", "on", "the", "GENIA", "2011", "dataset", "show", "that", "the", "AMR", "and", "external", "knowledge", "have", "contributed", "1.8", "%", "and", "3.0", "%", "absolute", "F-score", "gains", "respectively", ".", "In", "order", "to", "evaluate", "the", "impact", "of", "our", "approach", "on", "real-world", "problems", "that", "involve", "topic-specific", "fine-grained", "knowledge", "elements", ",", "we", "have", "also", "created", "a", "new", "ontology", "and", "annotated", "corpus", "for", "entity", "and", "event", "extraction", "for", "the", "COVID-19", "scientific", "literature", ",", "which", "can", "serve", "as", "a", "new", "benchmark", "for", "the", "biomedical", "IE", "community", "."], "entities": [{"type": "Operation", "start": 163, "end": 164, "text": "AMR", "sent_idx": 7}, {"type": "Effect", "start": 175, "end": 176, "text": "F-score", "sent_idx": 7}, {"type": "Operation", "start": 165, "end": 167, "text": "external knowledge", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--489"}
{"text": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.", "tokens": ["In", "this", "paper", ",", "we", "introduce", "a", "novel", "methodology", "to", "efficiently", "construct", "a", "corpus", "for", "question", "answering", "over", "structured", "data", ".", "For", "this", ",", "we", "introduce", "an", "intermediate", "representation", "that", "is", "based", "on", "the", "logical", "query", "plan", "in", "a", "database", ",", "called", "Operation", "Trees", "(", "OT", ")", ".", "This", "representation", "allows", "us", "to", "invert", "the", "annotation", "process", "without", "loosing", "flexibility", "in", "the", "types", "of", "queries", "that", "we", "generate", ".", "Furthermore", ",", "it", "allows", "for", "fine-grained", "alignment", "of", "the", "tokens", "to", "the", "operations", ".", "Thus", ",", "we", "randomly", "generate", "OTs", "from", "a", "context", "free", "grammar", "and", "annotators", "just", "have", "to", "write", "the", "appropriate", "question", "and", "assign", "the", "tokens", ".", "We", "compare", "our", "corpus", "OTTA", "(", "Operation", "Trees", "and", "Token", "Assignment", ")", ",", "a", "large", "semantic", "parsing", "corpus", "for", "evaluating", "natural", "language", "interfaces", "to", "databases", ",", "to", "Spider", "and", "LC-QuaD", "2.0", "and", "show", "that", "our", "methodology", "more", "than", "triples", "the", "annotation", "speed", "while", "maintaining", "the", "complexity", "of", "the", "queries", ".", "Finally", ",", "we", "train", "a", "state-of-the-art", "semantic", "parsing", "model", "on", "our", "data", "and", "show", "that", "our", "dataset", "is", "a", "challenging", "dataset", "and", "that", "the", "token", "alignment", "can", "be", "leveraged", "to", "significantly", "increase", "the", "performance", "."], "entities": [{"type": "Operation", "start": 25, "end": 29, "text": "introduce an intermediate representation", "sent_idx": 1}, {"type": "Effect", "start": 148, "end": 150, "text": "annotation speed", "sent_idx": 5}, {"type": "Effect", "start": 153, "end": 157, "text": "complexity of the queries", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--84"}
{"text": "Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.", "tokens": ["Extracting", "relational", "triples", "from", "unstructured", "text", "is", "crucial", "for", "large-scale", "knowledge", "graph", "construction", ".", "However", ",", "few", "existing", "works", "excel", "in", "solving", "the", "overlapping", "triple", "problem", "where", "multiple", "relational", "triples", "in", "the", "same", "sentence", "share", "the", "same", "entities", ".", "In", "this", "work", ",", "we", "introduce", "a", "fresh", "perspective", "to", "revisit", "the", "relational", "triple", "extraction", "task", "and", "propose", "a", "novel", "cascade", "binary", "tagging", "framework", "(", "CasRel", ")", "derived", "from", "a", "principled", "problem", "formulation", ".", "Instead", "of", "treating", "relations", "as", "discrete", "labels", "as", "in", "previous", "works", ",", "our", "new", "framework", "models", "relations", "as", "functions", "that", "map", "subjects", "to", "objects", "in", "a", "sentence", ",", "which", "naturally", "handles", "the", "overlapping", "problem", ".", "Experiments", "show", "that", "the", "CasRel", "framework", "already", "outperforms", "state-of-the-art", "methods", "even", "when", "its", "encoder", "module", "uses", "a", "randomly", "initialized", "BERT", "encoder", ",", "showing", "the", "power", "of", "the", "new", "tagging", "framework", ".", "It", "enjoys", "further", "performance", "boost", "when", "employing", "a", "pre-trained", "BERT", "encoder", ",", "outperforming", "the", "strongest", "baseline", "by", "17.5", "and", "30.2", "absolute", "gain", "in", "F1-score", "on", "two", "public", "datasets", "NYT", "and", "WebNLG", ",", "respectively", ".", "In-depth", "analysis", "on", "different", "scenarios", "of", "overlapping", "triples", "shows", "that", "the", "method", "delivers", "consistent", "performance", "gain", "across", "all", "these", "scenarios", ".", "The", "source", "code", "and", "data", "are", "released", "online", "."], "entities": [{"type": "Operation", "start": 145, "end": 150, "text": "employing a pre-trained BERT encoder", "sent_idx": 5}, {"type": "Effect", "start": 162, "end": 163, "text": "F1-score", "sent_idx": 5}, {"type": "Operation", "start": 59, "end": 66, "text": "cascade binary tagging framework (CasRel)", "sent_idx": 2}, {"type": "Effect", "start": 187, "end": 188, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--136"}
{"text": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.", "tokens": ["This", "paper", "shows", "that", "pretraining", "multilingual", "language", "models", "at", "scale", "leads", "to", "significant", "performance", "gains", "for", "a", "wide", "range", "of", "cross-lingual", "transfer", "tasks", ".", "We", "train", "a", "Transformer-based", "masked", "language", "model", "on", "one", "hundred", "languages", ",", "using", "more", "than", "two", "terabytes", "of", "filtered", "CommonCrawl", "data", ".", "Our", "model", ",", "dubbed", "XLM-R", ",", "significantly", "outperforms", "multilingual", "BERT", "(", "mBERT", ")", "on", "a", "variety", "of", "cross-lingual", "benchmarks", ",", "including", "+", "14.6", "%", "average", "accuracy", "on", "XNLI", ",", "+", "13", "%", "average", "F1", "score", "on", "MLQA", ",", "and", "+", "2.4", "%", "F1", "score", "on", "NER", ".", "XLM-R", "performs", "particularly", "well", "on", "low-resource", "languages", ",", "improving", "15.7", "%", "in", "XNLI", "accuracy", "for", "Swahili", "and", "11.4", "%", "for", "Urdu", "over", "previous", "XLM", "models", ".", "We", "also", "present", "a", "detailed", "empirical", "analysis", "of", "the", "key", "factors", "that", "are", "required", "to", "achieve", "these", "gains", ",", "including", "the", "trade-offs", "between", "(", "1", ")", "positive", "transfer", "and", "capacity", "dilution", "and", "(", "2", ")", "the", "performance", "of", "high", "and", "low", "resource", "languages", "at", "scale", ".", "Finally", ",", "we", "show", ",", "for", "the", "first", "time", ",", "the", "possibility", "of", "multilingual", "modeling", "without", "sacrificing", "per-language", "performance", ";", "XLM-R", "is", "very", "competitive", "with", "strong", "monolingual", "models", "on", "the", "GLUE", "and", "XNLI", "benchmarks", ".", "We", "will", "make", "our", "code", "and", "models", "publicly", "available", "."], "entities": [{"type": "Operation", "start": 50, "end": 51, "text": "XLM-R", "sent_idx": 2}, {"type": "Effect", "start": 70, "end": 72, "text": "average accuracy", "sent_idx": 2}, {"type": "Effect", "start": 78, "end": 81, "text": "average F1 score", "sent_idx": 2}, {"type": "Effect", "start": 88, "end": 90, "text": "F1 score", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--747"}
{"text": "An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (\u201cWells Fargo\u201d) and dates (\u201cJuly 5, 2020\u201d) as well as certain productive constructions (\u201cblow for blow\u201d, \u201cday after day\u201d). Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information. We empirically compare these two common strategies\u2014parsing and tagging\u2014for predicting flat MWEs. Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers.", "tokens": ["An", "interesting", "and", "frequent", "type", "of", "multi-word", "expression", "(", "MWE", ")", "is", "the", "headless", "MWE", ",", "for", "which", "there", "are", "no", "true", "internal", "syntactic", "dominance", "relations", ";", "examples", "include", "many", "named", "entities", "(", "\u201c", "Wells", "Fargo", "\u201d", ")", "and", "dates", "(", "\u201c", "July", "5", ",", "2020", "\u201d", ")", "as", "well", "as", "certain", "productive", "constructions", "(", "\u201c", "blow", "for", "blow", "\u201d", ",", "\u201c", "day", "after", "day", "\u201d", ")", ".", "Despite", "their", "special", "status", "and", "prevalence", ",", "current", "dependency-annotation", "schemes", "require", "treating", "such", "flat", "structures", "as", "if", "they", "had", "internal", "syntactic", "heads", ",", "and", "most", "current", "parsers", "handle", "them", "in", "the", "same", "fashion", "as", "headed", "constructions", ".", "Meanwhile", ",", "outside", "the", "context", "of", "parsing", ",", "taggers", "are", "typically", "used", "for", "identifying", "MWEs", ",", "but", "taggers", "might", "benefit", "from", "structural", "information", ".", "We", "empirically", "compare", "these", "two", "common", "strategies", "\u2014", "parsing", "and", "tagging", "\u2014", "for", "predicting", "flat", "MWEs", ".", "Additionally", ",", "we", "propose", "an", "efficient", "joint", "decoding", "algorithm", "that", "combines", "scores", "from", "both", "strategies", ".", "Experimental", "results", "on", "the", "MWE-Aware", "English", "Dependency", "Corpus", "and", "on", "six", "non-English", "dependency", "treebanks", "with", "frequent", "flat", "structures", "show", "that", ":", "(", "1", ")", "tagging", "is", "more", "accurate", "than", "parsing", "for", "identifying", "flat-structure", "MWEs", ",", "(", "2", ")", "our", "joint", "decoder", "reconciles", "the", "two", "different", "views", "and", ",", "for", "non-BERT", "features", ",", "leads", "to", "higher", "accuracies", ",", "and", "(", "3", ")", "most", "of", "the", "gains", "result", "from", "feature", "sharing", "between", "the", "parsers", "and", "taggers", "."], "entities": [{"type": "Operation", "start": 201, "end": 208, "text": "joint decoder reconciles the two different views", "sent_idx": 5}, {"type": "Effect", "start": 217, "end": 218, "text": "accuracies", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--775"}
{"text": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.", "tokens": ["This", "paper", "presents", "an", "audio", "visual", "automatic", "speech", "recognition", "(", "AV-ASR", ")", "system", "using", "a", "Transformer-based", "architecture", ".", "We", "particularly", "focus", "on", "the", "scene", "context", "provided", "by", "the", "visual", "information", ",", "to", "ground", "the", "ASR", ".", "We", "extract", "representations", "for", "audio", "features", "in", "the", "encoder", "layers", "of", "the", "transformer", "and", "fuse", "video", "features", "using", "an", "additional", "crossmodal", "multihead", "attention", "layer", ".", "Additionally", ",", "we", "incorporate", "a", "multitask", "training", "criterion", "for", "multiresolution", "ASR", ",", "where", "we", "train", "the", "model", "to", "generate", "both", "character", "and", "subword", "level", "transcriptions", ".", "Experimental", "results", "on", "the", "How2", "dataset", ",", "indicate", "that", "multiresolution", "training", "can", "speed", "up", "convergence", "by", "around", "50", "%", "and", "relatively", "improves", "word", "error", "rate", "(", "WER", ")", "performance", "by", "upto", "18", "%", "over", "subword", "prediction", "models", ".", "Further", ",", "incorporating", "visual", "information", "improves", "performance", "with", "relative", "gains", "upto", "3.76", "%", "over", "audio", "only", "models", ".", "Our", "results", "are", "comparable", "to", "state-of-the-art", "Listen", ",", "Attend", "and", "Spell-based", "architectures", "."], "entities": [{"type": "Operation", "start": 96, "end": 98, "text": "multiresolution training", "sent_idx": 4}, {"type": "Effect", "start": 101, "end": 102, "text": "convergence", "sent_idx": 4}, {"type": "Effect", "start": 109, "end": 115, "text": "word error rate (WER)", "sent_idx": 4}, {"type": "Operation", "start": 127, "end": 130, "text": "incorporating visual information", "sent_idx": 5}, {"type": "Effect", "start": 131, "end": 132, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 4}], "id": "abstract-2020--acl-main--216"}
{"text": "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.", "tokens": ["Massively", "multilingual", "models", "for", "neural", "machine", "translation", "(", "NMT", ")", "are", "theoretically", "attractive", ",", "but", "often", "underperform", "bilingual", "models", "and", "deliver", "poor", "zero-shot", "translations", ".", "In", "this", "paper", ",", "we", "explore", "ways", "to", "improve", "them", ".", "We", "argue", "that", "multilingual", "NMT", "requires", "stronger", "modeling", "capacity", "to", "support", "language", "pairs", "with", "varying", "typological", "characteristics", ",", "and", "overcome", "this", "bottleneck", "via", "language-specific", "components", "and", "deepening", "NMT", "architectures", ".", "We", "identify", "the", "off-target", "translation", "issue", "(", "i.e.", "translating", "into", "a", "wrong", "target", "language", ")", "as", "the", "major", "source", "of", "the", "inferior", "zero-shot", "performance", ",", "and", "propose", "random", "online", "backtranslation", "to", "enforce", "the", "translation", "of", "unseen", "training", "language", "pairs", ".", "Experiments", "on", "OPUS-100", "(", "a", "novel", "multilingual", "dataset", "with", "100", "languages", ")", "show", "that", "our", "approach", "substantially", "narrows", "the", "performance", "gap", "with", "bilingual", "models", "in", "both", "one-to-many", "and", "many-to-many", "settings", ",", "and", "improves", "zero-shot", "performance", "by", "~10", "BLEU", ",", "approaching", "conventional", "pivot-based", "methods", "."], "entities": [{"type": "Operation", "start": 93, "end": 96, "text": "random online backtranslation", "sent_idx": 3}, {"type": "Effect", "start": 139, "end": 141, "text": "zero-shot performance", "sent_idx": 4}, {"type": "Effect", "start": 143, "end": 144, "text": "BLEU", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--148"}
{"text": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.", "tokens": ["Training", "objectives", "based", "on", "predictive", "coding", "have", "recently", "been", "shown", "to", "be", "very", "effective", "at", "learning", "meaningful", "representations", "from", "unlabeled", "speech", ".", "One", "example", "is", "Autoregressive", "Predictive", "Coding", "(", "Chung", "et", "al.", ",", "2019", ")", ",", "which", "trains", "an", "autoregressive", "RNN", "to", "generate", "an", "unseen", "future", "frame", "given", "a", "context", "such", "as", "recent", "past", "frames", ".", "The", "basic", "hypothesis", "of", "these", "approaches", "is", "that", "hidden", "states", "that", "can", "accurately", "predict", "future", "frames", "are", "a", "useful", "representation", "for", "many", "downstream", "tasks", ".", "In", "this", "paper", "we", "extend", "this", "hypothesis", "and", "aim", "to", "enrich", "the", "information", "encoded", "in", "the", "hidden", "states", "by", "training", "the", "model", "to", "make", "more", "accurate", "future", "predictions", ".", "We", "propose", "an", "auxiliary", "objective", "that", "serves", "as", "a", "regularization", "to", "improve", "generalization", "of", "the", "future", "frame", "prediction", "task", ".", "Experimental", "results", "on", "phonetic", "classification", ",", "speech", "recognition", ",", "and", "speech", "translation", "not", "only", "support", "the", "hypothesis", ",", "but", "also", "demonstrate", "the", "effectiveness", "of", "our", "approach", "in", "learning", "representations", "that", "contain", "richer", "phonetic", "content", "."], "entities": [{"type": "Operation", "start": 113, "end": 115, "text": "auxiliary objective", "sent_idx": 4}, {"type": "Effect", "start": 122, "end": 123, "text": "generalization", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--213"}
{"text": "Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.", "tokens": ["Recent", "pretraining", "models", "in", "Chinese", "neglect", "two", "important", "aspects", "specific", "to", "the", "Chinese", "language", ":", "glyph", "and", "pinyin", ",", "which", "carry", "significant", "syntax", "and", "semantic", "information", "for", "language", "understanding", ".", "In", "this", "work", ",", "we", "propose", "ChineseBERT", ",", "which", "incorporates", "both", "the", "glyph", "and", "pinyin", "information", "of", "Chinese", "characters", "into", "language", "model", "pretraining", ".", "The", "glyph", "embedding", "is", "obtained", "based", "on", "different", "fonts", "of", "a", "Chinese", "character", ",", "being", "able", "to", "capture", "character", "semantics", "from", "the", "visual", "features", ",", "and", "the", "pinyin", "embedding", "characterizes", "the", "pronunciation", "of", "Chinese", "characters", ",", "which", "handles", "the", "highly", "prevalent", "heteronym", "phenomenon", "in", "Chinese", "(", "the", "same", "character", "has", "different", "pronunciations", "with", "different", "meanings", ")", ".", "Pretrained", "on", "large-scale", "unlabeled", "Chinese", "corpus", ",", "the", "proposed", "ChineseBERT", "model", "yields", "significant", "performance", "boost", "over", "baseline", "models", "with", "fewer", "training", "steps", ".", "The", "proposed", "model", "achieves", "new", "SOTA", "performances", "on", "a", "wide", "range", "of", "Chinese", "NLP", "tasks", ",", "including", "machine", "reading", "comprehension", ",", "natural", "language", "inference", ",", "text", "classification", ",", "sentence", "pair", "matching", ",", "and", "competitive", "performances", "in", "named", "entity", "recognition", "and", "word", "segmentation", "."], "entities": [{"type": "Operation", "start": 120, "end": 121, "text": "ChineseBERT", "sent_idx": 3}, {"type": "Effect", "start": 124, "end": 125, "text": "performance", "sent_idx": 3}, {"type": "Effect", "start": 131, "end": 133, "text": "training steps", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--161"}
{"text": "Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.", "tokens": ["Large", "vocabulary", "speech", "recognition", "systems", "fail", "to", "recognize", "words", "beyond", "their", "vocabulary", ",", "many", "of", "which", "are", "information", "rich", "terms", ",", "like", "named", "entities", "or", "foreign", "words", ".", "Hybrid", "word/sub-word", "systems", "solve", "this", "problem", "by", "adding", "sub-word", "units", "to", "large", "vocabulary", "word", "based", "systems", ";", "new", "words", "can", "then", "be", "represented", "by", "combinations", "of", "sub-word", "units", ".", "Previous", "work", "heuristically", "created", "the", "sub-word", "lexicon", "from", "phonetic", "representations", "of", "text", "using", "simple", "statistics", "to", "select", "common", "phone", "sequences", ".", "We", "propose", "a", "probabilistic", "model", "to", "learn", "the", "subword", "lexicon", "optimized", "for", "a", "given", "task", ".", "We", "consider", "the", "task", "of", "out", "of", "vocabulary", "(", "OOV", ")", "word", "detection", ",", "which", "relies", "on", "output", "from", "a", "hybrid", "model", ".", "A", "hybrid", "model", "with", "our", "learned", "sub-word", "lexicon", "reduces", "error", "by", "6.3", "%", "and", "7.6", "%", "(", "absolute", ")", "at", "a", "5", "%", "false", "alarm", "rate", "on", "an", "English", "Broadcast", "News", "and", "MIT", "Lectures", "task", "respectively", "."], "entities": [{"type": "Operation", "start": 117, "end": 125, "text": "A hybrid model with our learned sub-word lexicon", "sent_idx": 5}, {"type": "Effect", "start": 126, "end": 127, "text": "error", "sent_idx": 5}, {"type": "Effect", "start": 140, "end": 143, "text": "false alarm rate", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "P11-1072"}
{"text": "Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster.", "tokens": ["Multi-intent", "SLU", "can", "handle", "multiple", "intents", "in", "an", "utterance", ",", "which", "has", "attracted", "increasing", "attention", ".", "However", ",", "the", "state-of-the-art", "joint", "models", "heavily", "rely", "on", "autoregressive", "approaches", ",", "resulting", "in", "two", "issues", ":", "slow", "inference", "speed", "and", "information", "leakage", ".", "In", "this", "paper", ",", "we", "explore", "a", "non-autoregressive", "model", "for", "joint", "multiple", "intent", "detection", "and", "slot", "filling", ",", "achieving", "more", "fast", "and", "accurate", ".", "Specifically", ",", "we", "propose", "a", "Global-Locally", "Graph", "Interaction", "Network", "(", "GL-GIN", ")", "where", "a", "local", "slot-aware", "graph", "interaction", "layer", "is", "proposed", "to", "model", "slot", "dependency", "for", "alleviating", "uncoordinated", "slots", "problem", "while", "a", "global", "intent-slot", "graph", "interaction", "layer", "is", "introduced", "to", "model", "the", "interaction", "between", "multiple", "intents", "and", "all", "slots", "in", "the", "utterance", ".", "Experimental", "results", "on", "two", "public", "datasets", "show", "that", "our", "framework", "achieves", "state-of-the-art", "performance", "while", "being", "11.5", "times", "faster", "."], "entities": [{"type": "Operation", "start": 69, "end": 76, "text": "Global-Locally Graph Interaction Network (GL-GIN)", "sent_idx": 3}, {"type": "Effect", "start": 129, "end": 130, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--15"}
{"text": "TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.", "tokens": ["TACRED", "is", "one", "of", "the", "largest", ",", "most", "widely", "used", "crowdsourced", "datasets", "in", "Relation", "Extraction", "(", "RE", ")", ".", "But", ",", "even", "with", "recent", "advances", "in", "unsupervised", "pre-training", "and", "knowledge", "enhanced", "neural", "RE", ",", "models", "still", "show", "a", "high", "error", "rate", ".", "In", "this", "paper", ",", "we", "investigate", "the", "questions", ":", "Have", "we", "reached", "a", "performance", "ceiling", "or", "is", "there", "still", "room", "for", "improvement", "?", "And", "how", "do", "crowd", "annotations", ",", "dataset", ",", "and", "models", "contribute", "to", "this", "error", "rate", "?", "To", "answer", "these", "questions", ",", "we", "first", "validate", "the", "most", "challenging", "5", "K", "examples", "in", "the", "development", "and", "test", "sets", "using", "trained", "annotators", ".", "We", "find", "that", "label", "errors", "account", "for", "8", "%", "absolute", "F1", "test", "error", ",", "and", "that", "more", "than", "50", "%", "of", "the", "examples", "need", "to", "be", "relabeled", ".", "On", "the", "relabeled", "test", "set", "the", "average", "F1", "score", "of", "a", "large", "baseline", "model", "set", "improves", "from", "62.1", "to", "70.1", ".", "After", "validation", ",", "we", "analyze", "misclassifications", "on", "the", "challenging", "instances", ",", "categorize", "them", "into", "linguistically", "motivated", "error", "groups", ",", "and", "verify", "the", "resulting", "error", "hypotheses", "on", "three", "state-of-the-art", "RE", "models", ".", "We", "show", "that", "two", "groups", "of", "ambiguous", "relations", "are", "responsible", "for", "most", "of", "the", "remaining", "errors", "and", "that", "models", "may", "adopt", "shallow", "heuristics", "on", "the", "dataset", "when", "entities", "are", "not", "masked", "."], "entities": [{"type": "Operation", "start": 135, "end": 138, "text": "relabeled test set", "sent_idx": 3}, {"type": "Effect", "start": 139, "end": 142, "text": "average F1 score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--142"}
{"text": "The notion of \u201cin-domain data\u201d in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision \u2013 suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.", "tokens": ["The", "notion", "of", "\u201c", "in-domain", "data", "\u201d", "in", "NLP", "is", "often", "over-simplistic", "and", "vague", ",", "as", "textual", "data", "varies", "in", "many", "nuanced", "linguistic", "aspects", "such", "as", "topic", ",", "style", "or", "level", "of", "formality", ".", "In", "addition", ",", "domain", "labels", "are", "many", "times", "unavailable", ",", "making", "it", "challenging", "to", "build", "domain-specific", "systems", ".", "We", "show", "that", "massive", "pre-trained", "language", "models", "implicitly", "learn", "sentence", "representations", "that", "cluster", "by", "domains", "without", "supervision", "\u2013", "suggesting", "a", "simple", "data-driven", "definition", "of", "domains", "in", "textual", "data", ".", "We", "harness", "this", "property", "and", "propose", "domain", "data", "selection", "methods", "based", "on", "such", "models", ",", "which", "require", "only", "a", "small", "set", "of", "in-domain", "monolingual", "data", ".", "We", "evaluate", "our", "data", "selection", "methods", "for", "neural", "machine", "translation", "across", "five", "diverse", "domains", ",", "where", "they", "outperform", "an", "established", "approach", "as", "measured", "by", "both", "BLEU", "and", "precision", "and", "recall", "with", "respect", "to", "an", "oracle", "selection", "."], "entities": [{"type": "Operation", "start": 110, "end": 113, "text": "data selection methods", "sent_idx": 4}, {"type": "Effect", "start": 132, "end": 133, "text": "BLEU", "sent_idx": 4}, {"type": "Effect", "start": 134, "end": 135, "text": "precision", "sent_idx": 4}, {"type": "Effect", "start": 136, "end": 137, "text": "recall", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--692"}
{"text": "Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.", "tokens": ["Discourse", "representation", "tree", "structure", "(", "DRTS", ")", "parsing", "is", "a", "novel", "semantic", "parsing", "task", "which", "has", "been", "concerned", "most", "recently", ".", "State-of-the-art", "performance", "can", "be", "achieved", "by", "a", "neural", "sequence-to-sequence", "model", ",", "treating", "the", "tree", "construction", "as", "an", "incremental", "sequence", "generation", "problem", ".", "Structural", "information", "such", "as", "input", "syntax", "and", "the", "intermediate", "skeleton", "of", "the", "partial", "output", "has", "been", "ignored", "in", "the", "model", ",", "which", "could", "be", "potentially", "useful", "for", "the", "DRTS", "parsing", ".", "In", "this", "work", ",", "we", "propose", "a", "structural-aware", "model", "at", "both", "the", "encoder", "and", "decoder", "phase", "to", "integrate", "the", "structural", "information", ",", "where", "graph", "attention", "network", "(", "GAT", ")", "is", "exploited", "for", "effectively", "modeling", ".", "Experimental", "results", "on", "a", "benchmark", "dataset", "show", "that", "our", "proposed", "model", "is", "effective", "and", "can", "obtain", "the", "best", "performance", "in", "the", "literature", "."], "entities": [{"type": "Operation", "start": 81, "end": 90, "text": "structural-aware model at both the encoder and decoder phase", "sent_idx": 3}, {"type": "Effect", "start": 127, "end": 128, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--609"}
{"text": "Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.", "tokens": ["Open-domain", "code", "generation", "aims", "to", "generate", "code", "in", "a", "general-purpose", "programming", "language", "(", "such", "as", "Python", ")", "from", "natural", "language", "(", "NL", ")", "intents", ".", "Motivated", "by", "the", "intuition", "that", "developers", "usually", "retrieve", "resources", "on", "the", "web", "when", "writing", "code", ",", "we", "explore", "the", "effectiveness", "of", "incorporating", "two", "varieties", "of", "external", "knowledge", "into", "NL-to-code", "generation", ":", "automatically", "mined", "NL-code", "pairs", "from", "the", "online", "programming", "QA", "forum", "StackOverflow", "and", "programming", "language", "API", "documentation", ".", "Our", "evaluations", "show", "that", "combining", "the", "two", "sources", "with", "data", "augmentation", "and", "retrieval-based", "data", "re-sampling", "improves", "the", "current", "state-of-the-art", "by", "up", "to", "2.2", "%", "absolute", "BLEU", "score", "on", "the", "code", "generation", "testbed", "CoNaLa", ".", "The", "code", "and", "resources", "are", "available", "at", "https://github.com/neulab/external-knowledge-codegen", "."], "entities": [{"type": "Operation", "start": 77, "end": 88, "text": "combining the two sources with data augmentation and retrieval-based data re-sampling", "sent_idx": 2}, {"type": "Effect", "start": 98, "end": 100, "text": "BLEU score", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--538"}
{"text": "Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs\u2014as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.", "tokens": ["Sequence", "labeling", "systems", "should", "perform", "reliably", "not", "only", "under", "ideal", "conditions", "but", "also", "with", "corrupted", "inputs", "\u2014", "as", "these", "systems", "often", "process", "user-generated", "text", "or", "follow", "an", "error-prone", "upstream", "component", ".", "To", "this", "end", ",", "we", "formulate", "the", "noisy", "sequence", "labeling", "problem", ",", "where", "the", "input", "may", "undergo", "an", "unknown", "noising", "process", "and", "propose", "two", "Noise-Aware", "Training", "(", "NAT", ")", "objectives", "that", "improve", "robustness", "of", "sequence", "labeling", "performed", "on", "perturbed", "input", ":", "Our", "data", "augmentation", "method", "trains", "a", "neural", "model", "using", "a", "mixture", "of", "clean", "and", "noisy", "samples", ",", "whereas", "our", "stability", "training", "algorithm", "encourages", "the", "model", "to", "create", "a", "noise-invariant", "latent", "representation", ".", "We", "employ", "a", "vanilla", "noise", "model", "at", "training", "time", ".", "For", "evaluation", ",", "we", "use", "both", "the", "original", "data", "and", "its", "variants", "perturbed", "with", "real", "OCR", "errors", "and", "misspellings", ".", "Extensive", "experiments", "on", "English", "and", "German", "named", "entity", "recognition", "benchmarks", "confirmed", "that", "NAT", "consistently", "improved", "robustness", "of", "popular", "sequence", "labeling", "models", ",", "preserving", "accuracy", "on", "the", "original", "input", ".", "We", "make", "our", "code", "and", "data", "publicly", "available", "for", "the", "research", "community", "."], "entities": [{"type": "Operation", "start": 54, "end": 61, "text": "two Noise-Aware Training (NAT) objectives", "sent_idx": 1}, {"type": "Effect", "start": 149, "end": 150, "text": "robustness", "sent_idx": 4}, {"type": "Effect", "start": 157, "end": 158, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--138"}
{"text": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.", "tokens": ["Text", "segmentation", "aims", "to", "uncover", "latent", "structure", "by", "dividing", "text", "from", "a", "document", "into", "coherent", "sections", ".", "Where", "previous", "work", "on", "text", "segmentation", "considers", "the", "tasks", "of", "document", "segmentation", "and", "segment", "labeling", "separately", ",", "we", "show", "that", "the", "tasks", "contain", "complementary", "information", "and", "are", "best", "addressed", "jointly", ".", "We", "introduce", "Segment", "Pooling", "LSTM", "(", "S-LSTM", ")", ",", "which", "is", "capable", "of", "jointly", "segmenting", "a", "document", "and", "labeling", "segments", ".", "In", "support", "of", "joint", "training", ",", "we", "develop", "a", "method", "for", "teaching", "the", "model", "to", "recover", "from", "errors", "by", "aligning", "the", "predicted", "and", "ground", "truth", "segments", ".", "We", "show", "that", "S-LSTM", "reduces", "segmentation", "error", "by", "30", "%", "on", "average", ",", "while", "also", "improving", "segment", "labeling", "."], "entities": [{"type": "Operation", "start": 99, "end": 100, "text": "S-LSTM", "sent_idx": 4}, {"type": "Effect", "start": 102, "end": 103, "text": "error", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--29"}
{"text": "Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and token labeling are limited to binary classification. We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.", "tokens": ["Exploiting", "sentence-level", "labels", ",", "which", "are", "easy", "to", "obtain", ",", "is", "one", "of", "the", "plausible", "methods", "to", "improve", "low-resource", "named", "entity", "recognition", "(", "NER", ")", ",", "where", "token-level", "labels", "are", "costly", "to", "annotate", ".", "Current", "models", "for", "jointly", "learning", "sentence", "and", "token", "labeling", "are", "limited", "to", "binary", "classification", ".", "We", "present", "a", "joint", "model", "that", "supports", "multi-class", "classification", "and", "introduce", "a", "simple", "variant", "of", "self-attention", "that", "allows", "the", "model", "to", "learn", "scaling", "factors", ".", "Our", "model", "produces", "3.78", "%", ",", "4.20", "%", ",", "2.08", "%", "improvements", "in", "F1", "over", "the", "BiLSTM-CRF", "baseline", "on", "e-commerce", "product", "titles", "in", "three", "different", "low-resource", "languages", ":", "Vietnamese", ",", "Thai", ",", "and", "Indonesian", ",", "respectively", "."], "entities": [{"type": "Operation", "start": 52, "end": 58, "text": "joint model that supports multi-class classification", "sent_idx": 2}, {"type": "Effect", "start": 87, "end": 88, "text": "F1", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--523"}
{"text": "In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model\u2019s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.", "tokens": ["In", "this", "paper", ",", "we", "present", "CorefQA", ",", "an", "accurate", "and", "extensible", "approach", "for", "the", "coreference", "resolution", "task", ".", "We", "formulate", "the", "problem", "as", "a", "span", "prediction", "task", ",", "like", "in", "question", "answering", ":", "A", "query", "is", "generated", "for", "each", "candidate", "mention", "using", "its", "surrounding", "context", ",", "and", "a", "span", "prediction", "module", "is", "employed", "to", "extract", "the", "text", "spans", "of", "the", "coreferences", "within", "the", "document", "using", "the", "generated", "query", ".", "This", "formulation", "comes", "with", "the", "following", "key", "advantages", ":", "(", "1", ")", "The", "span", "prediction", "strategy", "provides", "the", "flexibility", "of", "retrieving", "mentions", "left", "out", "at", "the", "mention", "proposal", "stage", ";", "(", "2", ")", "In", "the", "question", "answering", "framework", ",", "encoding", "the", "mention", "and", "its", "context", "explicitly", "in", "a", "query", "makes", "it", "possible", "to", "have", "a", "deep", "and", "thorough", "examination", "of", "cues", "embedded", "in", "the", "context", "of", "coreferent", "mentions", ";", "and", "(", "3", ")", "A", "plethora", "of", "existing", "question", "answering", "datasets", "can", "be", "used", "for", "data", "augmentation", "to", "improve", "the", "model", "\u2019s", "generalization", "capability", ".", "Experiments", "demonstrate", "significant", "performance", "boost", "over", "previous", "models", ",", "with", "83.1", "(", "+", "3.5", ")", "F1", "score", "on", "the", "CoNLL-2012", "benchmark", "and", "87.5", "(", "+", "2.5", ")", "F1", "score", "on", "the", "GAP", "benchmark", "."], "entities": [{"type": "Operation", "start": 6, "end": 7, "text": "CorefQA", "sent_idx": 0}, {"type": "Effect", "start": 167, "end": 168, "text": "performance", "sent_idx": 3}, {"type": "Effect", "start": 179, "end": 181, "text": "F1 score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--622"}
{"text": "Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model\u2019s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.", "tokens": ["Medical", "report", "generation", "task", ",", "which", "targets", "to", "produce", "long", "and", "coherent", "descriptions", "of", "medical", "images", ",", "has", "attracted", "growing", "research", "interests", "recently", ".", "Different", "from", "the", "general", "image", "captioning", "tasks", ",", "medical", "report", "generation", "is", "more", "challenging", "for", "data-driven", "neural", "models", ".", "This", "is", "mainly", "due", "to", "1", ")", "the", "serious", "data", "bias", "and", "2", ")", "the", "limited", "medical", "data", ".", "To", "alleviate", "the", "data", "bias", "and", "make", "best", "use", "of", "available", "data", ",", "we", "propose", "a", "Competence-based", "Multimodal", "Curriculum", "Learning", "framework", "(", "CMCL", ")", ".", "Specifically", ",", "CMCL", "simulates", "the", "learning", "process", "of", "radiologists", "and", "optimizes", "the", "model", "in", "a", "step", "by", "step", "manner", ".", "Firstly", ",", "CMCL", "estimates", "the", "difficulty", "of", "each", "training", "instance", "and", "evaluates", "the", "competence", "of", "current", "model", ";", "Secondly", ",", "CMCL", "selects", "the", "most", "suitable", "batch", "of", "training", "instances", "considering", "current", "model", "competence", ".", "By", "iterating", "above", "two", "steps", ",", "CMCL", "can", "gradually", "improve", "the", "model", "\u2019s", "performance", ".", "The", "experiments", "on", "the", "public", "IU-Xray", "and", "MIMIC-CXR", "datasets", "show", "that", "CMCL", "can", "be", "incorporated", "into", "existing", "models", "to", "improve", "their", "performance", "."], "entities": [{"type": "Operation", "start": 167, "end": 168, "text": "CMCL", "sent_idx": 7}, {"type": "Effect", "start": 177, "end": 178, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--234"}
{"text": "Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.", "tokens": ["Auto-regressive", "text", "generation", "models", "usually", "focus", "on", "local", "fluency", ",", "and", "may", "cause", "inconsistent", "semantic", "meaning", "in", "long", "text", "generation", ".", "Further", ",", "automatically", "generating", "words", "with", "similar", "semantics", "is", "challenging", ",", "and", "hand-crafted", "linguistic", "rules", "are", "difficult", "to", "apply", ".", "We", "consider", "a", "text", "planning", "scheme", "and", "present", "a", "model-based", "imitation-learning", "approach", "to", "alleviate", "the", "aforementioned", "issues", ".", "Specifically", ",", "we", "propose", "a", "novel", "guider", "network", "to", "focus", "on", "the", "generative", "process", "over", "a", "longer", "horizon", ",", "which", "can", "assist", "next-word", "prediction", "and", "provide", "intermediate", "rewards", "for", "generator", "optimization", ".", "Extensive", "experiments", "demonstrate", "that", "the", "proposed", "method", "leads", "to", "improved", "performance", "."], "entities": [{"type": "Operation", "start": 65, "end": 67, "text": "guider network", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 102, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--227"}
{"text": "Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines.", "tokens": ["Most", "studies", "on", "abstractive", "summarization", "report", "ROUGE", "scores", "between", "system", "and", "reference", "summaries", ".", "However", ",", "we", "have", "a", "concern", "about", "the", "truthfulness", "of", "generated", "summaries", ":", "whether", "all", "facts", "of", "a", "generated", "summary", "are", "mentioned", "in", "the", "source", "text", ".", "This", "paper", "explores", "improving", "the", "truthfulness", "in", "headline", "generation", "on", "two", "popular", "datasets", ".", "Analyzing", "headlines", "generated", "by", "the", "state-of-the-art", "encoder-decoder", "model", ",", "we", "show", "that", "the", "model", "sometimes", "generates", "untruthful", "headlines", ".", "We", "conjecture", "that", "one", "of", "the", "reasons", "lies", "in", "untruthful", "supervision", "data", "used", "for", "training", "the", "model", ".", "In", "order", "to", "quantify", "the", "truthfulness", "of", "article-headline", "pairs", ",", "we", "consider", "the", "textual", "entailment", "of", "whether", "an", "article", "entails", "its", "headline", ".", "After", "confirming", "quite", "a", "few", "untruthful", "instances", "in", "the", "datasets", ",", "this", "study", "hypothesizes", "that", "removing", "untruthful", "instances", "from", "the", "supervision", "data", "may", "remedy", "the", "problem", "of", "the", "untruthful", "behaviors", "of", "the", "model", ".", "Building", "a", "binary", "classifier", "that", "predicts", "an", "entailment", "relation", "between", "an", "article", "and", "its", "headline", ",", "we", "filter", "out", "untruthful", "instances", "from", "the", "supervision", "data", ".", "Experimental", "results", "demonstrate", "that", "the", "headline", "generation", "model", "trained", "on", "filtered", "supervision", "data", "shows", "no", "clear", "difference", "in", "ROUGE", "scores", "but", "remarkable", "improvements", "in", "automatic", "and", "manual", "evaluations", "of", "the", "generated", "headlines", "."], "entities": [{"type": "Operation", "start": 180, "end": 188, "text": "headline generation model trained on filtered supervision data", "sent_idx": 8}, {"type": "Effect", "start": 193, "end": 195, "text": "ROUGE scores", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--123"}
{"text": "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.", "tokens": ["Reranking", "models", "enable", "the", "integration", "of", "rich", "features", "to", "select", "a", "better", "output", "hypothesis", "within", "an", "n-best", "list", "or", "lattice", ".", "These", "models", "have", "a", "long", "history", "in", "NLP", ",", "and", "we", "revisit", "discriminative", "reranking", "for", "modern", "neural", "machine", "translation", "models", "by", "training", "a", "large", "transformer", "architecture", ".", "This", "takes", "as", "input", "both", "the", "source", "sentence", "as", "well", "as", "a", "list", "of", "hypotheses", "to", "output", "a", "ranked", "list", ".", "The", "reranker", "is", "trained", "to", "predict", "the", "observed", "distribution", "of", "a", "desired", "metric", ",", "e.g.", "BLEU", ",", "over", "the", "n-best", "list", ".", "Since", "such", "a", "discriminator", "contains", "hundreds", "of", "millions", "of", "parameters", ",", "we", "improve", "its", "generalization", "using", "pre-training", "and", "data", "augmentation", "techniques", ".", "Experiments", "on", "four", "WMT", "directions", "show", "that", "our", "discriminative", "reranking", "approach", "is", "effective", "and", "complementary", "to", "existing", "generative", "reranking", "approaches", ",", "yielding", "improvements", "of", "up", "to", "4", "BLEU", "over", "the", "beam", "search", "output", "."], "entities": [{"type": "Operation", "start": 121, "end": 124, "text": "discriminative reranking approach", "sent_idx": 5}, {"type": "Effect", "start": 140, "end": 141, "text": "BLEU", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--563"}
{"text": "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model\u2019s understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.", "tokens": ["When", "translating", "natural", "language", "questions", "into", "SQL", "queries", "to", "answer", "questions", "from", "a", "database", ",", "contemporary", "semantic", "parsing", "models", "struggle", "to", "generalize", "to", "unseen", "database", "schemas", ".", "The", "generalization", "challenge", "lies", "in", "(", "a", ")", "encoding", "the", "database", "relations", "in", "an", "accessible", "way", "for", "the", "semantic", "parser", ",", "and", "(", "b", ")", "modeling", "alignment", "between", "database", "columns", "and", "their", "mentions", "in", "a", "given", "query", ".", "We", "present", "a", "unified", "framework", ",", "based", "on", "the", "relation-aware", "self-attention", "mechanism", ",", "to", "address", "schema", "encoding", ",", "schema", "linking", ",", "and", "feature", "representation", "within", "a", "text-to-SQL", "encoder", ".", "On", "the", "challenging", "Spider", "dataset", "this", "framework", "boosts", "the", "exact", "match", "accuracy", "to", "57.2", "%", ",", "surpassing", "its", "best", "counterparts", "by", "8.7", "%", "absolute", "improvement", ".", "Further", "augmented", "with", "BERT", ",", "it", "achieves", "the", "new", "state-of-the-art", "performance", "of", "65.6", "%", "on", "the", "Spider", "leaderboard", ".", "In", "addition", ",", "we", "observe", "qualitative", "improvements", "in", "the", "model", "\u2019s", "understanding", "of", "schema", "linking", "and", "alignment", ".", "Our", "implementation", "will", "be", "open-sourced", "at", "https://github.com/Microsoft/rat-sql", "."], "entities": [{"type": "Operation", "start": 74, "end": 77, "text": "relation-aware self-attention mechanism", "sent_idx": 2}, {"type": "Effect", "start": 130, "end": 131, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 68, "end": 70, "text": "unified framework", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--677"}
{"text": "Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.", "tokens": ["Data-driven", "approaches", "using", "neural", "networks", "have", "achieved", "promising", "performances", "in", "natural", "language", "generation", "(", "NLG", ")", ".", "However", ",", "neural", "generators", "are", "prone", "to", "make", "mistakes", ",", "e.g.", ",", "neglecting", "an", "input", "slot", "value", "and", "generating", "a", "redundant", "slot", "value", ".", "Prior", "works", "refer", "this", "to", "hallucination", "phenomenon", ".", "In", "this", "paper", ",", "we", "study", "slot", "consistency", "for", "building", "reliable", "NLG", "systems", "with", "all", "slot", "values", "of", "input", "dialogue", "act", "(", "DA", ")", "properly", "generated", "in", "output", "sentences", ".", "We", "propose", "Iterative", "Rectification", "Network", "(", "IRN", ")", "for", "improving", "general", "NLG", "systems", "to", "produce", "both", "correct", "and", "fluent", "responses", ".", "It", "applies", "a", "bootstrapping", "algorithm", "to", "sample", "training", "candidates", "and", "uses", "reinforcement", "learning", "to", "incorporate", "discrete", "reward", "related", "to", "slot", "inconsistency", "into", "training", ".", "Comprehensive", "studies", "have", "been", "conducted", "on", "multiple", "benchmark", "datasets", ",", "showing", "that", "the", "proposed", "methods", "have", "significantly", "reduced", "the", "slot", "error", "rate", "(", "ERR", ")", "for", "all", "strong", "baselines", ".", "Human", "evaluations", "also", "have", "confirmed", "its", "effectiveness", "."], "entities": [{"type": "Operation", "start": 81, "end": 87, "text": "Iterative Rectification Network (IRN)", "sent_idx": 4}, {"type": "Effect", "start": 143, "end": 149, "text": "slot error rate (ERR)", "sent_idx": 6}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--10"}
{"text": "We propose a novel text editing task, referred to as  fact-based text editing , in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach.", "tokens": ["We", "propose", "a", "novel", "text", "editing", "task", ",", "referred", "to", "as", " ", "fact-based", "text", "editing", ",", "in", "which", "the", "goal", "is", "to", "revise", "a", "given", "document", "to", "better", "describe", "the", "facts", "in", "a", "knowledge", "base", "(", "e.g.", ",", "several", "triples", ")", ".", "The", "task", "is", "important", "in", "practice", "because", "reflecting", "the", "truth", "is", "a", "common", "requirement", "in", "text", "editing", ".", "First", ",", "we", "propose", "a", "method", "for", "automatically", "generating", "a", "dataset", "for", "research", "on", "fact-based", "text", "editing", ",", "where", "each", "instance", "consists", "of", "a", "draft", "text", ",", "a", "revised", "text", ",", "and", "several", "facts", "represented", "in", "triples", ".", "We", "apply", "the", "method", "into", "two", "public", "table-to-text", "datasets", ",", "obtaining", "two", "new", "datasets", "consisting", "of", "233k", "and", "37k", "instances", ",", "respectively", ".", "Next", ",", "we", "propose", "a", "new", "neural", "network", "architecture", "for", "fact-based", "text", "editing", ",", "called", "FactEditor", ",", "which", "edits", "a", "draft", "text", "by", "referring", "to", "given", "facts", "using", "a", "buffer", ",", "a", "stream", ",", "and", "a", "memory", ".", "A", "straightforward", "approach", "to", "address", "the", "problem", "would", "be", "to", "employ", "an", "encoder-decoder", "model", ".", "Our", "experimental", "results", "on", "the", "two", "datasets", "show", "that", "FactEditor", "outperforms", "the", "encoder-decoder", "approach", "in", "terms", "of", "fidelity", "and", "fluency", ".", "The", "results", "also", "show", "that", "FactEditor", "conducts", "inference", "faster", "than", "the", "encoder-decoder", "approach", "."], "entities": [{"type": "Operation", "start": 183, "end": 184, "text": "FactEditor", "sent_idx": 6}, {"type": "Effect", "start": 191, "end": 192, "text": "fidelity", "sent_idx": 6}, {"type": "Effect", "start": 193, "end": 194, "text": "fluency", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--17"}
{"text": "Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.", "tokens": ["Models", "for", "natural", "language", "understanding", "(", "NLU", ")", "tasks", "often", "rely", "on", "the", "idiosyncratic", "biases", "of", "the", "dataset", ",", "which", "make", "them", "brittle", "against", "test", "cases", "outside", "the", "training", "distribution", ".", "Recently", ",", "several", "proposed", "debiasing", "methods", "are", "shown", "to", "be", "very", "effective", "in", "improving", "out-of-distribution", "performance", ".", "However", ",", "their", "improvements", "come", "at", "the", "expense", "of", "performance", "drop", "when", "models", "are", "evaluated", "on", "the", "in-distribution", "data", ",", "which", "contain", "examples", "with", "higher", "diversity", ".", "This", "seemingly", "inevitable", "trade-off", "may", "not", "tell", "us", "much", "about", "the", "changes", "in", "the", "reasoning", "and", "understanding", "capabilities", "of", "the", "resulting", "models", "on", "broader", "types", "of", "examples", "beyond", "the", "small", "subset", "represented", "in", "the", "out-of-distribution", "data", ".", "In", "this", "paper", ",", "we", "address", "this", "trade-off", "by", "introducing", "a", "novel", "debiasing", "method", ",", "called", "confidence", "regularization", ",", "which", "discourage", "models", "from", "exploiting", "biases", "while", "enabling", "them", "to", "receive", "enough", "incentive", "to", "learn", "from", "all", "the", "training", "examples", ".", "We", "evaluate", "our", "method", "on", "three", "NLU", "tasks", "and", "show", "that", ",", "in", "contrast", "to", "its", "predecessors", ",", "it", "improves", "the", "performance", "on", "out-of-distribution", "datasets", "(", "e.g.", ",", "7pp", "gain", "on", "HANS", "dataset", ")", "while", "maintaining", "the", "original", "in-distribution", "accuracy", "."], "entities": [{"type": "Operation", "start": 121, "end": 126, "text": "introducing a novel debiasing method", "sent_idx": 4}, {"type": "Effect", "start": 173, "end": 174, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 128, "end": 130, "text": "confidence regularization", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--770"}
{"text": "Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.", "tokens": ["Generating", "sequential", "natural", "language", "descriptions", "from", "graph-structured", "data", "(", "e.g.", ",", "knowledge", "graph", ")", "is", "challenging", ",", "partly", "because", "of", "the", "structural", "differences", "between", "the", "input", "graph", "and", "the", "output", "text", ".", "Hence", ",", "popular", "sequence-to-sequence", "models", ",", "which", "require", "serialized", "input", ",", "are", "not", "a", "natural", "fit", "for", "this", "task", ".", "Graph", "neural", "networks", ",", "on", "the", "other", "hand", ",", "can", "better", "encode", "the", "input", "graph", "but", "broaden", "the", "structural", "gap", "between", "the", "encoder", "and", "decoder", ",", "making", "faithful", "generation", "difficult", ".", "To", "narrow", "this", "gap", ",", "we", "propose", "DualEnc", ",", "a", "dual", "encoding", "model", "that", "can", "not", "only", "incorporate", "the", "graph", "structure", ",", "but", "can", "also", "cater", "to", "the", "linear", "structure", "of", "the", "output", "text", ".", "Empirical", "comparisons", "with", "strong", "single-encoder", "baselines", "demonstrate", "that", "dual", "encoding", "can", "significantly", "improve", "the", "quality", "of", "the", "generated", "text", "."], "entities": [{"type": "Operation", "start": 126, "end": 128, "text": "dual encoding", "sent_idx": 4}, {"type": "Effect", "start": 132, "end": 137, "text": "quality of the generated text", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--224"}
{"text": "Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.", "tokens": ["Finding", "allowable", "places", "in", "words", "to", "insert", "hyphens", "is", "an", "important", "practical", "problem", ".", "The", "algorithm", "that", "is", "used", "most", "often", "nowadays", "has", "remained", "essentially", "unchanged", "for", "25", "years", ".", "This", "method", "is", "the", "TEX", "hyphenation", "algorithm", "of", "Knuth", "and", "Liang", ".", "We", "present", "here", "a", "hyphenation", "method", "that", "is", "clearly", "more", "accurate", ".", "The", "new", "method", "is", "an", "application", "of", "conditional", "random", "fields", ".", "We", "create", "new", "training", "sets", "for", "English", "and", "Dutch", "from", "the", "CELEX", "European", "lexical", "resource", ",", "and", "achieve", "error", "rates", "for", "English", "of", "less", "than", "0.1", "%", "for", "correctly", "allowed", "hyphens", ",", "and", "less", "than", "0.01", "%", "for", "Dutch", ".", "Experiments", "show", "that", "both", "the", "Knuth/Liang", "method", "and", "a", "leading", "current", "commercial", "alternative", "have", "error", "rates", "several", "times", "higher", "for", "both", "languages", "."], "entities": [{"type": "Operation", "start": 61, "end": 64, "text": "conditional random fields", "sent_idx": 4}, {"type": "Effect", "start": 83, "end": 85, "text": "error rates", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1038"}
{"text": "Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.", "tokens": ["Chinese", "word", "segmentation", "(", "CWS", ")", "and", "part-of-speech", "(", "POS", ")", "tagging", "are", "important", "fundamental", "tasks", "for", "Chinese", "language", "processing", ",", "where", "joint", "learning", "of", "them", "is", "an", "effective", "one-step", "solution", "for", "both", "tasks", ".", "Previous", "studies", "for", "joint", "CWS", "and", "POS", "tagging", "mainly", "follow", "the", "character-based", "tagging", "paradigm", "with", "introducing", "contextual", "information", "such", "as", "n-gram", "features", "or", "sentential", "representations", "from", "recurrent", "neural", "models", ".", "However", ",", "for", "many", "cases", ",", "the", "joint", "tagging", "needs", "not", "only", "modeling", "from", "context", "features", "but", "also", "knowledge", "attached", "to", "them", "(", "e.g.", ",", "syntactic", "relations", "among", "words", ")", ";", "limited", "efforts", "have", "been", "made", "by", "existing", "research", "to", "meet", "such", "needs", ".", "In", "this", "paper", ",", "we", "propose", "a", "neural", "model", "named", "TwASP", "for", "joint", "CWS", "and", "POS", "tagging", "following", "the", "character-based", "sequence", "labeling", "paradigm", ",", "where", "a", "two-way", "attention", "mechanism", "is", "used", "to", "incorporate", "both", "context", "feature", "and", "their", "corresponding", "syntactic", "knowledge", "for", "each", "input", "character", ".", "Particularly", ",", "we", "use", "existing", "language", "processing", "toolkits", "to", "obtain", "the", "auto-analyzed", "syntactic", "knowledge", "for", "the", "context", ",", "and", "the", "proposed", "attention", "module", "can", "learn", "and", "benefit", "from", "them", "although", "their", "quality", "may", "not", "be", "perfect", ".", "Our", "experiments", "illustrate", "the", "effectiveness", "of", "the", "two-way", "attentions", "for", "joint", "CWS", "and", "POS", "tagging", ",", "where", "state-of-the-art", "performance", "is", "achieved", "on", "five", "benchmark", "datasets", "."], "entities": [{"type": "Operation", "start": 199, "end": 207, "text": "two-way attentions for joint CWS and POS tagging", "sent_idx": 5}, {"type": "Effect", "start": 210, "end": 211, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--735"}
{"text": "Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.", "tokens": ["Neural", "text", "generation", "has", "made", "tremendous", "progress", "in", "various", "tasks", ".", "One", "common", "characteristic", "of", "most", "of", "the", "tasks", "is", "that", "the", "texts", "are", "not", "restricted", "to", "some", "rigid", "formats", "when", "generating", ".", "However", ",", "we", "may", "confront", "some", "special", "text", "paradigms", "such", "as", "Lyrics", "(", "assume", "the", "music", "score", "is", "given", ")", ",", "Sonnet", ",", "SongCi", "(", "classical", "Chinese", "poetry", "of", "the", "Song", "dynasty", ")", ",", "etc", ".", "The", "typical", "characteristics", "of", "these", "texts", "are", "in", "three", "folds", ":", "(", "1", ")", "They", "must", "comply", "fully", "with", "the", "rigid", "predefined", "formats", ".", "(", "2", ")", "They", "must", "obey", "some", "rhyming", "schemes", ".", "(", "3", ")", "Although", "they", "are", "restricted", "to", "some", "formats", ",", "the", "sentence", "integrity", "must", "be", "guaranteed", ".", "To", "the", "best", "of", "our", "knowledge", ",", "text", "generation", "based", "on", "the", "predefined", "rigid", "formats", "has", "not", "been", "well", "investigated", ".", "Therefore", ",", "we", "propose", "a", "simple", "and", "elegant", "framework", "named", "SongNet", "to", "tackle", "this", "problem", ".", "The", "backbone", "of", "the", "framework", "is", "a", "Transformer-based", "auto-regressive", "language", "model", ".", "Sets", "of", "symbols", "are", "tailor-designed", "to", "improve", "the", "modeling", "performance", "especially", "on", "format", ",", "rhyme", ",", "and", "sentence", "integrity", ".", "We", "improve", "the", "attention", "mechanism", "to", "impel", "the", "model", "to", "capture", "some", "future", "information", "on", "the", "format", ".", "A", "pre-training", "and", "fine-tuning", "framework", "is", "designed", "to", "further", "improve", "the", "generation", "quality", ".", "Extensive", "experiments", "conducted", "on", "two", "collected", "corpora", "demonstrate", "that", "our", "proposed", "framework", "generates", "significantly", "better", "results", "in", "terms", "of", "both", "automatic", "metrics", "and", "the", "human", "evaluation", "."], "entities": [{"type": "Operation", "start": 209, "end": 213, "text": "pre-training and fine-tuning framework", "sent_idx": 11}, {"type": "Effect", "start": 219, "end": 221, "text": "generation quality", "sent_idx": 11}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--68"}
{"text": "Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3-RUC/MMIN.", "tokens": ["Multimodal", "fusion", "has", "been", "proved", "to", "improve", "emotion", "recognition", "performance", "in", "previous", "works", ".", "However", ",", "in", "real-world", "applications", ",", "we", "often", "encounter", "the", "problem", "of", "missing", "modality", ",", "and", "which", "modalities", "will", "be", "missing", "is", "uncertain", ".", "It", "makes", "the", "fixed", "multimodal", "fusion", "fail", "in", "such", "cases", ".", "In", "this", "work", ",", "we", "propose", "a", "unified", "model", ",", "Missing", "Modality", "Imagination", "Network", "(", "MMIN", ")", ",", "to", "deal", "with", "the", "uncertain", "missing", "modality", "problem", ".", "MMIN", "learns", "robust", "joint", "multimodal", "representations", ",", "which", "can", "predict", "the", "representation", "of", "any", "missing", "modality", "given", "available", "modalities", "under", "different", "missing", "modality", "conditions", ".", "Comprehensive", "experiments", "on", "two", "benchmark", "datasets", "demonstrate", "that", "the", "unified", "MMIN", "model", "significantly", "improves", "emotion", "recognition", "performance", "under", "both", "uncertain", "missing-modality", "testing", "conditions", "and", "full-modality", "ideal", "testing", "condition", ".", "The", "code", "will", "be", "available", "at", "https://github.com/AIM3-RUC/MMIN", "."], "entities": [{"type": "Operation", "start": 110, "end": 113, "text": "unified MMIN model", "sent_idx": 5}, {"type": "Effect", "start": 117, "end": 118, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--203"}
{"text": "Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.", "tokens": ["Language", "modeling", "is", "the", "technique", "to", "estimate", "the", "probability", "of", "a", "sequence", "of", "words", ".", "A", "bilingual", "language", "model", "is", "expected", "to", "model", "the", "sequential", "dependency", "for", "words", "across", "languages", ",", "which", "is", "difficult", "due", "to", "the", "inherent", "lack", "of", "suitable", "training", "data", "as", "well", "as", "diverse", "syntactic", "structure", "across", "languages", ".", "We", "propose", "a", "bilingual", "attention", "language", "model", "(", "BALM", ")", "that", "simultaneously", "performs", "language", "modeling", "objective", "with", "a", "quasi-translation", "objective", "to", "model", "both", "the", "monolingual", "as", "well", "as", "the", "cross-lingual", "sequential", "dependency", ".", "The", "attention", "mechanism", "learns", "the", "bilingual", "context", "from", "a", "parallel", "corpus", ".", "BALM", "achieves", "state-of-the-art", "performance", "on", "the", "SEAME", "code-switch", "database", "by", "reducing", "the", "perplexity", "of", "20.5", "%", "over", "the", "best-reported", "result", ".", "We", "also", "apply", "BALM", "in", "bilingual", "lexicon", "induction", ",", "and", "language", "normalization", "tasks", "to", "validate", "the", "idea", "."], "entities": [{"type": "Operation", "start": 97, "end": 98, "text": "BALM", "sent_idx": 4}, {"type": "Effect", "start": 109, "end": 110, "text": "perplexity", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--80"}
{"text": "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.", "tokens": ["Most", "general-purpose", "extractive", "summarization", "models", "are", "trained", "on", "news", "articles", ",", "which", "are", "short", "and", "present", "all", "important", "information", "upfront", ".", "As", "a", "result", ",", "such", "models", "are", "biased", "on", "position", "and", "often", "perform", "a", "smart", "selection", "of", "sentences", "from", "the", "beginning", "of", "the", "document", ".", "When", "summarizing", "long", "narratives", ",", "which", "have", "complex", "structure", "and", "present", "information", "piecemeal", ",", "simple", "position", "heuristics", "are", "not", "sufficient", ".", "In", "this", "paper", ",", "we", "propose", "to", "explicitly", "incorporate", "the", "underlying", "structure", "of", "narratives", "into", "general", "unsupervised", "and", "supervised", "extractive", "summarization", "models", ".", "We", "formalize", "narrative", "structure", "in", "terms", "of", "key", "narrative", "events", "(", "turning", "points", ")", "and", "treat", "it", "as", "latent", "in", "order", "to", "summarize", "screenplays", "(", "i.e.", ",", "extract", "an", "optimal", "sequence", "of", "scenes", ")", ".", "Experimental", "results", "on", "the", "CSI", "corpus", "of", "TV", "screenplays", ",", "which", "we", "augment", "with", "scene-level", "summarization", "labels", ",", "show", "that", "latent", "turning", "points", "correlate", "with", "important", "aspects", "of", "a", "CSI", "episode", "and", "improve", "summarization", "performance", "over", "general", "extractive", "algorithms", "leading", "to", "more", "complete", "and", "diverse", "summaries", "."], "entities": [{"type": "Operation", "start": 75, "end": 89, "text": "incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models", "sent_idx": 3}, {"type": "Effect", "start": 159, "end": 160, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--174"}
{"text": "We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.", "tokens": ["We", "present", "a", "new", "problem", ":", "grounding", "natural", "language", "instructions", "to", "mobile", "user", "interface", "actions", ",", "and", "create", "three", "new", "datasets", "for", "it", ".", "For", "full", "task", "evaluation", ",", "we", "create", "PixelHelp", ",", "a", "corpus", "that", "pairs", "English", "instructions", "with", "actions", "performed", "by", "people", "on", "a", "mobile", "UI", "emulator", ".", "To", "scale", "training", ",", "we", "decouple", "the", "language", "and", "action", "data", "by", "(", "a", ")", "annotating", "action", "phrase", "spans", "in", "How-To", "instructions", "and", "(", "b", ")", "synthesizing", "grounded", "descriptions", "of", "actions", "for", "mobile", "user", "interfaces", ".", "We", "use", "a", "Transformer", "to", "extract", "action", "phrase", "tuples", "from", "long-range", "natural", "language", "instructions", ".", "A", "grounding", "Transformer", "then", "contextually", "represents", "UI", "objects", "using", "both", "their", "content", "and", "screen", "position", "and", "connects", "them", "to", "object", "descriptions", ".", "Given", "a", "starting", "screen", "and", "instruction", ",", "our", "model", "achieves", "70.59", "%", "accuracy", "on", "predicting", "complete", "ground-truth", "action", "sequences", "in", "PixelHelp", "."], "entities": [{"type": "Operation", "start": 89, "end": 90, "text": "Transformer", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 136, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--729"}
{"text": "Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.", "tokens": ["Distant", "supervision", "tackles", "the", "data", "bottleneck", "in", "NER", "by", "automatically", "generating", "training", "instances", "via", "dictionary", "matching", ".", "Unfortunately", ",", "the", "learning", "of", "DS-NER", "is", "severely", "dictionary-biased", ",", "which", "suffers", "from", "spurious", "correlations", "and", "therefore", "undermines", "the", "effectiveness", "and", "the", "robustness", "of", "the", "learned", "models", ".", "In", "this", "paper", ",", "we", "fundamentally", "explain", "the", "dictionary", "bias", "via", "a", "Structural", "Causal", "Model", "(", "SCM", ")", ",", "categorize", "the", "bias", "into", "intra-dictionary", "and", "inter-dictionary", "biases", ",", "and", "identify", "their", "causes", ".", "Based", "on", "the", "SCM", ",", "we", "learn", "de-biased", "DS-NER", "via", "causal", "interventions", ".", "For", "intra-dictionary", "bias", ",", "we", "conduct", "backdoor", "adjustment", "to", "remove", "the", "spurious", "correlations", "introduced", "by", "the", "dictionary", "confounder", ".", "For", "inter-dictionary", "bias", ",", "we", "propose", "a", "causal", "invariance", "regularizer", "which", "will", "make", "DS-NER", "models", "more", "robust", "to", "the", "perturbation", "of", "dictionaries", ".", "Experiments", "on", "four", "datasets", "and", "three", "DS-NER", "models", "show", "that", "our", "method", "can", "significantly", "improve", "the", "performance", "of", "DS-NER", "."], "entities": [{"type": "Operation", "start": 57, "end": 63, "text": "Structural Causal Model (SCM)", "sent_idx": 2}, {"type": "Effect", "start": 149, "end": 150, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--371"}
{"text": "In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.", "tokens": ["In", "the", "present", "paper", ",", "we", "propose", "the", "effective", "usage", "of", "function", "words", "to", "generate", "generalized", "translation", "rules", "for", "forest-based", "translation", ".", "Given", "aligned", "forest-string", "pairs", ",", "we", "extract", "composed", "tree-to-string", "translation", "rules", "that", "account", "for", "multiple", "interpretations", "of", "both", "aligned", "and", "unaligned", "target", "function", "words", ".", "In", "order", "to", "constrain", "the", "exhaustive", "attachments", "of", "function", "words", ",", "we", "limit", "to", "bind", "them", "to", "the", "nearby", "syntactic", "chunks", "yielded", "by", "a", "target", "dependency", "parser", ".", "Therefore", ",", "the", "proposed", "approach", "can", "not", "only", "capture", "source-tree-to-target-chunk", "correspondences", "but", "can", "also", "use", "forest", "structures", "that", "compactly", "encode", "an", "exponential", "number", "of", "parse", "trees", "to", "properly", "generate", "target", "function", "words", "during", "decoding", ".", "Extensive", "experiments", "involving", "large-scale", "English-to-Japanese", "translation", "revealed", "a", "significant", "improvement", "of", "1.8", "points", "in", "BLEU", "score", ",", "as", "compared", "with", "a", "strong", "forest-to-string", "baseline", "system", "."], "entities": [{"type": "Operation", "start": 8, "end": 18, "text": "effective usage of function words to generate generalized translation rules", "sent_idx": 0}, {"type": "Effect", "start": 124, "end": 126, "text": "BLEU score", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1003"}
{"text": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). \n \nThis paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.", "tokens": ["Information", "extraction", "(", "IE", ")", "holds", "the", "promise", "of", "generating", "a", "large-scale", "knowledge", "base", "from", "the", "Web", "'s", "natural", "language", "text", ".", "Knowledge-based", "weak", "supervision", ",", "using", "structured", "data", "to", "heuristically", "label", "a", "training", "corpus", ",", "works", "towards", "this", "goal", "by", "enabling", "the", "automated", "learning", "of", "a", "potentially", "unbounded", "number", "of", "relation", "extractors", ".", "Recently", ",", "researchers", "have", "developed", "multi-instance", "learning", "algorithms", "to", "combat", "the", "noisy", "training", "data", "that", "can", "come", "from", "heuristic", "labeling", ",", "but", "their", "models", "assume", "relations", "are", "disjoint", "---", "for", "example", "they", "can", "not", "extract", "the", "pair", "Founded(Jobs", ",", "Apple", ")", "and", "CEO-of(Jobs", ",", "Apple", ")", ".", "\n \n", "This", "paper", "presents", "a", "novel", "approach", "for", "multi-instance", "learning", "with", "overlapping", "relations", "that", "combines", "a", "sentence-level", "extraction", "model", "with", "a", "simple", ",", "corpus-level", "component", "for", "aggregating", "the", "individual", "facts", ".", "We", "apply", "our", "model", "to", "learn", "extractors", "for", "NY", "Times", "text", "using", "weak", "supervision", "from", "Free-base", ".", "Experiments", "show", "that", "the", "approach", "runs", "quickly", "and", "yields", "surprising", "gains", "in", "accuracy", ",", "at", "both", "the", "aggregate", "and", "sentence", "level", "."], "entities": [{"type": "Operation", "start": 105, "end": 114, "text": "a novel approach for multi-instance learning with overlapping relations", "sent_idx": 3}, {"type": "Effect", "start": 161, "end": 162, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1055"}
{"text": "Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation. We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages. With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning. Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art. In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively. We make our code available on github https://github.com/xdqkid/XLPT-AMR.", "tokens": ["Due", "to", "the", "scarcity", "of", "annotated", "data", ",", "Abstract", "Meaning", "Representation", "(", "AMR", ")", "research", "is", "relatively", "limited", "and", "challenging", "for", "languages", "other", "than", "English", ".", "Upon", "the", "availability", "of", "English", "AMR", "dataset", "and", "English-to-", "X", "parallel", "datasets", ",", "in", "this", "paper", "we", "propose", "a", "novel", "cross-lingual", "pre-training", "approach", "via", "multi-task", "learning", "(", "MTL", ")", "for", "both", "zeroshot", "AMR", "parsing", "and", "AMR-to-text", "generation", ".", "Specifically", ",", "we", "consider", "three", "types", "of", "relevant", "tasks", ",", "including", "AMR", "parsing", ",", "AMR-to-text", "generation", ",", "and", "machine", "translation", ".", "We", "hope", "that", "knowledge", "gained", "while", "learning", "for", "English", "AMR", "parsing", "and", "text", "generation", "can", "be", "transferred", "to", "the", "counterparts", "of", "other", "languages", ".", "With", "properly", "pretrained", "models", ",", "we", "explore", "four", "different", "finetuning", "methods", ",", "i.e.", ",", "vanilla", "fine-tuning", "with", "a", "single", "task", ",", "one-for-all", "MTL", "fine-tuning", ",", "targeted", "MTL", "fine-tuning", ",", "and", "teacher-studentbased", "MTL", "fine-tuning", ".", "Experimental", "results", "on", "AMR", "parsing", "and", "text", "generation", "of", "multiple", "non-English", "languages", "demonstrate", "that", "our", "approach", "significantly", "outperforms", "a", "strong", "baseline", "of", "pre-training", "approach", ",", "and", "greatly", "advances", "the", "state", "of", "the", "art", ".", "In", "detail", ",", "on", "LDC2020T07", "we", "have", "achieved", "70.45", "%", ",", "71.76", "%", ",", "and", "70.80", "%", "in", "Smatch", "F1", "for", "AMR", "parsing", "of", "German", ",", "Spanish", ",", "and", "Italian", ",", "respectively", ",", "while", "for", "AMR-to-text", "generation", "of", "the", "languages", ",", "we", "have", "obtained", "25.69", ",", "31.36", ",", "and", "28.42", "in", "BLEU", "respectively", ".", "We", "make", "our", "code", "available", "on", "github", "https://github.com/xdqkid/XLPT-AMR", "."], "entities": [{"type": "Operation", "start": 46, "end": 49, "text": "cross-lingual pre-training approach", "sent_idx": 1}, {"type": "Effect", "start": 195, "end": 197, "text": "Smatch F1", "sent_idx": 6}, {"type": "Effect", "start": 228, "end": 229, "text": "BLEU", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--73"}
{"text": "Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework \u2013 NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.", "tokens": ["Weak", "supervision", "has", "shown", "promising", "results", "in", "many", "natural", "language", "processing", "tasks", ",", "such", "as", "Named", "Entity", "Recognition", "(", "NER", ")", ".", "Existing", "work", "mainly", "focuses", "on", "learning", "deep", "NER", "models", "only", "with", "weak", "supervision", ",", "i.e.", ",", "without", "any", "human", "annotation", ",", "and", "shows", "that", "by", "merely", "using", "weakly", "labeled", "data", ",", "one", "can", "achieve", "good", "performance", ",", "though", "still", "underperforms", "fully", "supervised", "NER", "with", "manually/strongly", "labeled", "data", ".", "In", "this", "paper", ",", "we", "consider", "a", "more", "practical", "scenario", ",", "where", "we", "have", "both", "a", "small", "amount", "of", "strongly", "labeled", "data", "and", "a", "large", "amount", "of", "weakly", "labeled", "data", ".", "Unfortunately", ",", "we", "observe", "that", "weakly", "labeled", "data", "does", "not", "necessarily", "improve", ",", "or", "even", "deteriorate", "the", "model", "performance", "(", "due", "to", "the", "extensive", "noise", "in", "the", "weak", "labels", ")", "when", "we", "train", "deep", "NER", "models", "over", "a", "simple", "or", "weighted", "combination", "of", "the", "strongly", "labeled", "and", "weakly", "labeled", "data", ".", "To", "address", "this", "issue", ",", "we", "propose", "a", "new", "multi-stage", "computational", "framework", "\u2013", "NEEDLE", "with", "three", "essential", "ingredients", ":", "(", "1", ")", "weak", "label", "completion", ",", "(", "2", ")", "noise-aware", "loss", "function", ",", "and", "(", "3", ")", "final", "fine-tuning", "over", "the", "strongly", "labeled", "data", ".", "Through", "experiments", "on", "E-commerce", "query", "NER", "and", "Biomedical", "NER", ",", "we", "demonstrate", "that", "NEEDLE", "can", "effectively", "suppress", "the", "noise", "of", "the", "weak", "labels", "and", "outperforms", "existing", "methods", ".", "In", "particular", ",", "we", "achieve", "new", "SOTA", "F1-scores", "on", "3", "Biomedical", "NER", "datasets", ":", "BC5CDR-chem", "93.74", ",", "BC5CDR-disease", "90.69", ",", "NCBI-disease", "92.28", "."], "entities": [{"type": "Operation", "start": 161, "end": 164, "text": "multi-stage computational framework", "sent_idx": 4}, {"type": "Effect", "start": 232, "end": 233, "text": "F1-scores", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--140"}
{"text": "Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.", "tokens": ["Fact", "checking", "is", "a", "challenging", "task", "because", "verifying", "the", "truthfulness", "of", "a", "claim", "requires", "reasoning", "about", "multiple", "retrievable", "evidence", ".", "In", "this", "work", ",", "we", "present", "a", "method", "suitable", "for", "reasoning", "about", "the", "semantic-level", "structure", "of", "evidence", ".", "Unlike", "most", "previous", "works", ",", "which", "typically", "represent", "evidence", "sentences", "with", "either", "string", "concatenation", "or", "fusing", "the", "features", "of", "isolated", "evidence", "sentences", ",", "our", "approach", "operates", "on", "rich", "semantic", "structures", "of", "evidence", "obtained", "by", "semantic", "role", "labeling", ".", "We", "propose", "two", "mechanisms", "to", "exploit", "the", "structure", "of", "evidence", "while", "leveraging", "the", "advances", "of", "pre-trained", "models", "like", "BERT", ",", "GPT", "or", "XLNet", ".", "Specifically", ",", "using", "XLNet", "as", "the", "backbone", ",", "we", "first", "utilize", "the", "graph", "structure", "to", "re-define", "the", "relative", "distances", "of", "words", ",", "with", "the", "intuition", "that", "semantically", "related", "words", "should", "have", "short", "distances", ".", "Then", ",", "we", "adopt", "graph", "convolutional", "network", "and", "graph", "attention", "network", "to", "propagate", "and", "aggregate", "information", "from", "neighboring", "nodes", "on", "the", "graph", ".", "We", "evaluate", "our", "system", "on", "FEVER", ",", "a", "benchmark", "dataset", "for", "fact", "checking", ",", "and", "find", "that", "rich", "structural", "information", "is", "helpful", "and", "both", "our", "graph-based", "mechanisms", "improve", "the", "accuracy", ".", "Our", "model", "is", "the", "state-of-the-art", "system", "in", "terms", "of", "both", "official", "evaluation", "metrics", ",", "namely", "claim", "verification", "accuracy", "and", "FEVER", "score", "."], "entities": [{"type": "Operation", "start": 182, "end": 184, "text": "graph-based mechanisms", "sent_idx": 6}, {"type": "Effect", "start": 186, "end": 187, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--549"}
{"text": "Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.", "tokens": ["Most", "previous", "work", "on", "trainable", "language", "generation", "has", "focused", "on", "two", "paradigms", ":", "(", "a", ")", "using", "a", "statistical", "model", "to", "rank", "a", "set", "of", "generated", "utterances", ",", "or", "(", "b", ")", "using", "statistics", "to", "inform", "the", "generation", "decision", "process", ".", "Both", "approaches", "rely", "on", "the", "existence", "of", "a", "handcrafted", "generator", ",", "which", "limits", "their", "scalability", "to", "new", "domains", ".", "This", "paper", "presents", "Bagel", ",", "a", "statistical", "language", "generator", "which", "uses", "dynamic", "Bayesian", "networks", "to", "learn", "from", "semantically-aligned", "data", "produced", "by", "42", "untrained", "annotators", ".", "A", "human", "evaluation", "shows", "that", "Bagel", "can", "generate", "natural", "and", "informative", "utterances", "from", "unseen", "inputs", "in", "the", "information", "presentation", "domain", ".", "Additionally", ",", "generation", "performance", "on", "sparse", "datasets", "is", "improved", "significantly", "by", "using", "certainty-based", "active", "learning", ",", "yielding", "ratings", "close", "to", "the", "human", "gold", "standard", "with", "a", "fraction", "of", "the", "data", "."], "entities": [{"type": "Operation", "start": 117, "end": 121, "text": "using certainty-based active learning", "sent_idx": 4}, {"type": "Effect", "start": 109, "end": 110, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1157"}
{"text": "Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method.", "tokens": ["Recently", ",", "word", "enhancement", "has", "become", "very", "popular", "for", "Chinese", "Named", "Entity", "Recognition", "(", "NER", ")", ",", "reducing", "segmentation", "errors", "and", "increasing", "the", "semantic", "and", "boundary", "information", "of", "Chinese", "words", ".", "However", ",", "these", "methods", "tend", "to", "ignore", "the", "information", "of", "the", "Chinese", "character", "structure", "after", "integrating", "the", "lexical", "information", ".", "Chinese", "characters", "have", "evolved", "from", "pictographs", "since", "ancient", "times", ",", "and", "their", "structure", "often", "reflects", "more", "information", "about", "the", "characters", ".", "This", "paper", "presents", "a", "novel", "Multi-metadata", "Embedding", "based", "Cross-Transformer", "(", "MECT", ")", "to", "improve", "the", "performance", "of", "Chinese", "NER", "by", "fusing", "the", "structural", "information", "of", "Chinese", "characters", ".", "Specifically", ",", "we", "use", "multi-metadata", "embedding", "in", "a", "two-stream", "Transformer", "to", "integrate", "Chinese", "character", "features", "with", "the", "radical-level", "embedding", ".", "With", "the", "structural", "characteristics", "of", "Chinese", "characters", ",", "MECT", "can", "better", "capture", "the", "semantic", "information", "of", "Chinese", "characters", "for", "NER", ".", "The", "experimental", "results", "obtained", "on", "several", "well-known", "benchmarking", "datasets", "demonstrate", "the", "merits", "and", "superiority", "of", "the", "proposed", "MECT", "method", "."], "entities": [{"type": "Operation", "start": 77, "end": 84, "text": "Multi-metadata Embedding based Cross-Transformer (MECT)", "sent_idx": 3}, {"type": "Effect", "start": 87, "end": 88, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--121"}
{"text": "Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.", "tokens": ["Current", "Semantic", "Role", "Labeling", "technologies", "are", "based", "on", "inductive", "algorithms", "trained", "over", "large", "scale", "repositories", "of", "annotated", "examples", ".", "Frame-based", "systems", "currently", "make", "use", "of", "the", "FrameNet", "database", "but", "fail", "to", "show", "suitable", "generalization", "capabilities", "in", "out-of-domain", "scenarios", ".", "In", "this", "paper", ",", "a", "state-of-art", "system", "for", "frame-based", "SRL", "is", "extended", "through", "the", "encapsulation", "of", "a", "distributional", "model", "of", "semantic", "similarity", ".", "The", "resulting", "argument", "classification", "model", "promotes", "a", "simpler", "feature", "space", "that", "limits", "the", "potential", "overfitting", "effects", ".", "The", "large", "scale", "empirical", "study", "here", "discussed", "confirms", "that", "state-of-art", "accuracy", "can", "be", "obtained", "for", "out-of-domain", "evaluations", "."], "entities": [{"type": "Operation", "start": 53, "end": 61, "text": "encapsulation of a distributional model of semantic similarity", "sent_idx": 2}, {"type": "Effect", "start": 89, "end": 90, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1025"}
{"text": "Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.", "tokens": ["Question", "answering", "and", "conversational", "systems", "are", "often", "baffled", "and", "need", "help", "clarifying", "certain", "ambiguities", ".", "However", ",", "limitations", "of", "existing", "datasets", "hinder", "the", "development", "of", "large-scale", "models", "capable", "of", "generating", "and", "utilising", "clarification", "questions", ".", "In", "order", "to", "overcome", "these", "limitations", ",", "we", "devise", "a", "novel", "bootstrapping", "framework", "(", "based", "on", "self-supervision", ")", "that", "assists", "in", "the", "creation", "of", "a", "diverse", ",", "large-scale", "dataset", "of", "clarification", "questions", "based", "on", "post-comment", "tuples", "extracted", "from", "stackexchange", ".", "The", "framework", "utilises", "a", "neural", "network", "based", "architecture", "for", "classifying", "clarification", "questions", ".", "It", "is", "a", "two-step", "method", "where", "the", "first", "aims", "to", "increase", "the", "precision", "of", "the", "classifier", "and", "second", "aims", "to", "increase", "its", "recall", ".", "We", "quantitatively", "demonstrate", "the", "utility", "of", "the", "newly", "created", "dataset", "by", "applying", "it", "to", "the", "downstream", "task", "of", "question-answering", ".", "The", "final", "dataset", ",", "ClarQ", ",", "consists", "of", "~2", "M", "examples", "distributed", "across", "173", "domains", "of", "stackexchange", ".", "We", "release", "this", "dataset", "in", "order", "to", "foster", "research", "into", "the", "field", "of", "clarification", "question", "generation", "with", "the", "larger", "goal", "of", "enhancing", "dialog", "and", "question", "answering", "systems", "."], "entities": [{"type": "Operation", "start": 46, "end": 53, "text": "bootstrapping framework (based on self-supervision)", "sent_idx": 2}, {"type": "Effect", "start": 100, "end": 101, "text": "precision", "sent_idx": 4}, {"type": "Effect", "start": 110, "end": 111, "text": "recall", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--651"}
{"text": "Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.", "tokens": ["Most", "of", "recent", "work", "in", "cross-lingual", "word", "embeddings", "is", "severely", "Anglocentric", ".", "The", "vast", "majority", "of", "lexicon", "induction", "evaluation", "dictionaries", "are", "between", "English", "and", "another", "language", ",", "and", "the", "English", "embedding", "space", "is", "selected", "by", "default", "as", "the", "hub", "when", "learning", "in", "a", "multilingual", "setting", ".", "With", "this", "work", ",", "however", ",", "we", "challenge", "these", "practices", ".", "First", ",", "we", "show", "that", "the", "choice", "of", "hub", "language", "can", "significantly", "impact", "downstream", "lexicon", "induction", "zero-shot", "POS", "tagging", "performance", ".", "Second", ",", "we", "both", "expand", "a", "standard", "English-centered", "evaluation", "dictionary", "collection", "to", "include", "all", "language", "pairs", "using", "triangulation", ",", "and", "create", "new", "dictionaries", "for", "under-represented", "languages", ".", "Evaluating", "established", "methods", "over", "all", "these", "language", "pairs", "sheds", "light", "into", "their", "suitability", "for", "aligning", "embeddings", "from", "distant", "languages", "and", "presents", "new", "challenges", "for", "the", "field", ".", "Finally", ",", "in", "our", "analysis", "we", "identify", "general", "guidelines", "for", "strong", "cross-lingual", "embedding", "baselines", ",", "that", "extend", "to", "language", "pairs", "that", "do", "not", "include", "English", "."], "entities": [{"type": "Operation", "start": 65, "end": 67, "text": "hub language", "sent_idx": 3}, {"type": "Effect", "start": 76, "end": 77, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--766"}
{"text": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.", "tokens": ["Cross-lingual", "word", "embeddings", "(", "CLWE", ")", "are", "often", "evaluated", "on", "bilingual", "lexicon", "induction", "(", "BLI", ")", ".", "Recent", "CLWE", "methods", "use", "linear", "projections", ",", "which", "underfit", "the", "training", "dictionary", ",", "to", "generalize", "on", "BLI", ".", "However", ",", "underfitting", "can", "hinder", "generalization", "to", "other", "downstream", "tasks", "that", "rely", "on", "words", "from", "the", "training", "dictionary", ".", "We", "address", "this", "limitation", "by", "retrofitting", "CLWE", "to", "the", "training", "dictionary", ",", "which", "pulls", "training", "translation", "pairs", "closer", "in", "the", "embedding", "space", "and", "overfits", "the", "training", "dictionary", ".", "This", "simple", "post-processing", "step", "often", "improves", "accuracy", "on", "two", "downstream", "tasks", ",", "despite", "lowering", "BLI", "test", "accuracy", ".", "We", "also", "retrofit", "to", "both", "the", "training", "dictionary", "and", "a", "synthetic", "dictionary", "induced", "from", "CLWE", ",", "which", "sometimes", "generalizes", "even", "better", "on", "downstream", "tasks", ".", "Our", "results", "confirm", "the", "importance", "of", "fully", "exploiting", "training", "dictionary", "in", "downstream", "tasks", "and", "explains", "why", "BLI", "is", "a", "flawed", "CLWE", "evaluation", "."], "entities": [{"type": "Operation", "start": 83, "end": 86, "text": "simple post-processing step", "sent_idx": 4}, {"type": "Effect", "start": 88, "end": 89, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 96, "end": 99, "text": "BLI test accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--201"}
{"text": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers\u2019 performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.", "tokens": ["Although", "pretrained", "Transformers", "such", "as", "BERT", "achieve", "high", "accuracy", "on", "in-distribution", "examples", ",", "do", "they", "generalize", "to", "new", "distributions", "?", "We", "systematically", "measure", "out-of-distribution", "(", "OOD", ")", "generalization", "for", "seven", "NLP", "datasets", "by", "constructing", "a", "new", "robustness", "benchmark", "with", "realistic", "distribution", "shifts", ".", "We", "measure", "the", "generalization", "of", "previous", "models", "including", "bag-of-words", "models", ",", "ConvNets", ",", "and", "LSTMs", ",", "and", "we", "show", "that", "pretrained", "Transformers", "\u2019", "performance", "declines", "are", "substantially", "smaller", ".", "Pretrained", "transformers", "are", "also", "more", "effective", "at", "detecting", "anomalous", "or", "OOD", "examples", ",", "while", "many", "previous", "models", "are", "frequently", "worse", "than", "chance", ".", "We", "examine", "which", "factors", "affect", "robustness", ",", "finding", "that", "larger", "models", "are", "not", "necessarily", "more", "robust", ",", "distillation", "can", "be", "harmful", ",", "and", "more", "diverse", "pretraining", "data", "can", "enhance", "robustness", ".", "Finally", ",", "we", "show", "where", "future", "work", "can", "improve", "OOD", "robustness", "."], "entities": [{"type": "Operation", "start": 118, "end": 122, "text": "more diverse pretraining data", "sent_idx": 4}, {"type": "Effect", "start": 124, "end": 125, "text": "robustness", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--244"}
{"text": "It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh->En and En->De).", "tokens": ["It", "is", "generally", "believed", "that", "a", "translation", "memory", "(", "TM", ")", "should", "be", "beneficial", "for", "machine", "translation", "tasks", ".", "Unfortunately", ",", "existing", "wisdom", "demonstrates", "the", "superiority", "of", "TM-based", "neural", "machine", "translation", "(", "NMT", ")", "only", "on", "the", "TM-specialized", "translation", "tasks", "rather", "than", "general", "tasks", ",", "with", "a", "non-negligible", "computational", "overhead", ".", "In", "this", "paper", ",", "we", "propose", "a", "fast", "and", "accurate", "approach", "to", "TM-based", "NMT", "within", "the", "Transformer", "framework", ":", "the", "model", "architecture", "is", "simple", "and", "employs", "a", "single", "bilingual", "sentence", "as", "its", "TM", ",", "leading", "to", "efficient", "training", "and", "inference", ";", "and", "its", "parameters", "are", "effectively", "optimized", "through", "a", "novel", "training", "criterion", ".", "Extensive", "experiments", "on", "six", "TM-specialized", "tasks", "show", "that", "the", "proposed", "approach", "substantially", "surpasses", "several", "strong", "baselines", "that", "use", "multiple", "TMs", ",", "in", "terms", "of", "BLEU", "and", "running", "time", ".", "In", "particular", ",", "the", "proposed", "approach", "also", "advances", "the", "strong", "baselines", "on", "two", "general", "tasks", "(", "WMT", "news", "Zh->En", "and", "En->De", ")", "."], "entities": [{"type": "Operation", "start": 63, "end": 69, "text": "TM-based NMT within the Transformer framework", "sent_idx": 2}, {"type": "Effect", "start": 128, "end": 129, "text": "BLEU", "sent_idx": 3}, {"type": "Effect", "start": 130, "end": 132, "text": "running time", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--246"}
{"text": "Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the attention mechanisms\u2019 capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input\u2019s contribution to the model outputs. We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease. The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model. We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers. Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision.", "tokens": ["Attention", "mechanisms", "have", "achieved", "substantial", "improvements", "in", "neural", "machine", "translation", "by", "dynamically", "selecting", "relevant", "inputs", "for", "different", "predictions", ".", "However", ",", "recent", "studies", "have", "questioned", "the", "attention", "mechanisms", "\u2019", "capability", "for", "discovering", "decisive", "inputs", ".", "In", "this", "paper", ",", "we", "propose", "to", "calibrate", "the", "attention", "weights", "by", "introducing", "a", "mask", "perturbation", "model", "that", "automatically", "evaluates", "each", "input", "\u2019s", "contribution", "to", "the", "model", "outputs", ".", "We", "increase", "the", "attention", "weights", "assigned", "to", "the", "indispensable", "tokens", ",", "whose", "removal", "leads", "to", "a", "dramatic", "performance", "decrease", ".", "The", "extensive", "experiments", "on", "the", "Transformer-based", "translation", "have", "demonstrated", "the", "effectiveness", "of", "our", "model", ".", "We", "further", "find", "that", "the", "calibrated", "attention", "weights", "are", "more", "uniform", "at", "lower", "layers", "to", "collect", "multiple", "information", "while", "more", "concentrated", "on", "the", "specific", "inputs", "at", "higher", "layers", ".", "Detailed", "analyses", "also", "show", "a", "great", "need", "for", "calibration", "in", "the", "attention", "weights", "with", "high", "entropy", "where", "the", "model", "is", "unconfident", "about", "its", "decision", "."], "entities": [{"type": "Operation", "start": 67, "end": 74, "text": "attention weights assigned to the indispensable tokens", "sent_idx": 3}, {"type": "Effect", "start": 81, "end": 82, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--103"}
{"text": "Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher\u2019s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student\u2019s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher\u2019s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher\u2019s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student\u2019s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x.", "tokens": ["Recently", ",", "knowledge", "distillation", "(", "KD", ")", "has", "shown", "great", "success", "in", "BERT", "compression", ".", "Instead", "of", "only", "learning", "from", "the", "teacher", "\u2019s", "soft", "label", "as", "in", "conventional", "KD", ",", "researchers", "find", "that", "the", "rich", "information", "contained", "in", "the", "hidden", "layers", "of", "BERT", "is", "conducive", "to", "the", "student", "\u2019s", "performance", ".", "To", "better", "exploit", "the", "hidden", "knowledge", ",", "a", "common", "practice", "is", "to", "force", "the", "student", "to", "deeply", "mimic", "the", "teacher", "\u2019s", "hidden", "states", "of", "all", "the", "tokens", "in", "a", "layer-wise", "manner", ".", "In", "this", "paper", ",", "however", ",", "we", "observe", "that", "although", "distilling", "the", "teacher", "\u2019s", "hidden", "state", "knowledge", "(", "HSK", ")", "is", "helpful", ",", "the", "performance", "gain", "(", "marginal", "utility", ")", "diminishes", "quickly", "as", "more", "HSK", "is", "distilled", ".", "To", "understand", "this", "effect", ",", "we", "conduct", "a", "series", "of", "analysis", ".", "Specifically", ",", "we", "divide", "the", "HSK", "of", "BERT", "into", "three", "dimensions", ",", "namely", "depth", ",", "length", "and", "width", ".", "We", "first", "investigate", "a", "variety", "of", "strategies", "to", "extract", "crucial", "knowledge", "for", "each", "single", "dimension", "and", "then", "jointly", "compress", "the", "three", "dimensions", ".", "In", "this", "way", ",", "we", "show", "that", "1", ")", "the", "student", "\u2019s", "performance", "can", "be", "improved", "by", "extracting", "and", "distilling", "the", "crucial", "HSK", ",", "and", "2", ")", "using", "a", "tiny", "fraction", "of", "HSK", "can", "achieve", "the", "same", "performance", "as", "extensive", "HSK", "distillation", ".", "Based", "on", "the", "second", "finding", ",", "we", "further", "propose", "an", "efficient", "KD", "paradigm", "to", "compress", "BERT", ",", "which", "does", "not", "require", "loading", "the", "teacher", "during", "the", "training", "of", "student", ".", "For", "two", "kinds", "of", "student", "models", "and", "computing", "devices", ",", "the", "proposed", "KD", "paradigm", "gives", "rise", "to", "training", "speedup", "of", "2.7x", "3.4x", "."], "entities": [{"type": "Operation", "start": 260, "end": 262, "text": "KD paradigm", "sent_idx": 9}, {"type": "Effect", "start": 266, "end": 267, "text": "speedup", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--228"}
{"text": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.", "tokens": ["State-of-the-art", "argument", "mining", "studies", "have", "advanced", "the", "techniques", "for", "predicting", "argument", "structures", ".", "However", ",", "the", "technology", "for", "capturing", "non-tree-structured", "arguments", "is", "still", "in", "its", "infancy", ".", "In", "this", "paper", ",", "we", "focus", "on", "non-tree", "argument", "mining", "with", "a", "neural", "model", ".", "We", "jointly", "predict", "proposition", "types", "and", "edges", "between", "propositions", ".", "Our", "proposed", "model", "incorporates", "(", "i", ")", "task-specific", "parameterization", "(", "TSP", ")", "that", "effectively", "encodes", "a", "sequence", "of", "propositions", "and", "(", "ii", ")", "a", "proposition-level", "biaffine", "attention", "(", "PLBA", ")", "that", "can", "predict", "a", "non-tree", "argument", "consisting", "of", "edges", ".", "Experimental", "results", "show", "that", "both", "TSP", "and", "PLBA", "boost", "edge", "prediction", "performance", "compared", "to", "baselines", "."], "entities": [{"type": "Operation", "start": 97, "end": 98, "text": "TSP", "sent_idx": 5}, {"type": "Effect", "start": 103, "end": 104, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 99, "end": 100, "text": "PLBA", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--298"}
{"text": "Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.", "tokens": ["Machine", "translation", "(", "MT", ")", "has", "benefited", "from", "using", "synthetic", "training", "data", "originating", "from", "translating", "monolingual", "corpora", ",", "a", "technique", "known", "as", "backtranslation", ".", "Combining", "backtranslated", "data", "from", "different", "sources", "has", "led", "to", "better", "results", "than", "when", "using", "such", "data", "in", "isolation", ".", "In", "this", "work", "we", "analyse", "the", "impact", "that", "data", "translated", "with", "rule-based", ",", "phrase-based", "statistical", "and", "neural", "MT", "systems", "has", "on", "new", "MT", "systems", ".", "We", "use", "a", "real-world", "low-resource", "use-case", "(", "Basque-to-Spanish", "in", "the", "clinical", "domain", ")", "as", "well", "as", "a", "high-resource", "language", "pair", "(", "German-to-English", ")", "to", "test", "different", "scenarios", "with", "backtranslation", "and", "employ", "data", "selection", "to", "optimise", "the", "synthetic", "corpora", ".", "We", "exploit", "different", "data", "selection", "strategies", "in", "order", "to", "reduce", "the", "amount", "of", "data", "used", ",", "while", "at", "the", "same", "time", "maintaining", "high-quality", "MT", "systems", ".", "We", "further", "tune", "the", "data", "selection", "method", "by", "taking", "into", "account", "the", "quality", "of", "the", "MT", "systems", "used", "for", "backtranslation", "and", "lexical", "diversity", "of", "the", "resulting", "corpora", ".", "Our", "experiments", "show", "that", "incorporating", "backtranslated", "data", "from", "different", "sources", "can", "be", "beneficial", ",", "and", "that", "availing", "of", "data", "selection", "can", "yield", "improved", "performance", "."], "entities": [{"type": "Operation", "start": 177, "end": 181, "text": "availing of data selection", "sent_idx": 6}, {"type": "Effect", "start": 184, "end": 185, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 165, "end": 171, "text": "incorporating backtranslated data from different sources", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--359"}
{"text": "Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named \u201cNegative Training\u201d to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.", "tokens": ["Although", "deep", "learning", "models", "have", "brought", "tremendous", "advancements", "to", "the", "field", "of", "open-domain", "dialogue", "response", "generation", ",", "recent", "research", "results", "have", "revealed", "that", "the", "trained", "models", "have", "undesirable", "generation", "behaviors", ",", "such", "as", "malicious", "responses", "and", "generic", "(", "boring", ")", "responses", ".", "In", "this", "work", ",", "we", "propose", "a", "framework", "named", "\u201c", "Negative", "Training", "\u201d", "to", "minimize", "such", "behaviors", ".", "Given", "a", "trained", "model", ",", "the", "framework", "will", "first", "find", "generated", "samples", "that", "exhibit", "the", "undesirable", "behavior", ",", "and", "then", "use", "them", "to", "feed", "negative", "training", "signals", "for", "fine-tuning", "the", "model", ".", "Our", "experiments", "show", "that", "negative", "training", "can", "significantly", "reduce", "the", "hit", "rate", "of", "malicious", "responses", ",", "or", "discourage", "frequent", "responses", "and", "improve", "response", "diversity", "."], "entities": [{"type": "Operation", "start": 96, "end": 98, "text": "negative training", "sent_idx": 3}, {"type": "Effect", "start": 102, "end": 107, "text": "hit rate of malicious responses", "sent_idx": 3}, {"type": "Effect", "start": 114, "end": 116, "text": "response diversity", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--185"}
{"text": "We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms. We present a dataset of explanations paired with demonstrations for web-based tasks. Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment. In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior. This is an interesting example of pragmatic language acquisition without any linguistic annotation.", "tokens": ["We", "explore", "learning", "web-based", "tasks", "from", "a", "human", "teacher", "through", "natural", "language", "explanations", "and", "a", "single", "demonstration", ".", "Our", "approach", "investigates", "a", "new", "direction", "for", "semantic", "parsing", "that", "models", "explaining", "a", "demonstration", "in", "a", "context", ",", "rather", "than", "mapping", "explanations", "to", "demonstrations", ".", "By", "leveraging", "the", "idea", "of", "inverse", "semantics", "from", "program", "synthesis", "to", "reason", "backwards", "from", "observed", "demonstrations", ",", "we", "ensure", "that", "all", "considered", "interpretations", "are", "consistent", "with", "executable", "actions", "in", "any", "context", ",", "thus", "simplifying", "the", "problem", "of", "search", "over", "logical", "forms", ".", "We", "present", "a", "dataset", "of", "explanations", "paired", "with", "demonstrations", "for", "web-based", "tasks", ".", "Our", "methods", "show", "better", "task", "completion", "rates", "than", "a", "supervised", "semantic", "parsing", "baseline", "(", "40", "%", "relative", "improvement", "on", "average", ")", ",", "and", "are", "competitive", "with", "simple", "exploration-and-demonstration", "based", "methods", ",", "while", "requiring", "no", "exploration", "of", "the", "environment", ".", "In", "learning", "to", "align", "explanations", "with", "demonstrations", ",", "basic", "properties", "of", "natural", "language", "syntax", "emerge", "as", "learned", "behavior", ".", "This", "is", "an", "interesting", "example", "of", "pragmatic", "language", "acquisition", "without", "any", "linguistic", "annotation", "."], "entities": [{"type": "Operation", "start": 98, "end": 100, "text": "Our methods", "sent_idx": 4}, {"type": "Effect", "start": 102, "end": 105, "text": "task completion rates", "sent_idx": 4}, {"type": "Operation", "start": 44, "end": 59, "text": "leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--684"}
{"text": "Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data. Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point. Obviously, it is not sufficient to build an entire DRS tree only through these local decisions. In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization. Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels. After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective. We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation. The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers.", "tokens": ["Text-level", "discourse", "rhetorical", "structure", "(", "DRS", ")", "parsing", "is", "known", "to", "be", "challenging", "due", "to", "the", "notorious", "lack", "of", "training", "data", ".", "Although", "recent", "top-down", "DRS", "parsers", "can", "better", "leverage", "global", "document", "context", "and", "have", "achieved", "certain", "success", ",", "the", "performance", "is", "still", "far", "from", "perfect", ".", "To", "our", "knowledge", ",", "all", "previous", "DRS", "parsers", "make", "local", "decisions", "for", "either", "bottom-up", "node", "composition", "or", "top-down", "split", "point", "ranking", "at", "each", "time", "step", ",", "and", "largely", "ignore", "DRS", "parsing", "from", "the", "global", "view", "point", ".", "Obviously", ",", "it", "is", "not", "sufficient", "to", "build", "an", "entire", "DRS", "tree", "only", "through", "these", "local", "decisions", ".", "In", "this", "work", ",", "we", "present", "our", "insight", "on", "evaluating", "the", "pros", "and", "cons", "of", "the", "entire", "DRS", "tree", "for", "global", "optimization", ".", "Specifically", ",", "based", "on", "recent", "well-performing", "top-down", "frameworks", ",", "we", "introduce", "a", "novel", "method", "to", "transform", "both", "gold", "standard", "and", "predicted", "constituency", "trees", "into", "tree", "diagrams", "with", "two", "color", "channels", ".", "After", "that", ",", "we", "learn", "an", "adversarial", "bot", "between", "gold", "and", "fake", "tree", "diagrams", "to", "estimate", "the", "generated", "DRS", "trees", "from", "a", "global", "perspective", ".", "We", "perform", "experiments", "on", "both", "RST-DT", "and", "CDTB", "corpora", "and", "use", "the", "original", "Parseval", "for", "performance", "evaluation", ".", "The", "experimental", "results", "show", "that", "our", "parser", "can", "substantially", "improve", "the", "performance", "when", "compared", "with", "previous", "state-of-the-art", "parsers", "."], "entities": [{"type": "Operation", "start": 161, "end": 170, "text": "an adversarial bot between gold and fake tree diagrams", "sent_idx": 6}, {"type": "Effect", "start": 210, "end": 211, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--305"}
{"text": "Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.", "tokens": ["Most", "classification", "models", "work", "by", "first", "predicting", "a", "posterior", "probability", "distribution", "over", "all", "classes", "and", "then", "selecting", "that", "class", "with", "the", "largest", "estimated", "probability", ".", "In", "many", "settings", "however", ",", "the", "quality", "of", "posterior", "probability", "itself", "(", "e.g.", ",", "65", "%", "chance", "having", "diabetes", ")", ",", "gives", "more", "reliable", "information", "than", "the", "final", "predicted", "class", "alone", ".", "When", "these", "methods", "are", "shown", "to", "be", "poorly", "calibrated", ",", "most", "fixes", "to", "date", "have", "relied", "on", "posterior", "calibration", ",", "which", "rescales", "the", "predicted", "probabilities", "but", "often", "has", "little", "impact", "on", "final", "classifications", ".", "Here", "we", "propose", "an", "end-to-end", "training", "procedure", "called", "posterior", "calibrated", "(", "PosCal", ")", "training", "that", "directly", "optimizes", "the", "objective", "while", "minimizing", "the", "difference", "between", "the", "predicted", "and", "empirical", "posterior", "probabilities", ".", "We", "show", "that", "PosCal", "not", "only", "helps", "reduce", "the", "calibration", "error", "but", "also", "improve", "task", "performance", "by", "penalizing", "drops", "in", "performance", "of", "both", "objectives", ".", "Our", "PosCal", "achieves", "about", "2.5", "%", "of", "task", "performance", "gain", "and", "16.1", "%", "of", "calibration", "error", "reduction", "on", "GLUE", "(", "Wang", "et", "al.", ",", "2018", ")", "compared", "to", "the", "baseline", ".", "We", "achieved", "the", "comparable", "task", "performance", "with", "13.2", "%", "calibration", "error", "reduction", "on", "xSLUE", "(", "Kang", "and", "Hovy", ",", "2019", ")", ",", "but", "not", "outperforming", "the", "two-stage", "calibration", "baseline", ".", "PosCal", "training", "can", "be", "easily", "extendable", "to", "any", "types", "of", "classification", "tasks", "as", "a", "form", "of", "regularization", "term", ".", "Also", ",", "PosCal", "has", "the", "advantage", "that", "it", "incrementally", "tracks", "needed", "statistics", "for", "the", "calibration", "objective", "during", "the", "training", "process", ",", "making", "efficient", "use", "of", "large", "training", "sets", "."], "entities": [{"type": "Operation", "start": 125, "end": 126, "text": "PosCal", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 133, "text": "calibration error", "sent_idx": 4}, {"type": "Operation", "start": 139, "end": 146, "text": "penalizing drops in performance of both objectives", "sent_idx": 4}, {"type": "Effect", "start": 136, "end": 138, "text": "task performance", "sent_idx": 4}, {"type": "Operation", "start": 148, "end": 149, "text": "PosCal", "sent_idx": 5}, {"type": "Effect", "start": 155, "end": 156, "text": "performance", "sent_idx": 5}, {"type": "Effect", "start": 161, "end": 163, "text": "calibration error", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Affect", "head": 4, "tail": 5}, {"type": "Neg_Affect", "head": 4, "tail": 6}], "id": "abstract-2020--acl-main--242"}
{"text": "Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data.", "tokens": ["Pre-trained", "multilingual", "language", "models", ",", "e.g.", ",", "multilingual-BERT", ",", "are", "widely", "used", "in", "cross-lingual", "tasks", ",", "yielding", "the", "state-of-the-art", "performance", ".", "However", ",", "such", "models", "suffer", "from", "a", "large", "performance", "gap", "between", "source", "and", "target", "languages", ",", "especially", "in", "the", "zero-shot", "setting", ",", "where", "the", "models", "are", "fine-tuned", "only", "on", "English", "but", "tested", "on", "other", "languages", "for", "the", "same", "task", ".", "We", "tackle", "this", "issue", "by", "incorporating", "language-agnostic", "information", ",", "specifically", ",", "universal", "syntax", "such", "as", "dependency", "relations", "and", "POS", "tags", ",", "into", "language", "models", ",", "based", "on", "the", "observation", "that", "universal", "syntax", "is", "transferable", "across", "different", "languages", ".", "Our", "approach", ",", "called", "COunterfactual", "SYntax", "(", "COSY", ")", ",", "includes", "the", "design", "of", "SYntax-aware", "networks", "as", "well", "as", "a", "COunterfactual", "training", "method", "to", "implicitly", "force", "the", "networks", "to", "learn", "not", "only", "the", "semantics", "but", "also", "the", "syntax", ".", "To", "evaluate", "COSY", ",", "we", "conduct", "cross-lingual", "experiments", "on", "natural", "language", "inference", "and", "question", "answering", "using", "mBERT", "and", "XLM-R", "as", "network", "backbones", ".", "Our", "results", "show", "that", "COSY", "achieves", "the", "state-of-the-art", "performance", "for", "both", "tasks", ",", "without", "using", "auxiliary", "training", "data", "."], "entities": [{"type": "Operation", "start": 165, "end": 166, "text": "COSY", "sent_idx": 5}, {"type": "Effect", "start": 169, "end": 170, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--48"}
{"text": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work \u2013 by up to 9 BLEU on our low-resource setting.", "tokens": ["End-to-end", "models", "for", "speech", "translation", "(", "ST", ")", "more", "tightly", "couple", "speech", "recognition", "(", "ASR", ")", "and", "machine", "translation", "(", "MT", ")", "than", "a", "traditional", "cascade", "of", "separate", "ASR", "and", "MT", "models", ",", "with", "simpler", "model", "architectures", "and", "the", "potential", "for", "reduced", "error", "propagation", ".", "Their", "performance", "is", "often", "assumed", "to", "be", "superior", ",", "though", "in", "many", "conditions", "this", "is", "not", "yet", "the", "case", ".", "We", "compare", "cascaded", "and", "end-to-end", "models", "across", "high", ",", "medium", ",", "and", "low-resource", "conditions", ",", "and", "show", "that", "cascades", "remain", "stronger", "baselines", ".", "Further", ",", "we", "introduce", "two", "methods", "to", "incorporate", "phone", "features", "into", "ST", "models", ".", "We", "show", "that", "these", "features", "improve", "both", "architectures", ",", "closing", "the", "gap", "between", "end-to-end", "models", "and", "cascades", ",", "and", "outperforming", "previous", "academic", "work", "\u2013", "by", "up", "to", "9", "BLEU", "on", "our", "low-resource", "setting", "."], "entities": [{"type": "Operation", "start": 95, "end": 101, "text": "incorporate phone features into ST models", "sent_idx": 3}, {"type": "Effect", "start": 130, "end": 131, "text": "BLEU", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--217"}
{"text": "The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.", "tokens": ["The", "choice", "of", "hyper-parameters", "affects", "the", "performance", "of", "neural", "models", ".", "While", "much", "previous", "research", "(", "Sutskever", "et", "al.", ",", "2013", ";", "Duchi", "et", "al.", ",", "2011", ";", "Kingma", "and", "Ba", ",", "2015", ")", "focuses", "on", "accelerating", "convergence", "and", "reducing", "the", "effects", "of", "the", "learning", "rate", ",", "comparatively", "few", "papers", "concentrate", "on", "the", "effect", "of", "batch", "size", ".", "In", "this", "paper", ",", "we", "analyze", "how", "increasing", "batch", "size", "affects", "gradient", "direction", ",", "and", "propose", "to", "evaluate", "the", "stability", "of", "gradients", "with", "their", "angle", "change", ".", "Based", "on", "our", "observations", ",", "the", "angle", "change", "of", "gradient", "direction", "first", "tends", "to", "stabilize", "(", "i.e.", "gradually", "decrease", ")", "while", "accumulating", "mini-batches", ",", "and", "then", "starts", "to", "fluctuate", ".", "We", "propose", "to", "automatically", "and", "dynamically", "determine", "batch", "sizes", "by", "accumulating", "gradients", "of", "mini-batches", "and", "performing", "an", "optimization", "step", "at", "just", "the", "time", "when", "the", "direction", "of", "gradients", "starts", "to", "fluctuate", ".", "To", "improve", "the", "efficiency", "of", "our", "approach", "for", "large", "models", ",", "we", "propose", "a", "sampling", "approach", "to", "select", "gradients", "of", "parameters", "sensitive", "to", "the", "batch", "size", ".", "Our", "approach", "dynamically", "determines", "proper", "and", "efficient", "batch", "sizes", "during", "training", ".", "In", "our", "experiments", "on", "the", "WMT", "14", "English", "to", "German", "and", "English", "to", "French", "tasks", ",", "our", "approach", "improves", "the", "Transformer", "with", "a", "fixed", "25k", "batch", "size", "by", "+", "0.73", "and", "+", "0.82", "BLEU", "respectively", "."], "entities": [{"type": "Operation", "start": 161, "end": 163, "text": "sampling approach", "sent_idx": 5}, {"type": "Effect", "start": 219, "end": 220, "text": "BLEU", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--323"}
{"text": "Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17%; spERT:27.81%. We make our annotated WLP-MSTG corpus available to the research community.", "tokens": ["Wet", "laboratory", "protocols", "(", "WLPs", ")", "are", "critical", "for", "conveying", "reproducible", "procedures", "in", "biological", "research", ".", "They", "are", "composed", "of", "instructions", "written", "in", "natural", "language", "describing", "the", "step-wise", "processing", "of", "materials", "by", "specific", "actions", ".", "This", "process", "flow", "description", "for", "reagents", "and", "materials", "synthesis", "in", "WLPs", "can", "be", "captured", "by", "material", "state", "transfer", "graphs", "(", "MSTGs", ")", ",", "which", "encode", "global", "temporal", "and", "causal", "relationships", "between", "actions", ".", "Here", ",", "we", "propose", "methods", "to", "automatically", "generate", "a", "MSTG", "for", "a", "given", "protocol", "by", "extracting", "all", "action", "relationships", "across", "multiple", "sentences", ".", "We", "also", "note", "that", "previous", "corpora", "and", "methods", "focused", "primarily", "on", "local", "intra-sentence", "relationships", "between", "actions", "and", "entities", "and", "did", "not", "address", "two", "critical", "issues", ":", "(", "i", ")", "resolution", "of", "implicit", "arguments", "and", "(", "ii", ")", "establishing", "long-range", "dependencies", "across", "sentences", ".", "We", "propose", "a", "new", "model", "that", "incrementally", "learns", "latent", "structures", "and", "is", "better", "suited", "to", "resolving", "inter-sentence", "relations", "and", "implicit", "arguments", ".", "This", "model", "draws", "upon", "a", "new", "corpus", "WLP-MSTG", "which", "was", "created", "by", "extending", "annotations", "in", "the", "WLP", "corpora", "for", "inter-sentence", "relations", "and", "implicit", "arguments", ".", "Our", "model", "achieves", "an", "F1", "score", "of", "54.53", "%", "for", "temporal", "and", "causal", "relations", "in", "protocols", "from", "our", "corpus", ",", "which", "is", "a", "significant", "improvement", "over", "previous", "models", "-", "DyGIE++:28.17", "%", ";", "spERT:27.81", "%", ".", "We", "make", "our", "annotated", "WLP-MSTG", "corpus", "available", "to", "the", "research", "community", "."], "entities": [{"type": "Operation", "start": 140, "end": 144, "text": "incrementally learns latent structures", "sent_idx": 5}, {"type": "Effect", "start": 185, "end": 187, "text": "F1 score", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--525"}
{"text": "We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the \"N-gram\" model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.", "tokens": ["We", "present", "a", "novel", "machine", "translation", "model", "which", "models", "translation", "by", "a", "linear", "sequence", "of", "operations", ".", "In", "contrast", "to", "the", "\"", "N-gram", "\"", "model", ",", "this", "sequence", "includes", "not", "only", "translation", "but", "also", "reordering", "operations", ".", "Key", "ideas", "of", "our", "model", "are", "(", "i", ")", "a", "new", "reordering", "approach", "which", "better", "restricts", "the", "position", "to", "which", "a", "word", "or", "phrase", "can", "be", "moved", ",", "and", "is", "able", "to", "handle", "short", "and", "long", "distance", "re-orderings", "in", "a", "unified", "way", ",", "and", "(", "ii", ")", "a", "joint", "sequence", "model", "for", "the", "translation", "and", "reordering", "probabilities", "which", "is", "more", "flexible", "than", "standard", "phrase-based", "MT", ".", "We", "observe", "statistically", "significant", "improvements", "in", "BLEU", "over", "Moses", "for", "German-to-English", "and", "Spanish-to-English", "tasks", ",", "and", "comparable", "results", "for", "a", "French-to-English", "task", "."], "entities": [{"type": "Operation", "start": 8, "end": 16, "text": "models translation by a linear sequence of operations", "sent_idx": 0}, {"type": "Effect", "start": 109, "end": 110, "text": "BLEU", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1105"}
{"text": "To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.", "tokens": ["To", "improve", "the", "coherence", "and", "knowledge", "retrieval", "capabilities", "of", "non-task-oriented", "dialogue", "systems", ",", "recent", "Transformer-based", "models", "aim", "to", "integrate", "fixed", "background", "context", ".", "This", "often", "comes", "in", "the", "form", "of", "knowledge", "graphs", ",", "and", "the", "integration", "is", "done", "by", "creating", "pseudo", "utterances", "through", "paraphrasing", "knowledge", "triples", ",", "added", "into", "the", "accumulated", "dialogue", "context", ".", "However", ",", "the", "context", "length", "is", "fixed", "in", "these", "architectures", ",", "which", "restricts", "how", "much", "background", "or", "dialogue", "context", "can", "be", "kept", ".", "In", "this", "work", ",", "we", "propose", "a", "more", "concise", "encoding", "for", "background", "context", "structured", "in", "the", "form", "of", "knowledge", "graphs", ",", "by", "expressing", "the", "graph", "connections", "through", "restrictions", "on", "the", "attention", "weights", ".", "The", "results", "of", "our", "human", "evaluation", "show", "that", "this", "encoding", "reduces", "space", "requirements", "without", "negative", "effects", "on", "the", "precision", "of", "reproduction", "of", "knowledge", "and", "perceived", "consistency", ".", "Further", ",", "models", "trained", "with", "our", "proposed", "context", "encoding", "generate", "dialogues", "that", "are", "judged", "to", "be", "more", "comprehensive", "and", "interesting", "."], "entities": [{"type": "Operation", "start": 99, "end": 109, "text": "expressing the graph connections through restrictions on the attention weights", "sent_idx": 3}, {"type": "Effect", "start": 121, "end": 123, "text": "space requirements", "sent_idx": 4}, {"type": "Effect", "start": 128, "end": 129, "text": "precision", "sent_idx": 4}, {"type": "Effect", "start": 134, "end": 136, "text": "perceived consistency", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--546"}
{"text": "We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.", "tokens": ["We", "propose", "a", "global", "algorithm", "for", "learning", "entailment", "relations", "between", "predicates", ".", "We", "define", "a", "graph", "structure", "over", "predicates", "that", "represents", "entailment", "relations", "as", "directed", "edges", ",", "and", "use", "a", "global", "transitivity", "constraint", "on", "the", "graph", "to", "learn", "the", "optimal", "set", "of", "edges", ",", "by", "formulating", "the", "optimization", "problem", "as", "an", "Integer", "Linear", "Program", ".", "We", "motivate", "this", "graph", "with", "an", "application", "that", "provides", "a", "hierarchical", "summary", "for", "a", "set", "of", "propositions", "that", "focus", "on", "a", "target", "concept", ",", "and", "show", "that", "our", "global", "algorithm", "improves", "performance", "by", "more", "than", "10", "%", "over", "baseline", "algorithms", "."], "entities": [{"type": "Operation", "start": 56, "end": 62, "text": "motivate this graph with an application", "sent_idx": 2}, {"type": "Effect", "start": 86, "end": 87, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1124"}
{"text": "We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.", "tokens": ["We", "present", "a", "neural", "framework", "for", "learning", "associations", "between", "interrelated", "groups", "of", "words", "such", "as", "the", "ones", "found", "in", "Subject-Verb-Object", "(", "SVO", ")", "structures", ".", "Our", "model", "induces", "a", "joint", "function-specific", "word", "vector", "space", ",", "where", "vectors", "of", "e.g.", "plausible", "SVO", "compositions", "lie", "close", "together", ".", "The", "model", "retains", "information", "about", "word", "group", "membership", "even", "in", "the", "joint", "space", ",", "and", "can", "thereby", "effectively", "be", "applied", "to", "a", "number", "of", "tasks", "reasoning", "over", "the", "SVO", "structure", ".", "We", "show", "the", "robustness", "and", "versatility", "of", "the", "proposed", "framework", "by", "reporting", "state-of-the-art", "results", "on", "the", "tasks", "of", "estimating", "selectional", "preference", "and", "event", "similarity", ".", "The", "results", "indicate", "that", "the", "combinations", "of", "representations", "learned", "with", "our", "task-independent", "model", "outperform", "task-specific", "architectures", "from", "prior", "work", ",", "while", "reducing", "the", "number", "of", "parameters", "by", "up", "to", "95", "%", "."], "entities": [{"type": "Operation", "start": 107, "end": 115, "text": "combinations of representations learned with our task-independent model", "sent_idx": 4}, {"type": "Effect", "start": 125, "end": 128, "text": "number of parameters", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--257"}
{"text": "Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.", "tokens": ["Neural", "lexicalized", "PCFGs", "(", "L-PCFGs", ")", "have", "been", "shown", "effective", "in", "grammar", "induction", ".", "However", ",", "to", "reduce", "computational", "complexity", ",", "they", "make", "a", "strong", "independence", "assumption", "on", "the", "generation", "of", "the", "child", "word", "and", "thus", "bilexical", "dependencies", "are", "ignored", ".", "In", "this", "paper", ",", "we", "propose", "an", "approach", "to", "parameterize", "L-PCFGs", "without", "making", "implausible", "independence", "assumptions", ".", "Our", "approach", "directly", "models", "bilexical", "dependencies", "and", "meanwhile", "reduces", "both", "learning", "and", "representation", "complexities", "of", "L-PCFGs", ".", "Experimental", "results", "on", "the", "English", "WSJ", "dataset", "confirm", "the", "effectiveness", "of", "our", "approach", "in", "improving", "both", "running", "speed", "and", "unsupervised", "parsing", "performance", "."], "entities": [{"type": "Operation", "start": 60, "end": 64, "text": "directly models bilexical dependencies", "sent_idx": 3}, {"type": "Effect", "start": 71, "end": 72, "text": "complexities", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--209"}
{"text": "Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.", "tokens": ["Different", "texts", "shall", "by", "nature", "correspond", "to", "different", "number", "of", "keyphrases", ".", "This", "desideratum", "is", "largely", "missing", "from", "existing", "neural", "keyphrase", "generation", "models", ".", "In", "this", "study", ",", "we", "address", "this", "problem", "from", "both", "modeling", "and", "evaluation", "perspectives", ".", "We", "first", "propose", "a", "recurrent", "generative", "model", "that", "generates", "multiple", "keyphrases", "as", "delimiter-separated", "sequences", ".", "Generation", "diversity", "is", "further", "enhanced", "with", "two", "novel", "techniques", "by", "manipulating", "decoder", "hidden", "states", ".", "In", "contrast", "to", "previous", "approaches", ",", "our", "model", "is", "capable", "of", "generating", "diverse", "keyphrases", "and", "controlling", "number", "of", "outputs", ".", "We", "further", "propose", "two", "evaluation", "metrics", "tailored", "towards", "the", "variable-number", "generation", ".", "We", "also", "introduce", "a", "new", "dataset", "StackEx", "that", "expands", "beyond", "the", "only", "existing", "genre", "(", "i.e.", ",", "academic", "writing", ")", "in", "keyphrase", "generation", "tasks", ".", "With", "both", "previous", "and", "new", "evaluation", "metrics", ",", "our", "model", "outperforms", "strong", "baselines", "on", "all", "datasets", "."], "entities": [{"type": "Operation", "start": 43, "end": 46, "text": "recurrent generative model", "sent_idx": 3}, {"type": "Effect", "start": 85, "end": 88, "text": "number of outputs", "sent_idx": 5}, {"type": "Operation", "start": 64, "end": 68, "text": "manipulating decoder hidden states", "sent_idx": 4}, {"type": "Effect", "start": 55, "end": 56, "text": "diversity", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--710"}
{"text": "In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis. It aims at extracting the potential pairs of emotions and their corresponding causes in a document. To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step. To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme. A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs. The 2D representation, interaction, and prediction are integrated into a joint framework. In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.", "tokens": ["In", "recent", "years", ",", "a", "new", "interesting", "task", ",", "called", "emotion-cause", "pair", "extraction", "(", "ECPE", ")", ",", "has", "emerged", "in", "the", "area", "of", "text", "emotion", "analysis", ".", "It", "aims", "at", "extracting", "the", "potential", "pairs", "of", "emotions", "and", "their", "corresponding", "causes", "in", "a", "document", ".", "To", "solve", "this", "task", ",", "the", "existing", "research", "employed", "a", "two-step", "framework", ",", "which", "first", "extracts", "individual", "emotion", "set", "and", "cause", "set", ",", "and", "then", "pair", "the", "corresponding", "emotions", "and", "causes", ".", "However", ",", "such", "a", "pipeline", "of", "two", "steps", "contains", "some", "inherent", "flaws", ":", "1", ")", "the", "modeling", "does", "not", "aim", "at", "extracting", "the", "final", "emotion-cause", "pair", "directly", ";", "2", ")", "the", "errors", "from", "the", "first", "step", "will", "affect", "the", "performance", "of", "the", "second", "step", ".", "To", "address", "these", "shortcomings", ",", "in", "this", "paper", "we", "propose", "a", "new", "end-to-end", "approach", ",", "called", "ECPE-Two-Dimensional", "(", "ECPE-2D", ")", ",", "to", "represent", "the", "emotion-cause", "pairs", "by", "a", "2D", "representation", "scheme", ".", "A", "2D", "transformer", "module", "and", "two", "variants", ",", "window-constrained", "and", "cross-road", "2D", "transformers", ",", "are", "further", "proposed", "to", "model", "the", "interactions", "of", "different", "emotion-cause", "pairs", ".", "The", "2D", "representation", ",", "interaction", ",", "and", "prediction", "are", "integrated", "into", "a", "joint", "framework", ".", "In", "addition", "to", "the", "advantages", "of", "joint", "modeling", ",", "the", "experimental", "results", "on", "the", "benchmark", "emotion", "cause", "corpus", "show", "that", "our", "approach", "improves", "the", "F1", "score", "of", "the", "state-of-the-art", "from", "61.28", "%", "to", "68.89", "%", "."], "entities": [{"type": "Operation", "start": 137, "end": 141, "text": "ECPE-Two-Dimensional (ECPE-2D)", "sent_idx": 4}, {"type": "Effect", "start": 218, "end": 220, "text": "F1 score", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--288"}
{"text": "We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.", "tokens": ["We", "propose", "methods", "for", "estimating", "the", "probability", "that", "an", "entity", "from", "an", "entity", "database", "is", "associated", "with", "a", "web", "search", "query", ".", "Association", "is", "modeled", "using", "a", "query", "entity", "click", "graph", ",", "blending", "general", "query", "click", "logs", "with", "vertical", "query", "click", "logs", ".", "Smoothing", "techniques", "are", "proposed", "to", "address", "the", "inherent", "data", "sparsity", "in", "such", "graphs", ",", "including", "interpolation", "using", "a", "query", "synonymy", "model", ".", "A", "large-scale", "empirical", "analysis", "of", "the", "smoothing", "techniques", ",", "over", "a", "2-year", "click", "graph", "collected", "from", "a", "commercial", "search", "engine", ",", "shows", "significant", "reductions", "in", "modeling", "error", ".", "The", "association", "models", "are", "then", "applied", "to", "the", "task", "of", "recommending", "products", "to", "web", "queries", ",", "by", "annotating", "queries", "with", "products", "from", "a", "large", "catalog", "and", "then", "mining", "query-product", "associations", "through", "web", "search", "session", "analysis", ".", "Experimental", "analysis", "shows", "that", "our", "smoothing", "techniques", "improve", "coverage", "while", "keeping", "precision", "stable", ",", "and", "overall", ",", "that", "our", "top-performing", "model", "affects", "9", "%", "of", "general", "web", "queries", "with", "94", "%", "precision", "."], "entities": [{"type": "Operation", "start": 134, "end": 136, "text": "smoothing techniques", "sent_idx": 5}, {"type": "Effect", "start": 137, "end": 138, "text": "coverage", "sent_idx": 5}, {"type": "Effect", "start": 140, "end": 141, "text": "precision", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "P11-1009"}
{"text": "This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system.", "tokens": ["This", "paper", "proposes", "new", "algorithms", "to", "compute", "the", "sense", "similarity", "between", "two", "units", "(", "words", ",", "phrases", ",", "rules", ",", "etc", ".", ")", "from", "parallel", "corpora", ".", "The", "sense", "similarity", "scores", "are", "computed", "by", "using", "the", "vector", "space", "model", ".", "We", "then", "apply", "the", "algorithms", "to", "statistical", "machine", "translation", "by", "computing", "the", "sense", "similarity", "between", "the", "source", "and", "target", "side", "of", "translation", "rule", "pairs", ".", "Similarity", "scores", "are", "used", "as", "additional", "features", "of", "the", "translation", "model", "to", "improve", "translation", "performance", ".", "Significant", "improvements", "are", "obtained", "over", "a", "state-of-the-art", "hierarchical", "phrase-based", "machine", "translation", "system", "."], "entities": [{"type": "Operation", "start": 65, "end": 67, "text": "Similarity scores", "sent_idx": 3}, {"type": "Effect", "start": 79, "end": 80, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1086"}
{"text": "Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks\u2019 label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell\u2019s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.", "tokens": ["Many", "joint", "entity", "relation", "extraction", "models", "setup", "two", "separated", "label", "spaces", "for", "the", "two", "sub-tasks", "(", "i.e.", ",", "entity", "detection", "and", "relation", "classification", ")", ".", "We", "argue", "that", "this", "setting", "may", "hinder", "the", "information", "interaction", "between", "entities", "and", "relations", ".", "In", "this", "work", ",", "we", "propose", "to", "eliminate", "the", "different", "treatment", "on", "the", "two", "sub-tasks", "\u2019", "label", "spaces", ".", "The", "input", "of", "our", "model", "is", "a", "table", "containing", "all", "word", "pairs", "from", "a", "sentence", ".", "Entities", "and", "relations", "are", "represented", "by", "squares", "and", "rectangles", "in", "the", "table", ".", "We", "apply", "a", "unified", "classifier", "to", "predict", "each", "cell", "\u2019s", "label", ",", "which", "unifies", "the", "learning", "of", "two", "sub-tasks", ".", "For", "testing", ",", "an", "effective", "(", "yet", "fast", ")", "approximate", "decoder", "is", "proposed", "for", "finding", "squares", "and", "rectangles", "from", "tables", ".", "Experiments", "on", "three", "benchmarks", "(", "ACE04", ",", "ACE05", ",", "SciERC", ")", "show", "that", ",", "using", "only", "half", "the", "number", "of", "parameters", ",", "our", "model", "achieves", "competitive", "accuracy", "with", "the", "best", "extractor", ",", "and", "is", "faster", "."], "entities": [{"type": "Operation", "start": 151, "end": 153, "text": "our model", "sent_idx": 7}, {"type": "Effect", "start": 155, "end": 156, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--19"}
{"text": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with  cross-lingual position representations  to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT\u201914 English \u21d2 German, WAT\u201917 Japanese \u21d2 English, and WMT\u201917 Chinese \u21d4 English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.", "tokens": ["Position", "encoding", "(", "PE", ")", ",", "an", "essential", "part", "of", "self-attention", "networks", "(", "SANs", ")", ",", "is", "used", "to", "preserve", "the", "word", "order", "information", "for", "natural", "language", "processing", "tasks", ",", "generating", "fixed", "position", "indices", "for", "input", "sequences", ".", "However", ",", "in", "cross-lingual", "scenarios", ",", "machine", "translation", ",", "the", "PEs", "of", "source", "and", "target", "sentences", "are", "modeled", "independently", ".", "Due", "to", "word", "order", "divergences", "in", "different", "languages", ",", "modeling", "the", "cross-lingual", "positional", "relationships", "might", "help", "SANs", "tackle", "this", "problem", ".", "In", "this", "paper", ",", "we", "augment", "SANs", "with", " ", "cross-lingual", "position", "representations", " ", "to", "model", "the", "bilingually", "aware", "latent", "structure", "for", "the", "input", "sentence", ".", "Specifically", ",", "we", "utilize", "bracketing", "transduction", "grammar", "(BTG)-based", "reordering", "information", "to", "encourage", "SANs", "to", "learn", "bilingual", "diagonal", "alignments", ".", "Experimental", "results", "on", "WMT\u201914", "English", "\u21d2", "German", ",", "WAT\u201917", "Japanese", "\u21d2", "English", ",", "and", "WMT\u201917", "Chinese", "\u21d4", "English", "translation", "tasks", "demonstrate", "that", "our", "approach", "significantly", "and", "consistently", "improves", "translation", "quality", "over", "strong", "baselines", ".", "Extensive", "analyses", "confirm", "that", "the", "performance", "gains", "come", "from", "the", "cross-lingual", "information", "."], "entities": [{"type": "Operation", "start": 167, "end": 169, "text": "cross-lingual information", "sent_idx": 6}, {"type": "Effect", "start": 162, "end": 163, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 107, "end": 114, "text": "utilize bracketing transduction grammar (BTG)-based reordering information", "sent_idx": 4}, {"type": "Effect", "start": 151, "end": 153, "text": "translation quality", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--153"}
{"text": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model\u2019s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.", "tokens": ["Neural", "module", "networks", "(", "NMNs", ")", "are", "a", "popular", "approach", "for", "modeling", "compositionality", ":", "they", "achieve", "high", "accuracy", "when", "applied", "to", "problems", "in", "language", "and", "vision", ",", "while", "reflecting", "the", "compositional", "structure", "of", "the", "problem", "in", "the", "network", "architecture", ".", "However", ",", "prior", "work", "implicitly", "assumed", "that", "the", "structure", "of", "the", "network", "modules", ",", "describing", "the", "abstract", "reasoning", "process", ",", "provides", "a", "faithful", "explanation", "of", "the", "model", "\u2019s", "reasoning", ";", "that", "is", ",", "that", "all", "modules", "perform", "their", "intended", "behaviour", ".", "In", "this", "work", ",", "we", "propose", "and", "conduct", "a", "systematic", "evaluation", "of", "the", "intermediate", "outputs", "of", "NMNs", "on", "NLVR2", "and", "DROP", ",", "two", "datasets", "which", "require", "composing", "multiple", "reasoning", "steps", ".", "We", "find", "that", "the", "intermediate", "outputs", "differ", "from", "the", "expected", "output", ",", "illustrating", "that", "the", "network", "structure", "does", "not", "provide", "a", "faithful", "explanation", "of", "model", "behaviour", ".", "To", "remedy", "that", ",", "we", "train", "the", "model", "with", "auxiliary", "supervision", "and", "propose", "particular", "choices", "for", "module", "architecture", "that", "yield", "much", "better", "faithfulness", ",", "at", "a", "minimal", "cost", "to", "accuracy", "."], "entities": [{"type": "Operation", "start": 151, "end": 157, "text": "propose particular choices for module architecture", "sent_idx": 4}, {"type": "Effect", "start": 168, "end": 169, "text": "accuracy", "sent_idx": 4}, {"type": "Operation", "start": 144, "end": 150, "text": "train the model with auxiliary supervision", "sent_idx": 4}, {"type": "Effect", "start": 161, "end": 162, "text": "faithfulness", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--495"}
{"text": "Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.", "tokens": ["Named", "entity", "recognition", "(", "NER", ")", "remains", "challenging", "when", "entity", "mentions", "can", "be", "discontinuous", ".", "Existing", "methods", "break", "the", "recognition", "process", "into", "several", "sequential", "steps", ".", "In", "training", ",", "they", "predict", "conditioned", "on", "the", "golden", "intermediate", "results", ",", "while", "at", "inference", "relying", "on", "the", "model", "output", "of", "the", "previous", "steps", ",", "which", "introduces", "exposure", "bias", ".", "To", "solve", "this", "problem", ",", "we", "first", "construct", "a", "segment", "graph", "for", "each", "sentence", ",", "in", "which", "each", "node", "denotes", "a", "segment", "(", "a", "continuous", "entity", "on", "its", "own", ",", "or", "a", "part", "of", "discontinuous", "entities", ")", ",", "and", "an", "edge", "links", "two", "nodes", "that", "belong", "to", "the", "same", "entity", ".", "The", "nodes", "and", "edges", "can", "be", "generated", "respectively", "in", "one", "stage", "with", "a", "grid", "tagging", "scheme", "and", "learned", "jointly", "using", "a", "novel", "architecture", "named", "Mac", ".", "Then", "discontinuous", "NER", "can", "be", "reformulated", "as", "a", "non-parametric", "process", "of", "discovering", "maximal", "cliques", "in", "the", "graph", "and", "concatenating", "the", "spans", "in", "each", "clique", ".", "Experiments", "on", "three", "benchmarks", "show", "that", "our", "method", "outperforms", "the", "state-of-the-art", "(", "SOTA", ")", "results", ",", "with", "up", "to", "3.5", "percentage", "points", "improvement", "on", "F1", ",", "and", "achieves", "5x", "speedup", "over", "the", "SOTA", "model", "."], "entities": [{"type": "Operation", "start": 63, "end": 70, "text": "construct a segment graph for each sentence", "sent_idx": 3}, {"type": "Effect", "start": 182, "end": 183, "text": "F1", "sent_idx": 6}, {"type": "Effect", "start": 187, "end": 188, "text": "speedup", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--63"}
{"text": "Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn\u2019t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn\u2019t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.", "tokens": ["Visual", "referring", "expression", "recognition", "is", "a", "challenging", "task", "that", "requires", "natural", "language", "understanding", "in", "the", "context", "of", "an", "image", ".", "We", "critically", "examine", "RefCOCOg", ",", "a", "standard", "benchmark", "for", "this", "task", ",", "using", "a", "human", "study", "and", "show", "that", "83.7", "%", "of", "test", "instances", "do", "not", "require", "reasoning", "on", "linguistic", "structure", ",", "i.e.", ",", "words", "are", "enough", "to", "identify", "the", "target", "object", ",", "the", "word", "order", "does", "n\u2019t", "matter", ".", "To", "measure", "the", "true", "progress", "of", "existing", "models", ",", "we", "split", "the", "test", "set", "into", "two", "sets", ",", "one", "which", "requires", "reasoning", "on", "linguistic", "structure", "and", "the", "other", "which", "does", "n\u2019t", ".", "Additionally", ",", "we", "create", "an", "out-of-distribution", "dataset", "Ref-Adv", "by", "asking", "crowdworkers", "to", "perturb", "in-domain", "examples", "such", "that", "the", "target", "object", "changes", ".", "Using", "these", "datasets", ",", "we", "empirically", "show", "that", "existing", "methods", "fail", "to", "exploit", "linguistic", "structure", "and", "are", "12", "%", "to", "23", "%", "lower", "in", "performance", "than", "the", "established", "progress", "for", "this", "task", ".", "We", "also", "propose", "two", "methods", ",", "one", "based", "on", "contrastive", "learning", "and", "the", "other", "based", "on", "multi-task", "learning", ",", "to", "increase", "the", "robustness", "of", "ViLBERT", ",", "the", "current", "state-of-the-art", "model", "for", "this", "task", ".", "Our", "datasets", "are", "publicly", "available", "at", "https://github.com/aws/aws-refcocog-adv", "."], "entities": [{"type": "Operation", "start": 173, "end": 175, "text": "multi-task learning", "sent_idx": 5}, {"type": "Effect", "start": 179, "end": 180, "text": "robustness", "sent_idx": 5}, {"type": "Operation", "start": 166, "end": 168, "text": "contrastive learning", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--586"}
{"text": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose P\u02c62 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P\u02c62 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.", "tokens": ["Despite", "the", "continuing", "efforts", "to", "improve", "the", "engagingness", "and", "consistency", "of", "chit-chat", "dialogue", "systems", ",", "the", "majority", "of", "current", "work", "simply", "focus", "on", "mimicking", "human-like", "responses", ",", "leaving", "understudied", "the", "aspects", "of", "modeling", "understanding", "between", "interlocutors", ".", "The", "research", "in", "cognitive", "science", ",", "instead", ",", "suggests", "that", "understanding", "is", "an", "essential", "signal", "for", "a", "high-quality", "chit-chat", "conversation", ".", "Motivated", "by", "this", ",", "we", "propose", "P\u02c62", "Bot", ",", "a", "transmitter-receiver", "based", "framework", "with", "the", "aim", "of", "explicitly", "modeling", "understanding", ".", "Specifically", ",", "P\u02c62", "Bot", "incorporates", "mutual", "persona", "perception", "to", "enhance", "the", "quality", "of", "personalized", "dialogue", "generation", ".", "Experiments", "on", "a", "large", "public", "dataset", ",", "Persona-Chat", ",", "demonstrate", "the", "effectiveness", "of", "our", "approach", ",", "with", "a", "considerable", "boost", "over", "the", "state-of-the-art", "baselines", "across", "both", "automatic", "metrics", "and", "human", "evaluations", "."], "entities": [{"type": "Operation", "start": 81, "end": 87, "text": "P\u02c62 Bot incorporates mutual persona perception", "sent_idx": 3}, {"type": "Effect", "start": 90, "end": 95, "text": "quality of personalized dialogue generation", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--131"}
{"text": "We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer\u2019s interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image holds a vital cue for this task. Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task. Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data. To enable BERT to process other visual information, we append image captions to the scene-text. This achieves an accuracy of 89.69%, which is an improvement of 4.7%. This is the best reported result for this task.", "tokens": ["We", "consider", "a", "task", "based", "on", "CVPR", "2018", "challenge", "dataset", "on", "advertisement", "(", "Ad", ")", "understanding", ".", "The", "task", "involves", "detecting", "the", "viewer", "\u2019s", "interpretation", "of", "an", "Ad", "image", "captured", "as", "text", ".", "Recent", "results", "have", "shown", "that", "the", "embedded", "scene-text", "in", "the", "image", "holds", "a", "vital", "cue", "for", "this", "task", ".", "Motivated", "by", "this", ",", "we", "fine-tune", "the", "base", "BERT", "model", "for", "a", "sentence-pair", "classification", "task", ".", "Despite", "utilizing", "the", "scene-text", "as", "the", "only", "source", "of", "visual", "information", ",", "we", "could", "achieve", "a", "hit-or-miss", "accuracy", "of", "84.95", "%", "on", "the", "challenge", "test", "data", ".", "To", "enable", "BERT", "to", "process", "other", "visual", "information", ",", "we", "append", "image", "captions", "to", "the", "scene-text", ".", "This", "achieves", "an", "accuracy", "of", "89.69", "%", ",", "which", "is", "an", "improvement", "of", "4.7", "%", ".", "This", "is", "the", "best", "reported", "result", "for", "this", "task", "."], "entities": [{"type": "Operation", "start": 105, "end": 111, "text": "append image captions to the scene-text", "sent_idx": 5}, {"type": "Effect", "start": 115, "end": 116, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--674"}
{"text": "The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.", "tokens": ["The", "automatic", "coding", "of", "clinical", "documents", "is", "an", "important", "task", "for", "today", "'s", "healthcare", "providers", ".", "Though", "it", "can", "be", "viewed", "as", "multi-label", "document", "classification", ",", "the", "coding", "problem", "has", "the", "interesting", "property", "that", "most", "code", "assignments", "can", "be", "supported", "by", "a", "single", "phrase", "found", "in", "the", "input", "document", ".", "We", "propose", "a", "Lexically-Triggered", "Hidden", "Markov", "Model", "(", "LT-HMM", ")", "that", "leverages", "these", "phrases", "to", "improve", "coding", "accuracy", ".", "The", "LT-HMM", "works", "in", "two", "stages", ":", "first", ",", "a", "lexical", "match", "is", "performed", "against", "a", "term", "dictionary", "to", "collect", "a", "set", "of", "candidate", "codes", "for", "a", "document", ".", "Next", ",", "a", "discriminative", "HMM", "selects", "the", "best", "subset", "of", "codes", "to", "assign", "to", "the", "document", "by", "tagging", "candidates", "as", "present", "or", "absent", ".", "By", "confirming", "codes", "proposed", "by", "a", "dictionary", ",", "the", "LT-HMM", "can", "share", "features", "across", "codes", ",", "enabling", "strong", "performance", "even", "on", "rare", "codes", ".", "In", "fact", ",", "we", "are", "able", "to", "recover", "codes", "that", "do", "not", "occur", "in", "the", "training", "set", "at", "all", ".", "Our", "approach", "achieves", "the", "best", "ever", "performance", "on", "the", "2007", "Medical", "NLP", "Challenge", "test", "set", ",", "with", "an", "F-measure", "of", "89.84", "."], "entities": [{"type": "Operation", "start": 53, "end": 60, "text": "Lexically-Triggered Hidden Markov Model (LT-HMM)", "sent_idx": 2}, {"type": "Effect", "start": 184, "end": 185, "text": "F-measure", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1075"}
{"text": "Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.", "tokens": ["Unsupervised", "style", "transfer", "aims", "to", "change", "the", "style", "of", "an", "input", "sentence", "while", "preserving", "its", "original", "content", "without", "using", "parallel", "training", "data", ".", "In", "current", "dominant", "approaches", ",", "owing", "to", "the", "lack", "of", "fine-grained", "control", "on", "the", "influence", "from", "the", "target", "style", ",", "they", "are", "unable", "to", "yield", "desirable", "output", "sentences", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "attentional", "sequence-to-sequence", "(", "Seq2seq", ")", "model", "that", "dynamically", "exploits", "the", "relevance", "of", "each", "output", "word", "to", "the", "target", "style", "for", "unsupervised", "style", "transfer", ".", "Specifically", ",", "we", "first", "pretrain", "a", "style", "classifier", ",", "where", "the", "relevance", "of", "each", "input", "word", "to", "the", "original", "style", "can", "be", "quantified", "via", "layer-wise", "relevance", "propagation", ".", "In", "a", "denoising", "auto-encoding", "manner", ",", "we", "train", "an", "attentional", "Seq2seq", "model", "to", "reconstruct", "input", "sentences", "and", "repredict", "word-level", "previously-quantified", "style", "relevance", "simultaneously", ".", "In", "this", "way", ",", "this", "model", "is", "endowed", "with", "the", "ability", "to", "automatically", "predict", "the", "style", "relevance", "of", "each", "output", "word", ".", "Then", ",", "we", "equip", "the", "decoder", "of", "this", "model", "with", "a", "neural", "style", "component", "to", "exploit", "the", "predicted", "wordlevel", "style", "relevance", "for", "better", "style", "transfer", ".", "Particularly", ",", "we", "fine-tune", "this", "model", "using", "a", "carefully-designed", "objective", "function", "involving", "style", "transfer", ",", "style", "relevance", "consistency", ",", "content", "preservation", "and", "fluency", "modeling", "loss", "terms", ".", "Experimental", "results", "show", "that", "our", "proposed", "model", "achieves", "state-of-the-art", "performance", "in", "terms", "of", "both", "transfer", "accuracy", "and", "content", "preservation", "."], "entities": [{"type": "Operation", "start": 60, "end": 66, "text": "attentional sequence-to-sequence (Seq2seq) model", "sent_idx": 2}, {"type": "Effect", "start": 220, "end": 221, "text": "performance", "sent_idx": 8}, {"type": "Effect", "start": 225, "end": 227, "text": "transfer accuracy", "sent_idx": 8}, {"type": "Effect", "start": 228, "end": 230, "text": "content preservation", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--639"}
{"text": "Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.", "tokens": ["Question-answer", "driven", "Semantic", "Role", "Labeling", "(", "QA-SRL", ")", "was", "proposed", "as", "an", "attractive", "open", "and", "natural", "flavour", "of", "SRL", ",", "potentially", "attainable", "from", "laymen", ".", "Recently", ",", "a", "large-scale", "crowdsourced", "QA-SRL", "corpus", "and", "a", "trained", "parser", "were", "released", ".", "Trying", "to", "replicate", "the", "QA-SRL", "annotation", "for", "new", "texts", ",", "we", "found", "that", "the", "resulting", "annotations", "were", "lacking", "in", "quality", ",", "particularly", "in", "coverage", ",", "making", "them", "insufficient", "for", "further", "research", "and", "evaluation", ".", "In", "this", "paper", ",", "we", "present", "an", "improved", "crowdsourcing", "protocol", "for", "complex", "semantic", "annotation", ",", "involving", "worker", "selection", "and", "training", ",", "and", "a", "data", "consolidation", "phase", ".", "Applying", "this", "protocol", "to", "QA-SRL", "yielded", "high-quality", "annotation", "with", "drastically", "higher", "coverage", ",", "producing", "a", "new", "gold", "evaluation", "dataset", ".", "We", "believe", "that", "our", "annotation", "protocol", "and", "gold", "standard", "will", "facilitate", "future", "replicable", "research", "of", "natural", "semantic", "annotations", "."], "entities": [{"type": "Operation", "start": 80, "end": 83, "text": "improved crowdsourcing protocol", "sent_idx": 3}, {"type": "Effect", "start": 111, "end": 112, "text": "coverage", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--626"}
{"text": "Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.", "tokens": ["Sentiment", "analysis", "on", "Twitter", "data", "has", "attracted", "much", "attention", "recently", ".", "In", "this", "paper", ",", "we", "focus", "on", "target-dependent", "Twitter", "sentiment", "classification", ";", "namely", ",", "given", "a", "query", ",", "we", "classify", "the", "sentiments", "of", "the", "tweets", "as", "positive", ",", "negative", "or", "neutral", "according", "to", "whether", "they", "contain", "positive", ",", "negative", "or", "neutral", "sentiments", "about", "that", "query", ".", "Here", "the", "query", "serves", "as", "the", "target", "of", "the", "sentiments", ".", "The", "state-of-the-art", "approaches", "for", "solving", "this", "problem", "always", "adopt", "the", "target-independent", "strategy", ",", "which", "may", "assign", "irrelevant", "sentiments", "to", "the", "given", "target", ".", "Moreover", ",", "the", "state-of-the-art", "approaches", "only", "take", "the", "tweet", "to", "be", "classified", "into", "consideration", "when", "classifying", "the", "sentiment", ";", "they", "ignore", "its", "context", "(", "i.e.", ",", "related", "tweets", ")", ".", "However", ",", "because", "tweets", "are", "usually", "short", "and", "more", "ambiguous", ",", "sometimes", "it", "is", "not", "enough", "to", "consider", "only", "the", "current", "tweet", "for", "sentiment", "classification", ".", "In", "this", "paper", ",", "we", "propose", "to", "improve", "target-dependent", "Twitter", "sentiment", "classification", "by", "1", ")", "incorporating", "target-dependent", "features", ";", "and", "2", ")", "taking", "related", "tweets", "into", "consideration", ".", "According", "to", "the", "experimental", "results", ",", "our", "approach", "greatly", "improves", "the", "performance", "of", "target-dependent", "sentiment", "classification", "."], "entities": [{"type": "Operation", "start": 162, "end": 165, "text": "incorporating target-dependent features", "sent_idx": 6}, {"type": "Effect", "start": 186, "end": 187, "text": "performance", "sent_idx": 7}, {"type": "Operation", "start": 169, "end": 174, "text": "taking related tweets into consideration", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P11-1016"}
{"text": "Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.", "tokens": ["Imitation", "learning", "algorithms", "provide", "state-of-the-art", "results", "on", "many", "structured", "prediction", "tasks", "by", "learning", "near-optimal", "search", "policies", ".", "Such", "algorithms", "assume", "training-time", "access", "to", "an", "expert", "that", "can", "provide", "the", "optimal", "action", "at", "any", "queried", "state", ";", "unfortunately", ",", "the", "number", "of", "such", "queries", "is", "often", "prohibitive", ",", "frequently", "rendering", "these", "approaches", "impractical", ".", "To", "combat", "this", "query", "complexity", ",", "we", "consider", "an", "active", "learning", "setting", "in", "which", "the", "learning", "algorithm", "has", "additional", "access", "to", "a", "much", "cheaper", "noisy", "heuristic", "that", "provides", "noisy", "guidance", ".", "Our", "algorithm", ",", "LEAQI", ",", "learns", "a", "difference", "classifier", "that", "predicts", "when", "the", "expert", "is", "likely", "to", "disagree", "with", "the", "heuristic", ",", "and", "queries", "the", "expert", "only", "when", "necessary", ".", "We", "apply", "LEAQI", "to", "three", "sequence", "labelling", "tasks", ",", "demonstrating", "significantly", "fewer", "queries", "to", "the", "expert", "and", "comparable", "(", "or", "better", ")", "accuracies", "over", "a", "passive", "approach", "."], "entities": [{"type": "Operation", "start": 116, "end": 117, "text": "LEAQI", "sent_idx": 4}, {"type": "Effect", "start": 126, "end": 127, "text": "queries", "sent_idx": 4}, {"type": "Effect", "start": 136, "end": 137, "text": "accuracies", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--189"}
{"text": "Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.", "tokens": ["Maintaining", "a", "consistent", "personality", "in", "conversations", "is", "quite", "natural", "for", "human", "beings", ",", "but", "is", "still", "a", "non-trivial", "task", "for", "machines", ".", "The", "persona-based", "dialogue", "generation", "task", "is", "thus", "introduced", "to", "tackle", "the", "personality-inconsistent", "problem", "by", "incorporating", "explicit", "persona", "text", "into", "dialogue", "generation", "models", ".", "Despite", "the", "success", "of", "existing", "persona-based", "models", "on", "generating", "human-like", "responses", ",", "their", "one-stage", "decoding", "framework", "can", "hardly", "avoid", "the", "generation", "of", "inconsistent", "persona", "words", ".", "In", "this", "work", ",", "we", "introduce", "a", "three-stage", "framework", "that", "employs", "a", "generate-delete-rewrite", "mechanism", "to", "delete", "inconsistent", "words", "from", "a", "generated", "response", "prototype", "and", "further", "rewrite", "it", "to", "a", "personality-consistent", "one", ".", "We", "carry", "out", "evaluations", "by", "both", "human", "and", "automatic", "metrics", ".", "Experiments", "on", "the", "Persona-Chat", "dataset", "show", "that", "our", "approach", "achieves", "good", "performance", "."], "entities": [{"type": "Operation", "start": 78, "end": 80, "text": "three-stage framework", "sent_idx": 3}, {"type": "Effect", "start": 125, "end": 126, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--516"}
{"text": "We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy.", "tokens": ["We", "investigate", "whether", "wording", ",", "stylistic", "choices", ",", "and", "online", "behavior", "can", "be", "used", "to", "predict", "the", "age", "category", "of", "blog", "authors", ".", "Our", "hypothesis", "is", "that", "significant", "changes", "in", "writing", "style", "distinguish", "pre-social", "media", "bloggers", "from", "post-social", "media", "bloggers", ".", "Through", "experimentation", "with", "a", "range", "of", "years", ",", "we", "found", "that", "the", "birth", "dates", "of", "students", "in", "college", "at", "the", "time", "when", "social", "media", "such", "as", "AIM", ",", "SMS", "text", "messaging", ",", "MySpace", "and", "Facebook", "first", "became", "popular", ",", "enable", "accurate", "age", "prediction", ".", "We", "also", "show", "that", "internet", "writing", "characteristics", "are", "important", "features", "for", "age", "prediction", ",", "but", "that", "lexical", "content", "is", "also", "needed", "to", "produce", "significantly", "more", "accurate", "results", ".", "Our", "best", "results", "allow", "for", "81.57", "%", "accuracy", "."], "entities": [{"type": "Operation", "start": 89, "end": 92, "text": "internet writing characteristics", "sent_idx": 3}, {"type": "Effect", "start": 120, "end": 121, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1077"}
{"text": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)\u2019s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.", "tokens": ["Sequence-to-sequence", "constituent", "parsing", "requires", "a", "linearization", "to", "represent", "trees", "as", "sequences", ".", "Top-down", "tree", "linearizations", ",", "which", "can", "be", "based", "on", "brackets", "or", "shift-reduce", "actions", ",", "have", "achieved", "the", "best", "accuracy", "to", "date", ".", "In", "this", "paper", ",", "we", "show", "that", "these", "results", "can", "be", "improved", "by", "using", "an", "in-order", "linearization", "instead", ".", "Based", "on", "this", "observation", ",", "we", "implement", "an", "enriched", "in-order", "shift-reduce", "linearization", "inspired", "by", "Vinyals", "et", "al.", "(", "2015", ")", "\u2019s", "approach", ",", "achieving", "the", "best", "accuracy", "to", "date", "on", "the", "English", "PTB", "dataset", "among", "fully-supervised", "single-model", "sequence-to-sequence", "constituent", "parsers", ".", "Finally", ",", "we", "apply", "deterministic", "attention", "mechanisms", "to", "match", "the", "speed", "of", "state-of-the-art", "transition-based", "parsers", ",", "thus", "showing", "that", "sequence-to-sequence", "models", "can", "match", "them", ",", "not", "only", "in", "accuracy", ",", "but", "also", "in", "speed", "."], "entities": [{"type": "Operation", "start": 61, "end": 65, "text": "enriched in-order shift-reduce linearization", "sent_idx": 3}, {"type": "Effect", "start": 79, "end": 80, "text": "accuracy", "sent_idx": 3}, {"type": "Operation", "start": 98, "end": 101, "text": "deterministic attention mechanisms", "sent_idx": 4}, {"type": "Effect", "start": 122, "end": 123, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 127, "end": 128, "text": "speed", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}, {"type": "Affect", "head": 2, "tail": 4}], "id": "abstract-2020--acl-main--376"}
{"text": "Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at https://github.com/StonyBrookNLP/irene.", "tokens": ["Existing", "software-based", "energy", "measurements", "of", "NLP", "models", "are", "not", "accurate", "because", "they", "do", "not", "consider", "the", "complex", "interactions", "between", "energy", "consumption", "and", "model", "execution", ".", "We", "present", "IrEne", ",", "an", "interpretable", "and", "extensible", "energy", "prediction", "system", "that", "accurately", "predicts", "the", "inference", "energy", "consumption", "of", "a", "wide", "range", "of", "Transformer-based", "NLP", "models", ".", "IrEne", "constructs", "a", "model", "tree", "graph", "that", "breaks", "down", "the", "NLP", "model", "into", "modules", "that", "are", "further", "broken", "down", "into", "low-level", "machine", "learning", "(", "ML", ")", "primitives", ".", "IrEne", "predicts", "the", "inference", "energy", "consumption", "of", "the", "ML", "primitives", "as", "a", "function", "of", "generalizable", "features", "and", "fine-grained", "runtime", "resource", "usage", ".", "IrEne", "then", "aggregates", "these", "low-level", "predictions", "recursively", "to", "predict", "the", "energy", "of", "each", "module", "and", "finally", "of", "the", "entire", "model", ".", "Experiments", "across", "multiple", "Transformer", "models", "show", "IrEne", "predicts", "inference", "energy", "consumption", "of", "transformer", "models", "with", "an", "error", "of", "under", "7", "%", "compared", "to", "the", "ground", "truth", ".", "In", "contrast", ",", "existing", "energy", "models", "see", "an", "error", "of", "over", "50", "%", ".", "We", "also", "show", "how", "IrEne", "can", "be", "used", "to", "conduct", "energy", "bottleneck", "analysis", "and", "to", "easily", "evaluate", "the", "energy", "impact", "of", "different", "architectural", "choices", ".", "We", "release", "the", "code", "and", "data", "at", "https://github.com/StonyBrookNLP/irene", "."], "entities": [{"type": "Operation", "start": 129, "end": 130, "text": "IrEne", "sent_idx": 5}, {"type": "Effect", "start": 139, "end": 140, "text": "error", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--167"}
{"text": "Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.", "tokens": ["Word", "embeddings", "derived", "from", "human-generated", "corpora", "inherit", "strong", "gender", "bias", "which", "can", "be", "further", "amplified", "by", "downstream", "models", ".", "Some", "commonly", "adopted", "debiasing", "approaches", ",", "including", "the", "seminal", "Hard", "Debias", "algorithm", ",", "apply", "post-processing", "procedures", "that", "project", "pre-trained", "word", "embeddings", "into", "a", "subspace", "orthogonal", "to", "an", "inferred", "gender", "subspace", ".", "We", "discover", "that", "semantic-agnostic", "corpus", "regularities", "such", "as", "word", "frequency", "captured", "by", "the", "word", "embeddings", "negatively", "impact", "the", "performance", "of", "these", "algorithms", ".", "We", "propose", "a", "simple", "but", "effective", "technique", ",", "Double", "Hard", "Debias", ",", "which", "purifies", "the", "word", "embeddings", "against", "such", "corpus", "regularities", "prior", "to", "inferring", "and", "removing", "the", "gender", "subspace", ".", "Experiments", "on", "three", "bias", "mitigation", "benchmarks", "show", "that", "our", "approach", "preserves", "the", "distributional", "semantics", "of", "the", "pre-trained", "word", "embeddings", "while", "reducing", "gender", "bias", "to", "a", "significantly", "larger", "degree", "than", "prior", "approaches", "."], "entities": [{"type": "Operation", "start": 81, "end": 84, "text": "Double Hard Debias", "sent_idx": 3}, {"type": "Effect", "start": 124, "end": 126, "text": "gender bias", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--484"}
{"text": "Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.", "tokens": ["Training", "data", "for", "NLP", "tasks", "often", "exhibits", "gender", "bias", "in", "that", "fewer", "sentences", "refer", "to", "women", "than", "to", "men", ".", "In", "Neural", "Machine", "Translation", "(", "NMT", ")", "gender", "bias", "has", "been", "shown", "to", "reduce", "translation", "quality", ",", "particularly", "when", "the", "target", "language", "has", "grammatical", "gender", ".", "The", "recent", "WinoMT", "challenge", "set", "allows", "us", "to", "measure", "this", "effect", "directly", "(", "Stanovsky", "et", "al", ",", "2019", ")", "Ideally", "we", "would", "reduce", "system", "bias", "by", "simply", "debiasing", "all", "data", "prior", "to", "training", ",", "but", "achieving", "this", "effectively", "is", "itself", "a", "challenge", ".", "Rather", "than", "attempt", "to", "create", "a", "\u2018", "balanced", "\u2019", "dataset", ",", "we", "use", "transfer", "learning", "on", "a", "small", "set", "of", "trusted", ",", "gender-balanced", "examples", ".", "This", "approach", "gives", "strong", "and", "consistent", "improvements", "in", "gender", "debiasing", "with", "much", "less", "computational", "cost", "than", "training", "from", "scratch", ".", "A", "known", "pitfall", "of", "transfer", "learning", "on", "new", "domains", "is", "\u2018", "catastrophic", "forgetting", "\u2019", ",", "which", "we", "address", "at", "adaptation", "and", "inference", "time", ".", "During", "adaptation", "we", "show", "that", "Elastic", "Weight", "Consolidation", "allows", "a", "performance", "trade-off", "between", "general", "translation", "quality", "and", "bias", "reduction", ".", "At", "inference", "time", "we", "propose", "a", "lattice-rescoring", "scheme", "which", "outperforms", "all", "systems", "evaluated", "in", "Stanovsky", "et", "al", ",", "2019", "on", "WinoMT", "with", "no", "degradation", "of", "general", "test", "set", "BLEU", ".", "We", "demonstrate", "our", "approach", "translating", "from", "English", "into", "three", "languages", "with", "varied", "linguistic", "properties", "and", "data", "availability", "."], "entities": [{"type": "Operation", "start": 184, "end": 186, "text": "lattice-rescoring scheme", "sent_idx": 7}, {"type": "Effect", "start": 206, "end": 207, "text": "BLEU", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--690"}
{"text": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).", "tokens": ["Modern", "models", "for", "event", "causality", "identification", "(", "ECI", ")", "are", "mainly", "based", "on", "supervised", "learning", ",", "which", "are", "prone", "to", "the", "data", "lacking", "problem", ".", "Unfortunately", ",", "the", "existing", "NLP-related", "augmentation", "methods", "can", "not", "directly", "produce", "available", "data", "required", "for", "this", "task", ".", "To", "solve", "the", "data", "lacking", "problem", ",", "we", "introduce", "a", "new", "approach", "to", "augment", "training", "data", "for", "event", "causality", "identification", ",", "by", "iteratively", "generating", "new", "examples", "and", "classifying", "event", "causality", "in", "a", "dual", "learning", "framework", ".", "On", "the", "one", "hand", ",", "our", "approach", "is", "knowledge", "guided", ",", "which", "can", "leverage", "existing", "knowledge", "bases", "to", "generate", "well-formed", "new", "sentences", ".", "On", "the", "other", "hand", ",", "our", "approach", "employs", "a", "dual", "mechanism", ",", "which", "is", "a", "learnable", "augmentation", "framework", ",", "and", "can", "interactively", "adjust", "the", "generation", "process", "to", "generate", "task-related", "sentences", ".", "Experimental", "results", "on", "two", "benchmarks", "EventStoryLine", "and", "Causal-TimeBank", "show", "that", "1", ")", "our", "method", "can", "augment", "suitable", "task-related", "training", "data", "for", "ECI", ";", "2", ")", "our", "method", "outperforms", "previous", "methods", "on", "EventStoryLine", "and", "Causal-TimeBank", "(", "+", "2.5", "and", "+", "2.1", "points", "on", "F1", "value", "respectively", ")", "."], "entities": [{"type": "Operation", "start": 65, "end": 78, "text": "iteratively generating new examples and classifying event causality in a dual learning framework", "sent_idx": 2}, {"type": "Effect", "start": 175, "end": 176, "text": "F1", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--276"}
{"text": "One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing. We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods. Experimental results demonstrate that our adaptation is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency.", "tokens": ["One", "of", "the", "main", "bottlenecks", "in", "developing", "discourse", "dependency", "parsers", "is", "the", "lack", "of", "annotated", "training", "data", ".", "A", "potential", "solution", "is", "to", "utilize", "abundant", "unlabeled", "data", "by", "using", "unsupervised", "techniques", ",", "but", "there", "is", "so", "far", "little", "research", "in", "unsupervised", "discourse", "dependency", "parsing", ".", "Fortunately", ",", "unsupervised", "syntactic", "dependency", "parsing", "has", "been", "studied", "by", "decades", ",", "which", "could", "potentially", "be", "adapted", "for", "discourse", "parsing", ".", "In", "this", "paper", ",", "we", "propose", "a", "simple", "yet", "effective", "method", "to", "adapt", "unsupervised", "syntactic", "dependency", "parsing", "methodology", "for", "unsupervised", "discourse", "dependency", "parsing", ".", "We", "apply", "the", "method", "to", "adapt", "two", "state-of-the-art", "unsupervised", "syntactic", "dependency", "parsing", "methods", ".", "Experimental", "results", "demonstrate", "that", "our", "adaptation", "is", "effective", ".", "Moreover", ",", "we", "extend", "the", "adapted", "methods", "to", "the", "semi-supervised", "and", "supervised", "setting", "and", "surprisingly", ",", "we", "find", "that", "they", "outperform", "previous", "methods", "specially", "designed", "for", "supervised", "discourse", "parsing", ".", "Further", "analysis", "shows", "our", "adaptations", "result", "in", "superiority", "not", "only", "in", "parsing", "accuracy", "but", "also", "in", "time", "and", "space", "efficiency", "."], "entities": [{"type": "Operation", "start": 147, "end": 148, "text": "adaptations", "sent_idx": 7}, {"type": "Effect", "start": 155, "end": 156, "text": "accuracy", "sent_idx": 7}, {"type": "Effect", "start": 159, "end": 160, "text": "time", "sent_idx": 7}, {"type": "Effect", "start": 161, "end": 163, "text": "space efficiency", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--449"}
{"text": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.", "tokens": ["We", "propose", "a", "novel", "constituency", "parsing", "model", "that", "casts", "the", "parsing", "problem", "into", "a", "series", "of", "pointing", "tasks", ".", "Specifically", ",", "our", "model", "estimates", "the", "likelihood", "of", "a", "span", "being", "a", "legitimate", "tree", "constituent", "via", "the", "pointing", "score", "corresponding", "to", "the", "boundary", "words", "of", "the", "span", ".", "Our", "parsing", "model", "supports", "efficient", "top-down", "decoding", "and", "our", "learning", "objective", "is", "able", "to", "enforce", "structural", "consistency", "without", "resorting", "to", "the", "expensive", "CKY", "inference", ".", "The", "experiments", "on", "the", "standard", "English", "Penn", "Treebank", "parsing", "task", "show", "that", "our", "method", "achieves", "92.78", "F1", "without", "using", "pre-trained", "models", ",", "which", "is", "higher", "than", "all", "the", "existing", "methods", "with", "similar", "time", "complexity", ".", "Using", "pre-trained", "BERT", ",", "our", "model", "achieves", "95.48", "F1", ",", "which", "is", "competitive", "with", "the", "state-of-the-art", "while", "being", "faster", ".", "Our", "approach", "also", "establishes", "new", "state-of-the-art", "in", "Basque", "and", "Swedish", "in", "the", "SPMRL", "shared", "tasks", "on", "multilingual", "constituency", "parsing", "."], "entities": [{"type": "Operation", "start": 107, "end": 110, "text": "Using pre-trained BERT", "sent_idx": 4}, {"type": "Effect", "start": 115, "end": 116, "text": "F1", "sent_idx": 4}, {"type": "Operation", "start": 8, "end": 18, "text": "casts the parsing problem into a series of pointing tasks", "sent_idx": 0}, {"type": "Effect", "start": 88, "end": 89, "text": "F1", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--301"}
{"text": "Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation.", "tokens": ["Lexica", "distinguishing", "all", "morphologically", "related", "forms", "of", "each", "lexeme", "are", "crucial", "to", "many", "language", "technologies", ",", "yet", "building", "them", "is", "expensive", ".", "We", "propose", "a", "frugal", "paradigm", "completion", "approach", "that", "predicts", "all", "related", "forms", "in", "a", "morphological", "paradigm", "from", "as", "few", "manually", "provided", "forms", "as", "possible", ".", "It", "induces", "typological", "information", "during", "training", "which", "it", "uses", "to", "determine", "the", "best", "sources", "at", "test", "time", ".", "We", "evaluate", "our", "language-agnostic", "approach", "on", "7", "diverse", "languages", ".", "Compared", "to", "popular", "alternative", "approaches", ",", "ours", "reduces", "manual", "labor", "by", "16", "-", "63", "%", "and", "is", "the", "most", "robust", "to", "typological", "variation", "."], "entities": [{"type": "Operation", "start": 25, "end": 29, "text": "frugal paradigm completion approach", "sent_idx": 1}, {"type": "Effect", "start": 83, "end": 85, "text": "manual labor", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--733"}
{"text": "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.", "tokens": ["Multilingual", "neural", "machine", "translation", "(", "NMT", ")", "has", "led", "to", "impressive", "accuracy", "improvements", "in", "low-resource", "scenarios", "by", "sharing", "common", "linguistic", "information", "across", "languages", ".", "However", ",", "the", "traditional", "multilingual", "model", "fails", "to", "capture", "the", "diversity", "and", "specificity", "of", "different", "languages", ",", "resulting", "in", "inferior", "performance", "compared", "with", "individual", "models", "that", "are", "sufficiently", "trained", ".", "In", "this", "paper", ",", "we", "incorporate", "a", "language-aware", "interlingua", "into", "the", "Encoder-Decoder", "architecture", ".", "The", "interlingual", "network", "enables", "the", "model", "to", "learn", "a", "language-independent", "representation", "from", "the", "semantic", "spaces", "of", "different", "languages", ",", "while", "still", "allowing", "for", "language-specific", "specialization", "of", "a", "particular", "language-pair", ".", "Experiments", "show", "that", "our", "proposed", "method", "achieves", "remarkable", "improvements", "over", "state-of-the-art", "multilingual", "NMT", "baselines", "and", "produces", "comparable", "performance", "with", "strong", "individual", "models", "."], "entities": [{"type": "Operation", "start": 59, "end": 67, "text": "incorporate a language-aware interlingua into the Encoder-Decoder architecture", "sent_idx": 2}, {"type": "Effect", "start": 115, "end": 116, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--150"}
{"text": "We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.", "tokens": ["We", "present", "a", "novel", "scheme", "to", "apply", "factored", "phrase-based", "SMT", "to", "a", "language", "pair", "with", "very", "disparate", "morphological", "structures", ".", "Our", "approach", "relies", "on", "syntactic", "analysis", "on", "the", "source", "side", "(", "English", ")", "and", "then", "encodes", "a", "wide", "variety", "of", "local", "and", "non-local", "syntactic", "structures", "as", "complex", "structural", "tags", "which", "appear", "as", "additional", "factors", "in", "the", "training", "data", ".", "On", "the", "target", "side", "(", "Turkish", ")", ",", "we", "only", "perform", "morphological", "analysis", "and", "disambiguation", "but", "treat", "the", "complete", "complex", "morphological", "tag", "as", "a", "factor", ",", "instead", "of", "separating", "morphemes", ".", "We", "incrementally", "explore", "capturing", "various", "syntactic", "substructures", "as", "complex", "tags", "on", "the", "English", "side", ",", "and", "evaluate", "how", "our", "translations", "improve", "in", "BLEU", "scores", ".", "Our", "maximal", "set", "of", "source", "and", "target", "side", "transformations", ",", "coupled", "with", "some", "additional", "techniques", ",", "provide", "an", "39", "%", "relative", "improvement", "from", "a", "baseline", "17.08", "to", "23.78", "BLEU", ",", "all", "averaged", "over", "10", "training", "and", "test", "sets", ".", "Now", "that", "the", "syntactic", "analysis", "on", "the", "English", "side", "is", "available", ",", "we", "also", "experiment", "with", "more", "long", "distance", "constituent", "reordering", "to", "bring", "the", "English", "constituent", "order", "close", "to", "Turkish", ",", "but", "find", "that", "these", "transformations", "do", "not", "provide", "any", "additional", "consistent", "tangible", "gains", "when", "averaged", "over", "the", "10", "sets", "."], "entities": [{"type": "Operation", "start": 93, "end": 104, "text": "capturing various syntactic substructures as complex tags on the English side", "sent_idx": 3}, {"type": "Effect", "start": 112, "end": 114, "text": "BLEU scores", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1047"}
{"text": "One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is O(n 2 ) , increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.", "tokens": ["One", "of", "the", "reasons", "Transformer", "translation", "models", "are", "popular", "is", "that", "self-attention", "networks", "for", "context", "modelling", "can", "be", "easily", "parallelized", "at", "sequence", "level", ".", "However", ",", "the", "computational", "complexity", "of", "a", "self-attention", "network", "is", "O(n", "2", ")", ",", "increasing", "quadratically", "with", "sequence", "length", ".", "By", "contrast", ",", "the", "complexity", "of", "LSTM-based", "approaches", "is", "only", "O(n", ")", ".", "In", "practice", ",", "however", ",", "LSTMs", "are", "much", "slower", "to", "train", "than", "self-attention", "networks", "as", "they", "can", "not", "be", "parallelized", "at", "sequence", "level", ":", "to", "model", "context", ",", "the", "current", "LSTM", "state", "relies", "on", "the", "full", "LSTM", "computation", "of", "the", "preceding", "state", ".", "This", "has", "to", "be", "computed", "n", "times", "for", "a", "sequence", "of", "length", "n.", "The", "linear", "transformations", "involved", "in", "the", "LSTM", "gate", "and", "state", "computations", "are", "the", "major", "cost", "factors", "in", "this", ".", "To", "enable", "sequence-level", "parallelization", "of", "LSTMs", ",", "we", "approximate", "full", "LSTM", "context", "modelling", "by", "computing", "hidden", "states", "and", "gates", "with", "the", "current", "input", "and", "a", "simple", "bag-of-words", "representation", "of", "the", "preceding", "tokens", "context", ".", "This", "allows", "us", "to", "compute", "each", "input", "step", "efficiently", "in", "parallel", ",", "avoiding", "the", "formerly", "costly", "sequential", "linear", "transformations", ".", "We", "then", "connect", "the", "outputs", "of", "each", "parallel", "step", "with", "computationally", "cheap", "element-wise", "computations", ".", "We", "call", "this", "the", "Highly", "Parallelized", "LSTM", ".", "To", "further", "constrain", "the", "number", "of", "LSTM", "parameters", ",", "we", "compute", "several", "small", "HPLSTMs", "in", "parallel", "like", "multi-head", "attention", "in", "the", "Transformer", ".", "The", "experiments", "show", "that", "our", "MHPLSTM", "decoder", "achieves", "significant", "BLEU", "improvements", ",", "while", "being", "even", "slightly", "faster", "than", "the", "self-attention", "network", "in", "training", ",", "and", "much", "faster", "than", "the", "standard", "LSTM", "."], "entities": [{"type": "Operation", "start": 237, "end": 239, "text": "MHPLSTM decoder", "sent_idx": 10}, {"type": "Effect", "start": 241, "end": 242, "text": "BLEU", "sent_idx": 10}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--23"}
{"text": "Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF). CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures. In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF. Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings.", "tokens": ["Virtual", "adversarial", "training", "(", "VAT", ")", "is", "a", "powerful", "technique", "to", "improve", "model", "robustness", "in", "both", "supervised", "and", "semi-supervised", "settings", ".", "It", "is", "effective", "and", "can", "be", "easily", "adopted", "on", "lots", "of", "image", "classification", "and", "text", "classification", "tasks", ".", "However", ",", "its", "benefits", "to", "sequence", "labeling", "tasks", "such", "as", "named", "entity", "recognition", "(", "NER", ")", "have", "not", "been", "shown", "as", "significant", ",", "mostly", ",", "because", "the", "previous", "approach", "can", "not", "combine", "VAT", "with", "the", "conditional", "random", "field", "(", "CRF", ")", ".", "CRF", "can", "significantly", "boost", "accuracy", "for", "sequence", "models", "by", "putting", "constraints", "on", "label", "transitions", ",", "which", "makes", "it", "an", "essential", "component", "in", "most", "state-of-the-art", "sequence", "labeling", "model", "architectures", ".", "In", "this", "paper", ",", "we", "propose", "SeqVAT", ",", "a", "method", "which", "naturally", "applies", "VAT", "to", "sequence", "labeling", "models", "with", "CRF", ".", "Empirical", "studies", "show", "that", "SeqVAT", "not", "only", "significantly", "improves", "the", "sequence", "labeling", "performance", "over", "baselines", "under", "supervised", "settings", ",", "but", "also", "outperforms", "state-of-the-art", "approaches", "under", "semi-supervised", "settings", "."], "entities": [{"type": "Operation", "start": 135, "end": 136, "text": "SeqVAT", "sent_idx": 5}, {"type": "Effect", "start": 143, "end": 144, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 0, "end": 6, "text": "Virtual adversarial training (VAT)", "sent_idx": 0}, {"type": "Effect", "start": 12, "end": 14, "text": "model robustness", "sent_idx": 0}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--777"}
{"text": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.", "tokens": ["We", "learn", "a", "joint", "model", "of", "sentence", "extraction", "and", "compression", "for", "multi-document", "summarization", ".", "Our", "model", "scores", "candidate", "summaries", "according", "to", "a", "combined", "linear", "model", "whose", "features", "factor", "over", "(", "1", ")", "the", "n-gram", "types", "in", "the", "summary", "and", "(", "2", ")", "the", "compressions", "used", ".", "We", "train", "the", "model", "using", "a", "margin-based", "objective", "whose", "loss", "captures", "end", "summary", "quality", ".", "Because", "of", "the", "exponentially", "large", "set", "of", "candidate", "summaries", ",", "we", "use", "a", "cutting-plane", "algorithm", "to", "incrementally", "detect", "and", "add", "active", "constraints", "efficiently", ".", "Inference", "in", "our", "model", "can", "be", "cast", "as", "an", "ILP", "and", "thereby", "solved", "in", "reasonable", "time", ";", "we", "also", "present", "a", "fast", "approximation", "scheme", "which", "achieves", "similar", "performance", ".", "Our", "jointly", "extracted", "and", "compressed", "summaries", "outperform", "both", "unlearned", "baselines", "and", "our", "learned", "extraction-only", "system", "on", "both", "ROUGE", "and", "Pyramid", ",", "without", "a", "drop", "in", "judged", "linguistic", "quality", ".", "We", "achieve", "the", "highest", "published", "ROUGE", "results", "to", "date", "on", "the", "TAC", "2008", "data", "set", "."], "entities": [{"type": "Operation", "start": 2, "end": 10, "text": "a joint model of sentence extraction and compression", "sent_idx": 0}, {"type": "Effect", "start": 131, "end": 132, "text": "ROUGE", "sent_idx": 5}, {"type": "Effect", "start": 133, "end": 134, "text": "Pyramid", "sent_idx": 5}, {"type": "Effect", "start": 148, "end": 149, "text": "ROUGE", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "P11-1049"}
{"text": "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.", "tokens": ["In", "this", "paper", ",", "we", "show", "that", "neural", "machine", "translation", "(", "NMT", ")", "systems", "trained", "on", "large", "back-translated", "data", "overfit", "some", "of", "the", "characteristics", "of", "machine-translated", "texts", ".", "Such", "NMT", "systems", "better", "translate", "human-produced", "translations", ",", "i.e.", ",", "translationese", ",", "but", "may", "largely", "worsen", "the", "translation", "quality", "of", "original", "texts", ".", "Our", "analysis", "reveals", "that", "adding", "a", "simple", "tag", "to", "back-translations", "prevents", "this", "quality", "degradation", "and", "improves", "on", "average", "the", "overall", "translation", "quality", "by", "helping", "the", "NMT", "system", "to", "distinguish", "back-translated", "data", "from", "original", "parallel", "data", "during", "training", ".", "We", "also", "show", "that", ",", "in", "contrast", "to", "high-resource", "configurations", ",", "NMT", "systems", "trained", "in", "low-resource", "settings", "are", "much", "less", "vulnerable", "to", "overfit", "back-translations", ".", "We", "conclude", "that", "the", "back-translations", "in", "the", "training", "data", "should", "always", "be", "tagged", "especially", "when", "the", "origin", "of", "the", "text", "to", "be", "translated", "is", "unknown", "."], "entities": [{"type": "Operation", "start": 55, "end": 61, "text": "adding a simple tag to back-translations", "sent_idx": 2}, {"type": "Effect", "start": 63, "end": 64, "text": "quality", "sent_idx": 2}, {"type": "Effect", "start": 71, "end": 73, "text": "translation quality", "sent_idx": 2}, {"type": "Operation", "start": 74, "end": 88, "text": "helping the NMT system to distinguish back-translated data from original parallel data during training", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 2}], "id": "abstract-2020--acl-main--532"}
{"text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "tokens": ["If", "we", "take", "an", "existing", "supervised", "NLP", "system", ",", "a", "simple", "and", "general", "way", "to", "improve", "accuracy", "is", "to", "use", "unsupervised", "word", "representations", "as", "extra", "word", "features", ".", "We", "evaluate", "Brown", "clusters", ",", "Collobert", "and", "Weston", "(", "2008", ")", "embeddings", ",", "and", "HLBL", "(", "Mnih", "&", "Hinton", ",", "2009", ")", "embeddings", "of", "words", "on", "both", "NER", "and", "chunking", ".", "We", "use", "near", "state-of-the-art", "supervised", "baselines", ",", "and", "find", "that", "each", "of", "the", "three", "word", "representations", "improves", "the", "accuracy", "of", "these", "baselines", ".", "We", "find", "further", "improvements", "by", "combining", "different", "word", "representations", ".", "You", "can", "download", "our", "word", "features", ",", "for", "off-the-shelf", "use", "in", "existing", "NLP", "systems", ",", "as", "well", "as", "our", "code", ",", "here", ":", "http://metaoptimize.com/projects/wordreprs/"], "entities": [{"type": "Operation", "start": 69, "end": 75, "text": "each of the three word representations", "sent_idx": 2}, {"type": "Effect", "start": 77, "end": 78, "text": "accuracy", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1040"}
{"text": "Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.", "tokens": ["Extractive", "methods", "for", "multi-document", "summarization", "are", "mainly", "governed", "by", "information", "overlap", ",", "coherence", ",", "and", "content", "constraints", ".", "We", "present", "an", "unsupervised", "probabilistic", "approach", "to", "model", "the", "hidden", "abstract", "concepts", "across", "documents", "as", "well", "as", "the", "correlation", "between", "these", "concepts", ",", "to", "generate", "topically", "coherent", "and", "non-redundant", "summaries", ".", "Based", "on", "human", "evaluations", "our", "models", "generate", "summaries", "with", "higher", "linguistic", "quality", "in", "terms", "of", "coherence", ",", "readability", ",", "and", "redundancy", "compared", "to", "benchmark", "systems", ".", "Although", "our", "system", "is", "unsupervised", "and", "optimized", "for", "topical", "coherence", ",", "we", "achieve", "a", "44.1", "ROUGE", "on", "the", "DUC-07", "test", "set", ",", "roughly", "in", "the", "range", "of", "state-of-the-art", "supervised", "models", "."], "entities": [{"type": "Operation", "start": 21, "end": 24, "text": "unsupervised probabilistic approach", "sent_idx": 1}, {"type": "Effect", "start": 90, "end": 91, "text": "ROUGE", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1050"}
{"text": "Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.", "tokens": ["Existing", "Visual", "Question", "Answering", "(", "VQA", ")", "methods", "tend", "to", "exploit", "dataset", "biases", "and", "spurious", "statistical", "correlations", ",", "instead", "of", "producing", "right", "answers", "for", "the", "right", "reasons", ".", "To", "address", "this", "issue", ",", "recent", "bias", "mitigation", "methods", "for", "VQA", "propose", "to", "incorporate", "visual", "cues", "(", "e.g.", ",", "human", "attention", "maps", ")", "to", "better", "ground", "the", "VQA", "models", ",", "showcasing", "impressive", "gains", ".", "However", ",", "we", "show", "that", "the", "performance", "improvements", "are", "not", "a", "result", "of", "improved", "visual", "grounding", ",", "but", "a", "regularization", "effect", "which", "prevents", "over-fitting", "to", "linguistic", "priors", ".", "For", "instance", ",", "we", "find", "that", "it", "is", "not", "actually", "necessary", "to", "provide", "proper", ",", "human-based", "cues", ";", "random", ",", "insensible", "cues", "also", "result", "in", "similar", "improvements", ".", "Based", "on", "this", "observation", ",", "we", "propose", "a", "simpler", "regularization", "scheme", "that", "does", "not", "require", "any", "external", "annotations", "and", "yet", "achieves", "near", "state-of-the-art", "performance", "on", "VQA-CPv2", "."], "entities": [{"type": "Operation", "start": 127, "end": 129, "text": "regularization scheme", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 142, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--727"}
{"text": "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.", "tokens": ["The", "uniform", "information", "density", "(", "UID", ")", "hypothesis", ",", "which", "posits", "that", "speakers", "behaving", "optimally", "tend", "to", "distribute", "information", "uniformly", "across", "a", "linguistic", "signal", ",", "has", "gained", "traction", "in", "psycholinguistics", "as", "an", "explanation", "for", "certain", "syntactic", ",", "morphological", ",", "and", "prosodic", "choices", ".", "In", "this", "work", ",", "we", "explore", "whether", "the", "UID", "hypothesis", "can", "be", "operationalized", "as", "an", "inductive", "bias", "for", "statistical", "language", "modeling", ".", "Specifically", ",", "we", "augment", "the", "canonical", "MLE", "objective", "for", "training", "language", "models", "with", "a", "regularizer", "that", "encodes", "UID", ".", "In", "experiments", "on", "ten", "languages", "spanning", "five", "language", "families", ",", "we", "find", "that", "using", "UID", "regularization", "consistently", "improves", "perplexity", "in", "language", "models", ",", "having", "a", "larger", "effect", "when", "training", "data", "is", "limited", ".", "Moreover", ",", "via", "an", "analysis", "of", "generated", "sequences", ",", "we", "find", "that", "UID-regularized", "language", "models", "have", "other", "desirable", "properties", ",", "e.g.", ",", "they", "generate", "text", "that", "is", "more", "lexically", "diverse", ".", "Our", "results", "not", "only", "suggest", "that", "UID", "is", "a", "reasonable", "inductive", "bias", "for", "language", "modeling", ",", "but", "also", "provide", "an", "alternative", "validation", "of", "the", "UID", "hypothesis", "using", "modern-day", "NLP", "tools", "."], "entities": [{"type": "Operation", "start": 97, "end": 100, "text": "using UID regularization", "sent_idx": 3}, {"type": "Effect", "start": 102, "end": 103, "text": "perplexity", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--404"}
{"text": "Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent\u2019s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person\u2019s speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor\u2019s persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.", "tokens": ["Non-goal", "oriented", "dialog", "agents", "(", "i.e.", "chatbots", ")", "aim", "to", "produce", "varying", "and", "engaging", "conversations", "with", "a", "user", ";", "however", ",", "they", "typically", "exhibit", "either", "inconsistent", "personality", "across", "conversations", "or", "the", "average", "personality", "of", "all", "users", ".", "This", "paper", "addresses", "these", "issues", "by", "controlling", "an", "agent", "\u2019s", "persona", "upon", "generation", "via", "conditioning", "on", "prior", "conversations", "of", "a", "target", "actor", ".", "In", "doing", "so", ",", "we", "are", "able", "to", "utilize", "more", "abstract", "patterns", "within", "a", "person", "\u2019s", "speech", "and", "better", "emulate", "them", "in", "generated", "responses", ".", "This", "work", "introduces", "the", "Generative", "Conversation", "Control", "model", ",", "an", "augmented", "and", "fine-tuned", "GPT-2", "language", "model", "that", "conditions", "on", "past", "reference", "conversations", "to", "probabilistically", "model", "multi-turn", "conversations", "in", "the", "actor", "\u2019s", "persona", ".", "We", "introduce", "an", "accompanying", "data", "collection", "procedure", "to", "obtain", "10.3", "M", "conversations", "from", "6", "months", "worth", "of", "Reddit", "comments", ".", "We", "demonstrate", "that", "scaling", "model", "sizes", "from", "117", "M", "to", "8.3B", "parameters", "yields", "an", "improvement", "from", "23.14", "to", "13.14", "perplexity", "on", "1.7", "M", "held", "out", "Reddit", "conversations", ".", "Increasing", "model", "scale", "yielded", "similar", "improvements", "in", "human", "evaluations", "that", "measure", "preference", "of", "model", "samples", "to", "the", "held", "out", "target", "distribution", "in", "terms", "of", "realism", "(", "31", "%", "increased", "to", "37", "%", "preference", ")", ",", "style", "matching", "(", "37", "%", "to", "42", "%", ")", ",", "grammar", "and", "content", "quality", "(", "29", "%", "to", "42", "%", ")", ",", "and", "conversation", "coherency", "(", "32", "%", "to", "40", "%", ")", ".", "We", "find", "that", "conditionally", "modeling", "past", "conversations", "improves", "perplexity", "by", "0.47", "in", "automatic", "evaluations", ".", "Through", "human", "trials", "we", "identify", "positive", "trends", "between", "conditional", "modeling", "and", "style", "matching", "and", "outline", "steps", "to", "further", "improve", "persona", "control", "."], "entities": [{"type": "Operation", "start": 166, "end": 169, "text": "Increasing model scale", "sent_idx": 6}, {"type": "Effect", "start": 177, "end": 178, "text": "preference", "sent_idx": 6}, {"type": "Operation", "start": 237, "end": 241, "text": "conditionally modeling past conversations", "sent_idx": 7}, {"type": "Effect", "start": 242, "end": 243, "text": "perplexity", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--8"}
{"text": "We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A \u201cself-diversity\u201d criterion is proposed for measuring the \u201cworthiness\u201d of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task. To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.", "tokens": ["We", "propose", "a", "Semi-supervIsed", "GeNerative", "Active", "Learning", "(", "SIGNAL", ")", "model", "to", "address", "the", "imbalance", ",", "efficiency", ",", "and", "text", "camouflage", "problems", "of", "Chinese", "text", "spam", "detection", "task", ".", "A", "\u201c", "self-diversity", "\u201d", "criterion", "is", "proposed", "for", "measuring", "the", "\u201c", "worthiness", "\u201d", "of", "a", "candidate", "for", "annotation", ".", "A", "semi-supervised", "variational", "autoencoder", "with", "masked", "attention", "learning", "approach", "and", "a", "character", "variation", "graph-enhanced", "augmentation", "procedure", "are", "proposed", "for", "data", "augmentation", ".", "The", "preliminary", "experiment", "demonstrates", "the", "proposed", "SIGNAL", "model", "is", "not", "only", "sensitive", "to", "spam", "sample", "selection", ",", "but", "also", "can", "improve", "the", "performance", "of", "a", "series", "of", "conventional", "active", "learning", "models", "for", "Chinese", "spam", "detection", "task", ".", "To", "the", "best", "of", "our", "knowledge", ",", "this", "is", "the", "first", "work", "to", "integrate", "active", "learning", "and", "semi-supervised", "generative", "learning", "for", "text", "spam", "detection", "."], "entities": [{"type": "Operation", "start": 76, "end": 78, "text": "SIGNAL model", "sent_idx": 3}, {"type": "Effect", "start": 92, "end": 93, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--279"}
{"text": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.", "tokens": ["Traditional", "named", "entity", "recognition", "models", "use", "gazetteers", "(", "lists", "of", "entities", ")", "as", "features", "to", "improve", "performance", ".", "Although", "modern", "neural", "network", "models", "do", "not", "require", "such", "hand-crafted", "features", "for", "strong", "performance", ",", "recent", "work", "has", "demonstrated", "their", "utility", "for", "named", "entity", "recognition", "on", "English", "data", ".", "However", ",", "designing", "such", "features", "for", "low-resource", "languages", "is", "challenging", ",", "because", "exhaustive", "entity", "gazetteers", "do", "not", "exist", "in", "these", "languages", ".", "To", "address", "this", "problem", ",", "we", "propose", "a", "method", "of", "\u201c", "soft", "gazetteers", "\u201d", "that", "incorporates", "ubiquitously", "available", "information", "from", "English", "knowledge", "bases", ",", "such", "as", "Wikipedia", ",", "into", "neural", "named", "entity", "recognition", "models", "through", "cross-lingual", "entity", "linking", ".", "Our", "experiments", "on", "four", "low-resource", "languages", "show", "an", "average", "improvement", "of", "4", "points", "in", "F1", "score", "."], "entities": [{"type": "Operation", "start": 84, "end": 92, "text": "incorporates ubiquitously available information from English knowledge bases", "sent_idx": 3}, {"type": "Effect", "start": 122, "end": 124, "text": "F1 score", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--722"}
{"text": "This paper describes a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing. Bottom-up and top-down parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.", "tokens": ["This", "paper", "describes", "a", "novel", "technique", "for", "incorporating", "syntactic", "knowledge", "into", "phrase-based", "machine", "translation", "through", "incremental", "syntactic", "parsing", ".", "Bottom-up", "and", "top-down", "parsers", "typically", "require", "a", "completed", "string", "as", "input", ".", "This", "requirement", "makes", "it", "difficult", "to", "incorporate", "them", "into", "phrase-based", "translation", ",", "which", "generates", "partial", "hypothesized", "translations", "from", "left-to-right", ".", "Incremental", "syntactic", "language", "models", "score", "sentences", "in", "a", "similar", "left-to-right", "fashion", ",", "and", "are", "therefore", "a", "good", "mechanism", "for", "incorporating", "syntax", "into", "phrase-based", "translation", ".", "We", "give", "a", "formal", "definition", "of", "one", "such", "lineartime", "syntactic", "language", "model", ",", "detail", "its", "relation", "to", "phrase-based", "decoding", ",", "and", "integrate", "the", "model", "with", "the", "Moses", "phrase-based", "translation", "system", ".", "We", "present", "empirical", "results", "on", "a", "constrained", "Urdu-English", "translation", "task", "that", "demonstrate", "a", "significant", "BLEU", "score", "improvement", "and", "a", "large", "decrease", "in", "perplexity", "."], "entities": [{"type": "Operation", "start": 51, "end": 55, "text": "Incremental syntactic language models", "sent_idx": 3}, {"type": "Effect", "start": 121, "end": 123, "text": "BLEU score", "sent_idx": 5}, {"type": "Effect", "start": 129, "end": 130, "text": "perplexity", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "P11-1063"}
{"text": "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.", "tokens": ["Reliably", "evaluating", "Machine", "Translation", "(", "MT", ")", "through", "automated", "metrics", "is", "a", "long-standing", "problem", ".", "One", "of", "the", "main", "challenges", "is", "the", "fact", "that", "multiple", "outputs", "can", "be", "equally", "valid", ".", "Attempts", "to", "minimise", "this", "issue", "include", "metrics", "that", "relax", "the", "matching", "of", "MT", "output", "and", "reference", "strings", ",", "and", "the", "use", "of", "multiple", "references", ".", "The", "latter", "has", "been", "shown", "to", "significantly", "improve", "the", "performance", "of", "evaluation", "metrics", ".", "However", ",", "collecting", "multiple", "references", "is", "expensive", "and", "in", "practice", "a", "single", "reference", "is", "generally", "used", ".", "In", "this", "paper", ",", "we", "propose", "an", "alternative", "approach", ":", "instead", "of", "modelling", "linguistic", "variation", "in", "human", "reference", "we", "exploit", "the", "MT", "model", "uncertainty", "to", "generate", "multiple", "diverse", "translations", "and", "use", "these", ":", "(", "i", ")", "as", "surrogates", "to", "reference", "translations", ";", "(", "ii", ")", "to", "obtain", "a", "quantification", "of", "translation", "variability", "to", "either", "complement", "existing", "metric", "scores", "or", "(", "iii", ")", "replace", "references", "altogether", ".", "We", "show", "that", "for", "a", "number", "of", "popular", "evaluation", "metrics", "our", "variability", "estimates", "lead", "to", "substantial", "improvements", "in", "correlation", "with", "human", "judgements", "of", "quality", "by", "up", "15", "%", "."], "entities": [{"type": "Operation", "start": 163, "end": 166, "text": "our variability estimates", "sent_idx": 6}, {"type": "Effect", "start": 171, "end": 177, "text": "correlation with human judgements of quality", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--113"}
{"text": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.", "tokens": ["N-gram", "language", "models", "are", "a", "major", "resource", "bottleneck", "in", "machine", "translation", ".", "In", "this", "paper", ",", "we", "present", "several", "language", "model", "implementations", "that", "are", "both", "highly", "compact", "and", "fast", "to", "query", ".", "Our", "fastest", "implementation", "is", "as", "fast", "as", "the", "widely", "used", "SRILM", "while", "requiring", "only", "25", "%", "of", "the", "storage", ".", "Our", "most", "compact", "representation", "can", "store", "all", "4", "billion", "n-grams", "and", "associated", "counts", "for", "the", "Google", "n-gram", "corpus", "in", "23", "bits", "per", "n-gram", ",", "the", "most", "compact", "lossless", "representation", "to", "date", ",", "and", "even", "more", "compact", "than", "recent", "lossy", "compression", "techniques", ".", "We", "also", "discuss", "techniques", "for", "improving", "query", "speed", "during", "decoding", ",", "including", "a", "simple", "but", "novel", "language", "model", "caching", "technique", "that", "improves", "the", "query", "speed", "of", "our", "language", "models", "(", "and", "SRILM", ")", "by", "up", "to", "300", "%", "."], "entities": [{"type": "Operation", "start": 110, "end": 114, "text": "language model caching technique", "sent_idx": 4}, {"type": "Effect", "start": 117, "end": 119, "text": "query speed", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1027"}
{"text": "Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow\u2019s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.", "tokens": ["Human", "conversations", "naturally", "evolve", "around", "related", "concepts", "and", "hop", "to", "distant", "concepts", ".", "This", "paper", "presents", "a", "new", "conversation", "generation", "model", ",", "ConceptFlow", ",", "which", "leverages", "commonsense", "knowledge", "graphs", "to", "explicitly", "model", "conversation", "flows", ".", "By", "grounding", "conversations", "to", "the", "concept", "space", ",", "ConceptFlow", "represents", "the", "potential", "conversation", "flow", "as", "traverses", "in", "the", "concept", "space", "along", "commonsense", "relations", ".", "The", "traverse", "is", "guided", "by", "graph", "attentions", "in", "the", "concept", "graph", ",", "moving", "towards", "more", "meaningful", "directions", "in", "the", "concept", "space", ",", "in", "order", "to", "generate", "more", "semantic", "and", "informative", "responses", ".", "Experiments", "on", "Reddit", "conversations", "demonstrate", "ConceptFlow", "\u2019s", "effectiveness", "over", "previous", "knowledge-aware", "conversation", "models", "and", "GPT-2", "based", "models", "while", "using", "70", "%", "fewer", "parameters", ",", "confirming", "the", "advantage", "of", "explicit", "modeling", "conversation", "structures", ".", "All", "source", "codes", "of", "this", "work", "are", "available", "at", "https://github.com/thunlp/ConceptFlow", "."], "entities": [{"type": "Operation", "start": 96, "end": 97, "text": "ConceptFlow", "sent_idx": 4}, {"type": "Effect", "start": 113, "end": 114, "text": "parameters", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--184"}
{"text": "We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.", "tokens": ["We", "propose", "a", "simple", "data", "augmentation", "protocol", "aimed", "at", "providing", "a", "compositional", "inductive", "bias", "in", "conditional", "and", "unconditional", "sequence", "models", ".", "Under", "this", "protocol", ",", "synthetic", "training", "examples", "are", "constructed", "by", "taking", "real", "training", "examples", "and", "replacing", "(", "possibly", "discontinuous", ")", "fragments", "with", "other", "fragments", "that", "appear", "in", "at", "least", "one", "similar", "environment", ".", "The", "protocol", "is", "model-agnostic", "and", "useful", "for", "a", "variety", "of", "tasks", ".", "Applied", "to", "neural", "sequence-to-sequence", "models", ",", "it", "reduces", "error", "rate", "by", "as", "much", "as", "87", "%", "on", "diagnostic", "tasks", "from", "the", "SCAN", "dataset", "and", "16", "%", "on", "a", "semantic", "parsing", "task", ".", "Applied", "to", "n-gram", "language", "models", ",", "it", "reduces", "perplexity", "by", "roughly", "1", "%", "on", "small", "corpora", "in", "several", "languages", "."], "entities": [{"type": "Operation", "start": 4, "end": 7, "text": "data augmentation protocol", "sent_idx": 0}, {"type": "Effect", "start": 74, "end": 76, "text": "error rate", "sent_idx": 3}, {"type": "Effect", "start": 106, "end": 107, "text": "perplexity", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--676"}
{"text": "We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.", "tokens": ["We", "present", "a", "novel", "approach", "to", "integrate", "transliteration", "into", "Hindi-to-Urdu", "statistical", "machine", "translation", ".", "We", "propose", "two", "probabilistic", "models", ",", "based", "on", "conditional", "and", "joint", "probability", "formulations", ",", "that", "are", "novel", "solutions", "to", "the", "problem", ".", "Our", "models", "consider", "both", "transliteration", "and", "translation", "when", "translating", "a", "particular", "Hindi", "word", "given", "the", "context", "whereas", "in", "previous", "work", "transliteration", "is", "only", "used", "for", "translating", "OOV", "(", "out-of-vocabulary", ")", "words", ".", "We", "use", "transliteration", "as", "a", "tool", "for", "disambiguation", "of", "Hindi", "homonyms", "which", "can", "be", "both", "translated", "or", "transliterated", "or", "transliterated", "differently", "based", "on", "different", "contexts", ".", "We", "obtain", "final", "BLEU", "scores", "of", "19.35", "(", "conditional", "probability", "model", ")", "and", "19.00", "(", "joint", "probability", "model", ")", "as", "compared", "to", "14.30", "for", "a", "baseline", "phrase-based", "system", "and", "16.25", "for", "a", "system", "which", "transliterates", "OOV", "words", "in", "the", "baseline", "system", ".", "This", "indicates", "that", "transliteration", "is", "useful", "for", "more", "than", "only", "translating", "OOV", "words", "for", "language", "pairs", "like", "Hindi-Urdu", "."], "entities": [{"type": "Operation", "start": 69, "end": 79, "text": "use transliteration as a tool for disambiguation of Hindi homonyms", "sent_idx": 3}, {"type": "Effect", "start": 97, "end": 99, "text": "BLEU scores", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1048"}
{"text": "Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world\u2019s languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.", "tokens": ["Pretrained", "multilingual", "models", "(", "PMMs", ")", "enable", "zero-shot", "learning", "via", "cross-lingual", "transfer", ",", "performing", "best", "for", "languages", "seen", "during", "pretraining", ".", "While", "methods", "exist", "to", "improve", "performance", "for", "unseen", "languages", ",", "they", "have", "almost", "exclusively", "been", "evaluated", "using", "amounts", "of", "raw", "text", "only", "available", "for", "a", "small", "fraction", "of", "the", "world", "\u2019s", "languages", ".", "In", "this", "paper", ",", "we", "evaluate", "the", "performance", "of", "existing", "methods", "to", "adapt", "PMMs", "to", "new", "languages", "using", "a", "resource", "available", "for", "close", "to", "1600", "languages", ":", "the", "New", "Testament", ".", "This", "is", "challenging", "for", "two", "reasons", ":", "(", "1", ")", "the", "small", "corpus", "size", ",", "and", "(", "2", ")", "the", "narrow", "domain", ".", "While", "performance", "drops", "for", "all", "approaches", ",", "we", "surprisingly", "still", "see", "gains", "of", "up", "to", "17.69", "%", "accuracy", "for", "part-of-speech", "tagging", "and", "6.29", "F1", "for", "NER", "on", "average", "over", "all", "languages", "as", "compared", "to", "XLM-R.", "Another", "unexpected", "finding", "is", "that", "continued", "pretraining", ",", "the", "simplest", "approach", ",", "performs", "best", ".", "Finally", ",", "we", "perform", "a", "case", "study", "to", "disentangle", "the", "effects", "of", "domain", "and", "size", "and", "to", "shed", "light", "on", "the", "influence", "of", "the", "finetuning", "source", "language", "."], "entities": [{"type": "Operation", "start": 0, "end": 6, "text": "Pretrained multilingual models (PMMs)", "sent_idx": 0}, {"type": "Effect", "start": 125, "end": 126, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 132, "text": "F1", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--351"}
{"text": "While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.", "tokens": ["While", "state-of-the-art", "NLP", "models", "have", "been", "achieving", "the", "excellent", "performance", "of", "a", "wide", "range", "of", "tasks", "in", "recent", "years", ",", "important", "questions", "are", "being", "raised", "about", "their", "robustness", "and", "their", "underlying", "sensitivity", "to", "systematic", "biases", "that", "may", "exist", "in", "their", "training", "and", "test", "data", ".", "Such", "issues", "come", "to", "be", "manifest", "in", "performance", "problems", "when", "faced", "with", "out-of-distribution", "data", "in", "the", "field", ".", "One", "recent", "solution", "has", "been", "to", "use", "counterfactually", "augmented", "datasets", "in", "order", "to", "reduce", "any", "reliance", "on", "spurious", "patterns", "that", "may", "exist", "in", "the", "original", "data", ".", "Producing", "high-quality", "augmented", "data", "can", "be", "costly", "and", "time-consuming", "as", "it", "usually", "needs", "to", "involve", "human", "feedback", "and", "crowdsourcing", "efforts", ".", "In", "this", "work", ",", "we", "propose", "an", "alternative", "by", "describing", "and", "evaluating", "an", "approach", "to", "automatically", "generating", "counterfactual", "data", "for", "the", "purpose", "of", "data", "augmentation", "and", "explanation", ".", "A", "comprehensive", "evaluation", "on", "several", "different", "datasets", "and", "using", "a", "variety", "of", "state-of-the-art", "benchmarks", "demonstrate", "how", "our", "approach", "can", "achieve", "significant", "improvements", "in", "model", "performance", "when", "compared", "to", "models", "training", "on", "the", "original", "data", "and", "even", "when", "compared", "to", "models", "trained", "with", "the", "benefit", "of", "human-generated", "augmented", "data", "."], "entities": [{"type": "Operation", "start": 120, "end": 130, "text": "describing and evaluating an approach to automatically generating counterfactual data", "sent_idx": 4}, {"type": "Effect", "start": 163, "end": 164, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--26"}
{"text": "This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures. Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned. Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average). The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP.", "tokens": ["This", "paper", "presents", "a", "novel", "pre-trained", "language", "models", "(", "PLM", ")", "compression", "approach", "based", "on", "the", "matrix", "product", "operator", "(", "short", "as", "MPO", ")", "from", "quantum", "many-body", "physics", ".", "It", "can", "decompose", "an", "original", "matrix", "into", "central", "tensors", "(", "containing", "the", "core", "information", ")", "and", "auxiliary", "tensors", "(", "with", "only", "a", "small", "proportion", "of", "parameters", ")", ".", "With", "the", "decomposed", "MPO", "structure", ",", "we", "propose", "a", "novel", "fine-tuning", "strategy", "by", "only", "updating", "the", "parameters", "from", "the", "auxiliary", "tensors", ",", "and", "design", "an", "optimization", "algorithm", "for", "MPO-based", "approximation", "over", "stacked", "network", "architectures", ".", "Our", "approach", "can", "be", "applied", "to", "the", "original", "or", "the", "compressed", "PLMs", "in", "a", "general", "way", ",", "which", "derives", "a", "lighter", "network", "and", "significantly", "reduces", "the", "parameters", "to", "be", "fine-tuned", ".", "Extensive", "experiments", "have", "demonstrated", "the", "effectiveness", "of", "the", "proposed", "approach", "in", "model", "compression", ",", "especially", "the", "reduction", "in", "fine-tuning", "parameters", "(", "91", "%", "reduction", "on", "average", ")", ".", "The", "code", "to", "reproduce", "the", "results", "of", "this", "paper", "can", "be", "found", "at", "https://github.com/RUCAIBox/MPOP", "."], "entities": [{"type": "Operation", "start": 16, "end": 24, "text": "matrix product operator (short as MPO)", "sent_idx": 0}, {"type": "Effect", "start": 141, "end": 143, "text": "fine-tuning parameters", "sent_idx": 4}, {"type": "Operation", "start": 5, "end": 13, "text": "pre-trained language models (PLM) compression approach", "sent_idx": 0}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--418"}
{"text": "To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.", "tokens": ["To", "date", ",", "few", "attempts", "have", "been", "made", "to", "develop", "and", "validate", "methods", "for", "automatic", "evaluation", "of", "linguistic", "quality", "in", "text", "summarization", ".", "We", "present", "the", "first", "systematic", "assessment", "of", "several", "diverse", "classes", "of", "metrics", "designed", "to", "capture", "various", "aspects", "of", "well-written", "text", ".", "We", "train", "and", "test", "linguistic", "quality", "models", "on", "consecutive", "years", "of", "NIST", "evaluation", "data", "in", "order", "to", "show", "the", "generality", "of", "results", ".", "For", "grammaticality", ",", "the", "best", "results", "come", "from", "a", "set", "of", "syntactic", "features", ".", "Focus", ",", "coherence", "and", "referential", "clarity", "are", "best", "evaluated", "by", "a", "class", "of", "features", "measuring", "local", "coherence", "on", "the", "basis", "of", "cosine", "similarity", "between", "sentences", ",", "coreference", "information", ",", "and", "summarization", "specific", "features", ".", "Our", "best", "results", "are", "90", "%", "accuracy", "for", "pairwise", "comparisons", "of", "competing", "systems", "over", "a", "test", "set", "of", "several", "inputs", "and", "70", "%", "for", "ranking", "summaries", "of", "a", "specific", "input", "."], "entities": [{"type": "Operation", "start": 75, "end": 80, "text": "a set of syntactic features", "sent_idx": 3}, {"type": "Effect", "start": 121, "end": 122, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1056"}
{"text": "We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser's search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.", "tokens": ["We", "present", "a", "systematic", "comparison", "and", "combination", "of", "two", "orthogonal", "techniques", "for", "efficient", "parsing", "of", "Combinatory", "Categorial", "Grammar", "(", "CCG", ")", ".", "First", "we", "consider", "adaptive", "supertagging", ",", "a", "widely", "used", "approximate", "search", "technique", "that", "prunes", "most", "lexical", "categories", "from", "the", "parser", "'s", "search", "space", "using", "a", "separate", "sequence", "model", ".", "Next", "we", "consider", "several", "variants", "on", "A", "*", ",", "a", "classic", "exact", "search", "technique", "which", "to", "our", "knowledge", "has", "not", "been", "applied", "to", "more", "expressive", "grammar", "formalisms", "like", "CCG", ".", "In", "addition", "to", "standard", "hardware-independent", "measures", "of", "parser", "effort", "we", "also", "present", "what", "we", "believe", "is", "the", "first", "evaluation", "of", "A", "*", "parsing", "on", "the", "more", "realistic", "but", "more", "stringent", "metric", "of", "CPU", "time", ".", "By", "itself", ",", "A", "*", "substantially", "reduces", "parser", "effort", "as", "measured", "by", "the", "number", "of", "edges", "considered", "during", "parsing", ",", "but", "we", "show", "that", "for", "CCG", "this", "does", "not", "always", "correspond", "to", "improvements", "in", "CPU", "time", "over", "a", "CKY", "baseline", ".", "Combining", "A", "*", "with", "adaptive", "supertagging", "decreases", "CPU", "time", "by", "15", "%", "for", "our", "best", "model", "."], "entities": [{"type": "Operation", "start": 157, "end": 163, "text": "Combining A* with adaptive supertagging", "sent_idx": 5}, {"type": "Effect", "start": 164, "end": 166, "text": "CPU time", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P11-1158"}
{"text": "User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.", "tokens": ["User", "interest", "modeling", "is", "critical", "for", "personalized", "news", "recommendation", ".", "Existing", "news", "recommendation", "methods", "usually", "learn", "a", "single", "user", "embedding", "for", "each", "user", "from", "their", "previous", "behaviors", "to", "represent", "their", "overall", "interest", ".", "However", ",", "user", "interest", "is", "usually", "diverse", "and", "multi-grained", ",", "which", "is", "difficult", "to", "be", "accurately", "modeled", "by", "a", "single", "user", "embedding", ".", "In", "this", "paper", ",", "we", "propose", "a", "news", "recommendation", "method", "with", "hierarchical", "user", "interest", "modeling", ",", "named", "HieRec", ".", "Instead", "of", "a", "single", "user", "embedding", ",", "in", "our", "method", "each", "user", "is", "represented", "in", "a", "hierarchical", "interest", "tree", "to", "better", "capture", "their", "diverse", "and", "multi-grained", "interest", "in", "news", ".", "We", "use", "a", "three-level", "hierarchy", "to", "represent", "1", ")", "overall", "user", "interest", ";", "2", ")", "user", "interest", "in", "coarse-grained", "topics", "like", "sports", ";", "and", "3", ")", "user", "interest", "in", "fine-grained", "topics", "like", "football", ".", "Moreover", ",", "we", "propose", "a", "hierarchical", "user", "interest", "matching", "framework", "to", "match", "candidate", "news", "with", "different", "levels", "of", "user", "interest", "for", "more", "accurate", "user", "interest", "targeting", ".", "Extensive", "experiments", "on", "two", "real-world", "datasets", "validate", "our", "method", "can", "effectively", "improve", "the", "performance", "of", "user", "modeling", "for", "personalized", "news", "recommendation", "."], "entities": [{"type": "Operation", "start": 144, "end": 149, "text": "hierarchical user interest matching framework", "sent_idx": 6}, {"type": "Effect", "start": 179, "end": 180, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--423"}
{"text": "The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.", "tokens": ["The", "key", "to", "effortless", "end-user", "programming", "is", "natural", "language", ".", "We", "examine", "how", "to", "teach", "intelligent", "systems", "new", "functions", ",", "expressed", "in", "natural", "language", ".", "As", "a", "first", "step", ",", "we", "collected", "3168", "samples", "of", "teaching", "efforts", "in", "plain", "English", ".", "Then", "we", "built", "fuSE", ",", "a", "novel", "system", "that", "translates", "English", "function", "descriptions", "into", "code", ".", "Our", "approach", "is", "three-tiered", "and", "each", "task", "is", "evaluated", "separately", ".", "We", "first", "classify", "whether", "an", "intent", "to", "teach", "new", "functionality", "is", "present", "in", "the", "utterance", "(", "accuracy", ":", "97.7", "%", "using", "BERT", ")", ".", "Then", "we", "analyze", "the", "linguistic", "structure", "and", "construct", "a", "semantic", "model", "(", "accuracy", ":", "97.6", "%", "using", "a", "BiLSTM", ")", ".", "Finally", ",", "we", "synthesize", "the", "signature", "of", "the", "method", ",", "map", "the", "intermediate", "steps", "(", "instructions", "in", "the", "method", "body", ")", "to", "API", "calls", "and", "inject", "control", "structures", "(", "F1", ":", "67.0", "%", "with", "information", "retrieval", "and", "knowledge-based", "methods", ")", ".", "In", "an", "end-to-end", "evaluation", "on", "an", "unseen", "dataset", "fuSE", "synthesized", "84.6", "%", "of", "the", "method", "signatures", "and", "79.2", "%", "of", "the", "API", "calls", "correctly", "."], "entities": [{"type": "Operation", "start": 108, "end": 111, "text": "using a BiLSTM", "sent_idx": 6}, {"type": "Effect", "start": 104, "end": 105, "text": "accuracy", "sent_idx": 6}, {"type": "Operation", "start": 88, "end": 90, "text": "using BERT", "sent_idx": 5}, {"type": "Effect", "start": 84, "end": 85, "text": "accuracy", "sent_idx": 5}, {"type": "Operation", "start": 147, "end": 152, "text": "information retrieval and knowledge-based methods", "sent_idx": 7}, {"type": "Effect", "start": 142, "end": 143, "text": "F1", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}, {"type": "Affect", "head": 4, "tail": 5}], "id": "abstract-2020--acl-main--395"}
{"text": "Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks. To our best knowledge, it is the first single model that outperforms the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.", "tokens": ["Reading", "long", "documents", "to", "answer", "open-domain", "questions", "remains", "challenging", "in", "natural", "language", "understanding", ".", "In", "this", "paper", ",", "we", "introduce", "a", "new", "model", ",", "called", "RikiNet", ",", "which", "reads", "Wikipedia", "pages", "for", "natural", "question", "answering", ".", "RikiNet", "contains", "a", "dynamic", "paragraph", "dual-attention", "reader", "and", "a", "multi-level", "cascaded", "answer", "predictor", ".", "The", "reader", "dynamically", "represents", "the", "document", "and", "question", "by", "utilizing", "a", "set", "of", "complementary", "attention", "mechanisms", ".", "The", "representations", "are", "then", "fed", "into", "the", "predictor", "to", "obtain", "the", "span", "of", "the", "short", "answer", ",", "the", "paragraph", "of", "the", "long", "answer", ",", "and", "the", "answer", "type", "in", "a", "cascaded", "manner", ".", "On", "the", "Natural", "Questions", "(", "NQ", ")", "dataset", ",", "a", "single", "RikiNet", "achieves", "74.3", "F1", "and", "57.9", "F1", "on", "long-answer", "and", "short-answer", "tasks", ".", "To", "our", "best", "knowledge", ",", "it", "is", "the", "first", "single", "model", "that", "outperforms", "the", "single", "human", "performance", ".", "Furthermore", ",", "an", "ensemble", "RikiNet", "obtains", "76.1", "F1", "and", "61.3", "F1", "on", "long-answer", "and", "short-answer", "tasks", ",", "achieving", "the", "best", "performance", "on", "the", "official", "NQ", "leaderboard", "."], "entities": [{"type": "Operation", "start": 111, "end": 112, "text": "RikiNet", "sent_idx": 5}, {"type": "Effect", "start": 114, "end": 115, "text": "F1", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--604"}
{"text": "Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate \u201cdistractor\u201d sentences to augment the bags and train the model to ignore the distractors. Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.", "tokens": ["Recent", "neural", "models", "for", "relation", "extraction", "with", "distant", "supervision", "alleviate", "the", "impact", "of", "irrelevant", "sentences", "in", "a", "bag", "by", "learning", "importance", "weights", "for", "the", "sentences", ".", "Efforts", "thus", "far", "have", "focused", "on", "improving", "extraction", "accuracy", "but", "little", "is", "known", "about", "their", "explanability", ".", "In", "this", "work", "we", "annotate", "a", "test", "set", "with", "ground-truth", "sentence-level", "explanations", "to", "evaluate", "the", "quality", "of", "explanations", "afforded", "by", "the", "relation", "extraction", "models", ".", "We", "demonstrate", "that", "replacing", "the", "entity", "mentions", "in", "the", "sentences", "with", "their", "fine-grained", "entity", "types", "not", "only", "enhances", "extraction", "accuracy", "but", "also", "improves", "explanation", ".", "We", "also", "propose", "to", "automatically", "generate", "\u201c", "distractor", "\u201d", "sentences", "to", "augment", "the", "bags", "and", "train", "the", "model", "to", "ignore", "the", "distractors", ".", "Evaluations", "on", "the", "widely", "used", "FB-NYT", "dataset", "show", "that", "our", "methods", "achieve", "new", "state-of-the-art", "accuracy", "while", "improving", "model", "explanability", "."], "entities": [{"type": "Operation", "start": 97, "end": 107, "text": "automatically generate \u201cdistractor\u201d sentences to augment the bags", "sent_idx": 4}, {"type": "Effect", "start": 130, "end": 131, "text": "accuracy", "sent_idx": 5}, {"type": "Effect", "start": 134, "end": 135, "text": "explanability", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--579"}
{"text": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.", "tokens": ["Increasing", "the", "input", "length", "has", "been", "a", "driver", "of", "progress", "in", "language", "modeling", "with", "transformers", ".", "We", "identify", "conditions", "where", "shorter", "inputs", "are", "not", "harmful", ",", "and", "achieve", "perplexity", "and", "efficiency", "improvements", "through", "two", "new", "methods", "that", "decrease", "input", "length", ".", "First", ",", "we", "show", "that", "initially", "training", "a", "model", "on", "short", "subsequences", "before", "moving", "on", "to", "longer", "ones", "both", "reduces", "overall", "training", "time", "and", ",", "surprisingly", ",", "substantially", "improves", "perplexity", ".", "Second", ",", "we", "show", "how", "to", "improve", "the", "efficiency", "of", "recurrence", "methods", "in", "transformers", ",", "which", "let", "models", "condition", "on", "previously", "processed", "tokens", "when", "generating", "sequences", "that", "exceed", "the", "maximal", "length", "the", "transformer", "can", "handle", "at", "once", ".", "Existing", "methods", "require", "computationally", "expensive", "relative", "position", "embeddings", ";", "we", "introduce", "a", "simple", "alternative", "of", "adding", "absolute", "position", "embeddings", "to", "queries", "and", "keys", "instead", "of", "to", "word", "embeddings", ",", "which", "efficiently", "produces", "superior", "results", ".", "We", "show", "that", "these", "recurrent", "models", "also", "benefit", "from", "short", "input", "lengths", ".", "Combining", "these", "techniques", "speeds", "up", "training", "by", "a", "factor", "of", "1.65", ",", "reduces", "memory", "usage", ",", "and", "substantially", "improves", "perplexity", "on", "WikiText-103", ",", "without", "adding", "any", "parameters", "."], "entities": [{"type": "Operation", "start": 158, "end": 161, "text": "Combining these techniques", "sent_idx": 6}, {"type": "Effect", "start": 171, "end": 173, "text": "memory usage", "sent_idx": 6}, {"type": "Effect", "start": 177, "end": 178, "text": "perplexity", "sent_idx": 6}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--427"}
{"text": "We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%.", "tokens": ["We", "propose", "an", "automatic", "method", "of", "extracting", "paraphrases", "from", "definition", "sentences", ",", "which", "are", "also", "automatically", "acquired", "from", "the", "Web", ".", "We", "observe", "that", "a", "huge", "number", "of", "concepts", "are", "defined", "in", "Web", "documents", ",", "and", "that", "the", "sentences", "that", "define", "the", "same", "concept", "tend", "to", "convey", "mostly", "the", "same", "information", "using", "different", "expressions", "and", "thus", "contain", "many", "paraphrases", ".", "We", "show", "that", "a", "large", "number", "of", "paraphrases", "can", "be", "automatically", "extracted", "with", "high", "precision", "by", "regarding", "the", "sentences", "that", "define", "the", "same", "concept", "as", "parallel", "corpora", ".", "Experimental", "results", "indicated", "that", "with", "our", "method", "it", "was", "possible", "to", "extract", "about", "300,000", "paraphrases", "from", "6", "x", "108", "Web", "documents", "with", "a", "precision", "rate", "of", "about", "94", "%", "."], "entities": [{"type": "Operation", "start": 76, "end": 87, "text": "regarding the sentences that define the same concept as parallel corpora", "sent_idx": 2}, {"type": "Effect", "start": 74, "end": 75, "text": "precision", "sent_idx": 2}, {"type": "Effect", "start": 111, "end": 113, "text": "precision rate", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "P11-1109"}
{"text": "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code.", "tokens": ["Recent", "dialogue", "coherence", "models", "use", "the", "coherence", "features", "designed", "for", "monologue", "texts", ",", "e.g.", "nominal", "entities", ",", "to", "represent", "utterances", "and", "then", "explicitly", "augment", "them", "with", "dialogue-relevant", "features", ",", "e.g.", ",", "dialogue", "act", "labels", ".", "It", "indicates", "two", "drawbacks", ",", "(", "a", ")", "semantics", "of", "utterances", "are", "limited", "to", "entity", "mentions", ",", "and", "(", "b", ")", "the", "performance", "of", "coherence", "models", "strongly", "relies", "on", "the", "quality", "of", "the", "input", "dialogue", "act", "labels", ".", "We", "address", "these", "issues", "by", "introducing", "a", "novel", "approach", "to", "dialogue", "coherence", "assessment", ".", "We", "use", "dialogue", "act", "prediction", "as", "an", "auxiliary", "task", "in", "a", "multi-task", "learning", "scenario", "to", "obtain", "informative", "utterance", "representations", "for", "coherence", "assessment", ".", "Our", "approach", "alleviates", "the", "need", "for", "explicit", "dialogue", "act", "labels", "during", "evaluation", ".", "The", "results", "of", "our", "experiments", "show", "that", "our", "model", "substantially", "(", "more", "than", "20", "accuracy", "points", ")", "outperforms", "its", "strong", "competitors", "on", "the", "DailyDialogue", "corpus", ",", "and", "performs", "on", "par", "with", "them", "on", "the", "SwitchBoard", "corpus", "for", "ranking", "dialogues", "concerning", "their", "coherence", ".", "We", "release", "our", "source", "code", "."], "entities": [{"type": "Operation", "start": 78, "end": 86, "text": "introducing a novel approach to dialogue coherence assessment", "sent_idx": 2}, {"type": "Effect", "start": 137, "end": 139, "text": "accuracy points", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--133"}
{"text": "Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.", "tokens": ["Open", "Domain", "dialog", "system", "evaluation", "is", "one", "of", "the", "most", "important", "challenges", "in", "dialog", "research", ".", "Existing", "automatic", "evaluation", "metrics", ",", "such", "as", "BLEU", "are", "mostly", "reference-based", ".", "They", "calculate", "the", "difference", "between", "the", "generated", "response", "and", "a", "limited", "number", "of", "available", "references", ".", "Likert-score", "based", "self-reported", "user", "rating", "is", "widely", "adopted", "by", "social", "conversational", "systems", ",", "such", "as", "Amazon", "Alexa", "Prize", "chatbots", ".", "However", ",", "self-reported", "user", "rating", "suffers", "from", "bias", "and", "variance", "among", "different", "users", ".", "To", "alleviate", "this", "problem", ",", "we", "formulate", "dialog", "evaluation", "as", "a", "comparison", "task", ".", "We", "also", "propose", "an", "automatic", "evaluation", "model", "CMADE", "(", "Comparison", "Model", "for", "Automatic", "Dialog", "Evaluation", ")", "that", "automatically", "cleans", "self-reported", "user", "ratings", "as", "it", "trains", "on", "them", ".", "Specifically", ",", "we", "first", "use", "a", "self-supervised", "method", "to", "learn", "better", "dialog", "feature", "representation", ",", "and", "then", "use", "KNN", "and", "Shapley", "to", "remove", "confusing", "samples", ".", "Our", "experiments", "show", "that", "CMADE", "achieves", "89.2", "%", "accuracy", "in", "the", "dialog", "comparison", "task", "."], "entities": [{"type": "Operation", "start": 150, "end": 151, "text": "CMADE", "sent_idx": 8}, {"type": "Effect", "start": 154, "end": 155, "text": "accuracy", "sent_idx": 8}, {"type": "Operation", "start": 96, "end": 108, "text": "automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation)", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--126"}
{"text": "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO .", "tokens": ["Existed", "pre-training", "methods", "either", "focus", "on", "single-modal", "tasks", "or", "multi-modal", "tasks", ",", "and", "can", "not", "effectively", "adapt", "to", "each", "other", ".", "They", "can", "only", "utilize", "single-modal", "data", "(", "i.e.", ",", "text", "or", "image", ")", "or", "limited", "multi-modal", "data", "(", "i.e.", ",", "image-text", "pairs", ")", ".", "In", "this", "work", ",", "we", "propose", "a", "UNIfied-MOdal", "pre-training", "architecture", ",", "namely", "UNIMO", ",", "which", "can", "effectively", "adapt", "to", "both", "single-modal", "and", "multi-modal", "understanding", "and", "generation", "tasks", ".", "Large", "scale", "of", "free", "text", "corpus", "and", "image", "collections", "are", "utilized", "to", "improve", "the", "capability", "of", "visual", "and", "textual", "understanding", ",", "and", "cross-modal", "contrastive", "learning", "(", "CMCL", ")", "is", "leveraged", "to", "align", "the", "textual", "and", "visual", "information", "into", "a", "unified", "semantic", "space", ",", "over", "a", "corpus", "of", "image-text", "pairs", "augmented", "with", "related", "images", "and", "texts", ".", "With", "the", "help", "of", "rich", "non-paired", "single-modal", "data", ",", "our", "model", "is", "able", "to", "learn", "more", "generalizable", "representations", ",", "by", "allowing", "textual", "knowledge", "and", "visual", "knowledge", "to", "enhance", "each", "other", "in", "the", "unified", "semantic", "space", ".", "The", "experimental", "results", "show", "that", "UNIMO", "greatly", "improves", "the", "performance", "of", "several", "single-modal", "and", "multi-modal", "downstream", "tasks", ".", "Our", "code", "and", "pre-trained", "models", "are", "public", "at", "https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO", "."], "entities": [{"type": "Operation", "start": 170, "end": 171, "text": "UNIMO", "sent_idx": 5}, {"type": "Effect", "start": 174, "end": 175, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--202"}
{"text": "Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method \u2014 multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R_L).", "tokens": ["Commonsense", "reasoning", "research", "has", "so", "far", "been", "limited", "to", "English", ".", "We", "aim", "to", "evaluate", "and", "improve", "popular", "multilingual", "language", "models", "(", "ML-LMs", ")", "to", "help", "advance", "commonsense", "reasoning", "(", "CSR", ")", "beyond", "English", ".", "We", "collect", "the", "Mickey", "corpus", ",", "consisting", "of", "561k", "sentences", "in", "11", "different", "languages", ",", "which", "can", "be", "used", "for", "analyzing", "and", "improving", "ML-LMs", ".", "We", "propose", "Mickey", "Probe", ",", "a", "language-general", "probing", "task", "for", "fairly", "evaluating", "the", "common", "sense", "of", "popular", "ML-LMs", "across", "different", "languages", ".", "In", "addition", ",", "we", "also", "create", "two", "new", "datasets", ",", "X-CSQA", "and", "X-CODAH", ",", "by", "translating", "their", "English", "versions", "to", "14", "other", "languages", ",", "so", "that", "we", "can", "evaluate", "popular", "ML-LMs", "for", "cross-lingual", "commonsense", "reasoning", ".", "To", "improve", "the", "performance", "beyond", "English", ",", "we", "propose", "a", "simple", "yet", "effective", "method", "\u2014", "multilingual", "contrastive", "pretraining", "(", "MCP", ")", ".", "It", "significantly", "enhances", "sentence", "representations", ",", "yielding", "a", "large", "performance", "gain", "on", "both", "benchmarks", "(", "e.g.", ",", "+", "2.7", "%", "accuracy", "for", "X-CSQA", "over", "XLM-R_L", ")", "."], "entities": [{"type": "Operation", "start": 133, "end": 139, "text": "multilingual contrastive pretraining (MCP)", "sent_idx": 5}, {"type": "Effect", "start": 149, "end": 150, "text": "performance", "sent_idx": 6}, {"type": "Effect", "start": 160, "end": 161, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--102"}
{"text": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.", "tokens": ["We", "present", "a", "discriminative", "model", "that", "directly", "predicts", "which", "set", "of", "phrasal", "translation", "rules", "should", "be", "extracted", "from", "a", "sentence", "pair", ".", "Our", "model", "scores", "extraction", "sets", ":", "nested", "collections", "of", "all", "the", "overlapping", "phrase", "pairs", "consistent", "with", "an", "underlying", "word", "alignment", ".", "Extraction", "set", "models", "provide", "two", "principle", "advantages", "over", "word-factored", "alignment", "models", ".", "First", ",", "we", "can", "incorporate", "features", "on", "phrase", "pairs", ",", "in", "addition", "to", "word", "links", ".", "Second", ",", "we", "can", "optimize", "for", "an", "extraction-based", "loss", "function", "that", "relates", "directly", "to", "the", "end", "task", "of", "generating", "translations", ".", "Our", "model", "gives", "improvements", "in", "alignment", "quality", "relative", "to", "state-of-the-art", "unsupervised", "and", "supervised", "baselines", ",", "as", "well", "as", "providing", "up", "to", "a", "1.4", "improvement", "in", "BLEU", "score", "in", "Chinese-to-English", "translation", "experiments", "."], "entities": [{"type": "Operation", "start": 92, "end": 94, "text": "Our model", "sent_idx": 5}, {"type": "Effect", "start": 117, "end": 119, "text": "BLEU score", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1147"}
{"text": "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.", "tokens": ["We", "study", "the", "problem", "of", "event", "coreference", "resolution", "(", "ECR", ")", "that", "seeks", "to", "group", "coreferent", "event", "mentions", "into", "the", "same", "clusters", ".", "Deep", "learning", "methods", "have", "recently", "been", "applied", "for", "this", "task", "to", "deliver", "state-of-the-art", "performance", ".", "However", ",", "existing", "deep", "learning", "models", "for", "ECR", "are", "limited", "in", "that", "they", "can", "not", "exploit", "important", "interactions", "between", "relevant", "objects", "for", "ECR", ",", "e.g.", ",", "context", "words", "and", "entity", "mentions", ",", "to", "support", "the", "encoding", "of", "document-level", "context", ".", "In", "addition", ",", "consistency", "constraints", "between", "golden", "and", "predicted", "clusters", "of", "event", "mentions", "have", "not", "been", "considered", "to", "improve", "representation", "learning", "in", "prior", "deep", "learning", "models", "for", "ECR", ".", "This", "work", "addresses", "such", "limitations", "by", "introducing", "a", "novel", "deep", "learning", "model", "for", "ECR", ".", "At", "the", "core", "of", "our", "model", "are", "document", "structures", "to", "explicitly", "capture", "relevant", "objects", "for", "ECR", ".", "Our", "document", "structures", "introduce", "diverse", "knowledge", "sources", "(", "discourse", ",", "syntax", ",", "semantics", ")", "to", "compute", "edges/interactions", "between", "structure", "nodes", "for", "document-level", "representation", "learning", ".", "We", "also", "present", "novel", "regularization", "techniques", "based", "on", "consistencies", "of", "golden", "and", "predicted", "clusters", "for", "event", "mentions", "in", "documents", ".", "Extensive", "experiments", "show", "that", "our", "model", "achieve", "state-of-the-art", "performance", "on", "two", "benchmark", "datasets", "."], "entities": [{"type": "Operation", "start": 168, "end": 170, "text": "regularization techniques", "sent_idx": 7}, {"type": "Effect", "start": 192, "end": 193, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--374"}
{"text": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).", "tokens": ["The", "goal-oriented", "dialogue", "system", "needs", "to", "be", "optimized", "for", "tracking", "the", "dialogue", "flow", "and", "carrying", "out", "an", "effective", "conversation", "under", "various", "situations", "to", "meet", "the", "user", "goal", ".", "The", "traditional", "approach", "to", "build", "such", "a", "dialogue", "system", "is", "to", "take", "a", "pipelined", "modular", "architecture", ",", "where", "its", "modules", "are", "optimized", "individually", ".", "However", ",", "such", "an", "optimization", "scheme", "does", "not", "necessarily", "yield", "the", "overall", "performance", "improvement", "of", "the", "whole", "system", ".", "On", "the", "other", "hand", ",", "end-to-end", "dialogue", "systems", "with", "monolithic", "neural", "architecture", "are", "often", "trained", "only", "with", "input-output", "utterances", ",", "without", "taking", "into", "account", "the", "entire", "annotations", "available", "in", "the", "corpus", ".", "This", "scheme", "makes", "it", "difficult", "for", "goal-oriented", "dialogues", "where", "the", "system", "needs", "to", "integrate", "with", "external", "systems", "or", "to", "provide", "interpretable", "information", "about", "why", "the", "system", "generated", "a", "particular", "response", ".", "In", "this", "paper", ",", "we", "present", "an", "end-to-end", "neural", "architecture", "for", "dialogue", "systems", "that", "addresses", "both", "challenges", "above", ".", "In", "the", "human", "evaluation", ",", "our", "dialogue", "system", "achieved", "the", "success", "rate", "of", "68.32", "%", ",", "the", "language", "understanding", "score", "of", "4.149", ",", "and", "the", "response", "appropriateness", "score", "of", "4.287", ",", "which", "ranked", "the", "system", "at", "the", "top", "position", "in", "the", "end-to-end", "multi-domain", "dialogue", "system", "task", "in", "the", "8th", "dialogue", "systems", "technology", "challenge", "(", "DSTC8", ")", "."], "entities": [{"type": "Operation", "start": 141, "end": 144, "text": "end-to-end neural architecture", "sent_idx": 5}, {"type": "Effect", "start": 163, "end": 165, "text": "success rate", "sent_idx": 6}, {"type": "Effect", "start": 170, "end": 173, "text": "language understanding score", "sent_idx": 6}, {"type": "Effect", "start": 178, "end": 181, "text": "response appropriateness score", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--54"}
{"text": "Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%--8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines.", "tokens": ["Most", "previous", "work", "on", "multilingual", "sentiment", "analysis", "has", "focused", "on", "methods", "to", "adapt", "sentiment", "resources", "from", "resource-rich", "languages", "to", "resource-poor", "languages", ".", "We", "present", "a", "novel", "approach", "for", "joint", "bilingual", "sentiment", "classification", "at", "the", "sentence", "level", "that", "augments", "available", "labeled", "data", "in", "each", "language", "with", "unlabeled", "parallel", "data", ".", "We", "rely", "on", "the", "intuition", "that", "the", "sentiment", "labels", "for", "parallel", "sentences", "should", "be", "similar", "and", "present", "a", "model", "that", "jointly", "learns", "improved", "monolingual", "sentiment", "classifiers", "for", "each", "language", ".", "Experiments", "on", "multiple", "data", "sets", "show", "that", "the", "proposed", "approach", "(", "1", ")", "outperforms", "the", "monolingual", "baselines", ",", "significantly", "improving", "the", "accuracy", "for", "both", "languages", "by", "3.44%--8.12", "%", ";", "(", "2", ")", "outperforms", "two", "standard", "approaches", "for", "leveraging", "unlabeled", "data", ";", "and", "(", "3", ")", "produces", "(", "albeit", "smaller", ")", "performance", "gains", "when", "employing", "pseudo-parallel", "data", "from", "machine", "translation", "engines", "."], "entities": [{"type": "Operation", "start": 37, "end": 48, "text": "augments available labeled data in each language with unlabeled parallel data", "sent_idx": 1}, {"type": "Effect", "start": 100, "end": 101, "text": "accuracy", "sent_idx": 3}, {"type": "Effect", "start": 129, "end": 130, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P11-1033"}
{"text": "Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.", "tokens": ["Statistical", "translation", "models", "that", "try", "to", "capture", "the", "recursive", "structure", "of", "language", "have", "been", "widely", "adopted", "over", "the", "last", "few", "years", ".", "These", "models", "make", "use", "of", "varying", "amounts", "of", "information", "from", "linguistic", "theory", ":", "some", "use", "none", "at", "all", ",", "some", "use", "information", "about", "the", "grammar", "of", "the", "target", "language", ",", "some", "use", "information", "about", "the", "grammar", "of", "the", "source", "language", ".", "But", "progress", "has", "been", "slower", "on", "translation", "models", "that", "are", "able", "to", "learn", "the", "relationship", "between", "the", "grammars", "of", "both", "the", "source", "and", "target", "language", ".", "We", "discuss", "the", "reasons", "why", "this", "has", "been", "a", "challenge", ",", "review", "existing", "attempts", "to", "meet", "this", "challenge", ",", "and", "show", "how", "some", "old", "and", "new", "ideas", "can", "be", "combined", "into", "a", "simple", "approach", "that", "uses", "both", "source", "and", "target", "syntax", "for", "significant", "improvements", "in", "translation", "accuracy", "."], "entities": [{"type": "Operation", "start": 124, "end": 130, "text": "uses both source and target syntax", "sent_idx": 2}, {"type": "Effect", "start": 135, "end": 136, "text": "accuracy", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1146"}
{"text": "Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).", "tokens": ["Recent", "studies", "in", "dialogue", "state", "tracking", "(", "DST", ")", "leverage", "historical", "information", "to", "determine", "states", "which", "are", "generally", "represented", "as", "slot-value", "pairs", ".", "However", ",", "most", "of", "them", "have", "limitations", "to", "efficiently", "exploit", "relevant", "context", "due", "to", "the", "lack", "of", "a", "powerful", "mechanism", "for", "modeling", "interactions", "between", "the", "slot", "and", "the", "dialogue", "history", ".", "Besides", ",", "existing", "methods", "usually", "ignore", "the", "slot", "imbalance", "problem", "and", "treat", "all", "slots", "indiscriminately", ",", "which", "limits", "the", "learning", "of", "hard", "slots", "and", "eventually", "hurts", "overall", "performance", ".", "In", "this", "paper", ",", "we", "propose", "to", "enhance", "the", "DST", "through", "employing", "a", "contextual", "hierarchical", "attention", "network", "to", "not", "only", "discern", "relevant", "information", "at", "both", "word", "level", "and", "turn", "level", "but", "also", "learn", "contextual", "representations", ".", "We", "further", "propose", "an", "adaptive", "objective", "to", "alleviate", "the", "slot", "imbalance", "problem", "by", "dynamically", "adjust", "weights", "of", "different", "slots", "during", "training", ".", "Experimental", "results", "show", "that", "our", "approach", "reaches", "52.68", "%", "and", "58.55", "%", "joint", "accuracy", "on", "MultiWOZ", "2.0", "and", "MultiWOZ", "2.1", "datasets", "respectively", "and", "achieves", "new", "state-of-the-art", "performance", "with", "considerable", "improvements", "(", "+", "1.24", "%", "and", "+", "5.98", "%", ")", "."], "entities": [{"type": "Operation", "start": 94, "end": 100, "text": "employing a contextual hierarchical attention network", "sent_idx": 3}, {"type": "Effect", "start": 153, "end": 155, "text": "joint accuracy", "sent_idx": 5}, {"type": "Effect", "start": 167, "end": 168, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--563"}
{"text": "Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods.", "tokens": ["Medical", "named", "entity", "recognition", "(", "NER", ")", "and", "normalization", "(", "NEN", ")", "are", "fundamental", "for", "constructing", "knowledge", "graphs", "and", "building", "QA", "systems", ".", "Existing", "implementations", "for", "medical", "NER", "and", "NEN", "are", "suffered", "from", "the", "error", "propagation", "between", "the", "two", "tasks", ".", "The", "mispredicted", "mentions", "from", "NER", "will", "directly", "influence", "the", "results", "of", "NEN", ".", "Therefore", ",", "the", "NER", "module", "is", "the", "bottleneck", "of", "the", "whole", "system", ".", "Besides", ",", "the", "learnable", "features", "for", "both", "tasks", "are", "beneficial", "to", "improving", "the", "model", "performance", ".", "To", "avoid", "the", "disadvantages", "of", "existing", "models", "and", "exploit", "the", "generalized", "representation", "across", "the", "two", "tasks", ",", "we", "design", "an", "end-to-end", "progressive", "multi-task", "learning", "model", "for", "jointly", "modeling", "medical", "NER", "and", "NEN", "in", "an", "effective", "way", ".", "There", "are", "three", "level", "tasks", "with", "progressive", "difficulty", "in", "the", "framework", ".", "The", "progressive", "tasks", "can", "reduce", "the", "error", "propagation", "with", "the", "incremental", "task", "settings", "which", "implies", "the", "lower", "level", "tasks", "gain", "the", "supervised", "signals", "other", "than", "errors", "from", "the", "higher", "level", "tasks", "to", "improve", "their", "performances", ".", "Besides", ",", "the", "context", "features", "are", "exploited", "to", "enrich", "the", "semantic", "information", "of", "entity", "mentions", "extracted", "by", "NER", ".", "The", "performance", "of", "NEN", "profits", "from", "the", "enhanced", "entity", "mention", "features", ".", "The", "standard", "entities", "from", "knowledge", "bases", "are", "introduced", "into", "the", "NER", "module", "for", "extracting", "corresponding", "entity", "mentions", "correctly", ".", "The", "empirical", "results", "on", "two", "publicly", "available", "medical", "literature", "datasets", "demonstrate", "the", "superiority", "of", "our", "method", "over", "nine", "typical", "methods", "."], "entities": [{"type": "Operation", "start": 194, "end": 198, "text": "enhanced entity mention features", "sent_idx": 9}, {"type": "Effect", "start": 188, "end": 189, "text": "performance", "sent_idx": 9}, {"type": "Operation", "start": 142, "end": 145, "text": "incremental task settings", "sent_idx": 7}, {"type": "Effect", "start": 166, "end": 167, "text": "performances", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--485"}
{"text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.", "tokens": ["Graphs", "that", "capture", "relations", "between", "textual", "units", "have", "great", "benefits", "for", "detecting", "salient", "information", "from", "multiple", "documents", "and", "generating", "overall", "coherent", "summaries", ".", "In", "this", "paper", ",", "we", "develop", "a", "neural", "abstractive", "multi-document", "summarization", "(", "MDS", ")", "model", "which", "can", "leverage", "well-known", "graph", "representations", "of", "documents", "such", "as", "similarity", "graph", "and", "discourse", "graph", ",", "to", "more", "effectively", "process", "multiple", "input", "documents", "and", "produce", "abstractive", "summaries", ".", "Our", "model", "utilizes", "graphs", "to", "encode", "documents", "in", "order", "to", "capture", "cross-document", "relations", ",", "which", "is", "crucial", "to", "summarizing", "long", "documents", ".", "Our", "model", "can", "also", "take", "advantage", "of", "graphs", "to", "guide", "the", "summary", "generation", "process", ",", "which", "is", "beneficial", "for", "generating", "coherent", "and", "concise", "summaries", ".", "Furthermore", ",", "pre-trained", "language", "models", "can", "be", "easily", "combined", "with", "our", "model", ",", "which", "further", "improve", "the", "summarization", "performance", "significantly", ".", "Empirical", "results", "on", "the", "WikiSum", "and", "MultiNews", "dataset", "show", "that", "the", "proposed", "architecture", "brings", "substantial", "improvements", "over", "several", "strong", "baselines", "."], "entities": [{"type": "Operation", "start": 115, "end": 118, "text": "pre-trained language models", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 132, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--555"}
{"text": "Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search \u2013 the de facto standard inference algorithm in NMT \u2013 and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.", "tokens": ["Neural", "Machine", "Translation", "(", "NMT", ")", "currently", "exhibits", "biases", "such", "as", "producing", "translations", "that", "are", "too", "short", "and", "overgenerating", "frequent", "words", ",", "and", "shows", "poor", "robustness", "to", "copy", "noise", "in", "training", "data", "or", "domain", "shift", ".", "Recent", "work", "has", "tied", "these", "shortcomings", "to", "beam", "search", "\u2013", "the", "de", "facto", "standard", "inference", "algorithm", "in", "NMT", "\u2013", "and", "Eikema", "&", "Aziz", "(", "2020", ")", "propose", "to", "use", "Minimum", "Bayes", "Risk", "(", "MBR", ")", "decoding", "on", "unbiased", "samples", "instead", ".", "In", "this", "paper", ",", "we", "empirically", "investigate", "the", "properties", "of", "MBR", "decoding", "on", "a", "number", "of", "previously", "reported", "biases", "and", "failure", "cases", "of", "beam", "search", ".", "We", "find", "that", "MBR", "still", "exhibits", "a", "length", "and", "token", "frequency", "bias", ",", "owing", "to", "the", "MT", "metrics", "used", "as", "utility", "functions", ",", "but", "that", "MBR", "also", "increases", "robustness", "against", "copy", "noise", "in", "the", "training", "data", "and", "domain", "shift", "."], "entities": [{"type": "Operation", "start": 128, "end": 129, "text": "MBR", "sent_idx": 3}, {"type": "Effect", "start": 131, "end": 132, "text": "robustness", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--22"}
{"text": "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.", "tokens": ["Most", "neural", "machine", "translation", "models", "only", "rely", "on", "pairs", "of", "parallel", "sentences", ",", "assuming", "syntactic", "information", "is", "automatically", "learned", "by", "an", "attention", "mechanism", ".", "In", "this", "work", ",", "we", "investigate", "different", "approaches", "to", "incorporate", "syntactic", "knowledge", "in", "the", "Transformer", "model", "and", "also", "propose", "a", "novel", ",", "parameter-free", ",", "dependency-aware", "self-attention", "mechanism", "that", "improves", "its", "translation", "quality", ",", "especially", "for", "long", "sentences", "and", "in", "low-resource", "scenarios", ".", "We", "show", "the", "efficacy", "of", "each", "approach", "on", "WMT", "English-German", "and", "English-Turkish", ",", "and", "WAT", "English-Japanese", "translation", "tasks", "."], "entities": [{"type": "Operation", "start": 49, "end": 51, "text": "self-attention mechanism", "sent_idx": 1}, {"type": "Effect", "start": 54, "end": 56, "text": "translation quality", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--147"}
{"text": "This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.", "tokens": ["This", "paper", "solves", "the", "fake", "news", "detection", "problem", "under", "a", "more", "realistic", "scenario", "on", "social", "media", ".", "Given", "the", "source", "short-text", "tweet", "and", "the", "corresponding", "sequence", "of", "retweet", "users", "without", "text", "comments", ",", "we", "aim", "at", "predicting", "whether", "the", "source", "tweet", "is", "fake", "or", "not", ",", "and", "generating", "explanation", "by", "highlighting", "the", "evidences", "on", "suspicious", "retweeters", "and", "the", "words", "they", "concern", ".", "We", "develop", "a", "novel", "neural", "network-based", "model", ",", "Graph-aware", "Co-Attention", "Networks", "(", "GCAN", ")", ",", "to", "achieve", "the", "goal", ".", "Extensive", "experiments", "conducted", "on", "real", "tweet", "datasets", "exhibit", "that", "GCAN", "can", "significantly", "outperform", "state-of-the-art", "methods", "by", "16", "%", "in", "accuracy", "on", "average", ".", "In", "addition", ",", "the", "case", "studies", "also", "show", "that", "GCAN", "can", "produce", "reasonable", "explanations", "."], "entities": [{"type": "Operation", "start": 91, "end": 92, "text": "GCAN", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 102, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--48"}
{"text": "Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.", "tokens": ["Task-oriented", "dialogue", "systems", "typically", "require", "manual", "annotation", "of", "dialogue", "slots", "in", "training", "data", ",", "which", "is", "costly", "to", "obtain", ".", "We", "propose", "a", "method", "that", "eliminates", "this", "requirement", ":", "We", "use", "weak", "supervision", "from", "existing", "linguistic", "annotation", "models", "to", "identify", "potential", "slot", "candidates", ",", "then", "automatically", "identify", "domain-relevant", "slots", "by", "using", "clustering", "algorithms", ".", "Furthermore", ",", "we", "use", "the", "resulting", "slot", "annotation", "to", "train", "a", "neural-network-based", "tagger", "that", "is", "able", "to", "perform", "slot", "tagging", "with", "no", "human", "intervention", ".", "This", "tagger", "is", "trained", "solely", "on", "the", "outputs", "of", "our", "method", "and", "thus", "does", "not", "rely", "on", "any", "labeled", "data", ".", "Our", "model", "demonstrates", "state-of-the-art", "performance", "in", "slot", "tagging", "without", "labeled", "training", "data", "on", "four", "different", "dialogue", "domains", ".", "Moreover", ",", "we", "find", "that", "slot", "annotations", "discovered", "by", "our", "model", "significantly", "improve", "the", "performance", "of", "an", "end-to-end", "dialogue", "response", "generation", "model", ",", "compared", "to", "using", "no", "slot", "annotation", "at", "all", "."], "entities": [{"type": "Operation", "start": 123, "end": 125, "text": "slot annotations", "sent_idx": 5}, {"type": "Effect", "start": 132, "end": 133, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--189"}
{"text": "Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.", "tokens": ["Entity", "linking", "(", "EL", ")", "is", "concerned", "with", "disambiguating", "entity", "mentions", "in", "a", "text", "against", "knowledge", "bases", "(", "KB", ")", ".", "It", "is", "crucial", "in", "a", "considerable", "number", "of", "fields", "like", "humanities", ",", "technical", "writing", "and", "biomedical", "sciences", "to", "enrich", "texts", "with", "semantics", "and", "discover", "more", "knowledge", ".", "The", "use", "of", "EL", "in", "such", "domains", "requires", "handling", "noisy", "texts", ",", "low", "resource", "settings", "and", "domain-specific", "KBs", ".", "Existing", "approaches", "are", "mostly", "inappropriate", "for", "this", ",", "as", "they", "depend", "on", "training", "data", ".", "However", ",", "in", "the", "above", "scenario", ",", "there", "exists", "hardly", "annotated", "data", ",", "and", "it", "needs", "to", "be", "created", "from", "scratch", ".", "We", "therefore", "present", "a", "novel", "domain-agnostic", "Human-In-The-Loop", "annotation", "approach", ":", "we", "use", "recommenders", "that", "suggest", "potential", "concepts", "and", "adaptive", "candidate", "ranking", ",", "thereby", "speeding", "up", "the", "overall", "annotation", "process", "and", "making", "it", "less", "tedious", "for", "users", ".", "We", "evaluate", "our", "ranking", "approach", "in", "a", "simulation", "on", "difficult", "texts", "and", "show", "that", "it", "greatly", "outperforms", "a", "strong", "baseline", "in", "ranking", "accuracy", ".", "In", "a", "user", "study", ",", "the", "annotation", "speed", "improves", "by", "35", "%", "compared", "to", "annotating", "without", "interactive", "support", ";", "users", "report", "that", "they", "strongly", "prefer", "our", "system", ".", "An", "open-source", "and", "ready-to-use", "implementation", "based", "on", "the", "text", "annotation", "platform", "INCEpTION", "(", "https://inception-project.github.io", ")", "is", "made", "available", "."], "entities": [{"type": "Operation", "start": 109, "end": 113, "text": "domain-agnostic Human-In-The-Loop annotation approach", "sent_idx": 5}, {"type": "Effect", "start": 171, "end": 173, "text": "annotation speed", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--624"}
{"text": "We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.", "tokens": ["We", "propose", "a", "graph-based", "method", "to", "tackle", "the", "dependency", "tree", "linearization", "task", ".", "We", "formulate", "the", "task", "as", "a", "Traveling", "Salesman", "Problem", "(", "TSP", ")", ",", "and", "use", "a", "biaffine", "attention", "model", "to", "calculate", "the", "edge", "costs", ".", "We", "facilitate", "the", "decoding", "by", "solving", "the", "TSP", "for", "each", "subtree", "and", "combining", "the", "solution", "into", "a", "projective", "tree", ".", "We", "then", "design", "a", "transition", "system", "as", "post-processing", ",", "inspired", "by", "non-projective", "transition-based", "parsing", ",", "to", "obtain", "non-projective", "sentences", ".", "Our", "proposed", "method", "outperforms", "the", "state-of-the-art", "linearizer", "while", "being", "10", "times", "faster", "in", "training", "and", "decoding", "."], "entities": [{"type": "Operation", "start": 3, "end": 5, "text": "graph-based method", "sent_idx": 0}, {"type": "Effect", "start": 88, "end": 89, "text": "times", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--134"}
{"text": "To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).", "tokens": ["To", "achieve", "the", "long-term", "goal", "of", "machines", "being", "able", "to", "engage", "humans", "in", "conversation", ",", "our", "models", "should", "captivate", "the", "interest", "of", "their", "speaking", "partners", ".", "Communication", "grounded", "in", "images", ",", "whereby", "a", "dialogue", "is", "conducted", "based", "on", "a", "given", "photo", ",", "is", "a", "setup", "naturally", "appealing", "to", "humans", "(", "Hu", "et", "al.", ",", "2014", ")", ".", "In", "this", "work", "we", "study", "large-scale", "architectures", "and", "datasets", "for", "this", "goal", ".", "We", "test", "a", "set", "of", "neural", "architectures", "using", "state-of-the-art", "image", "and", "text", "representations", ",", "considering", "various", "ways", "to", "fuse", "the", "components", ".", "To", "test", "such", "models", ",", "we", "collect", "a", "dataset", "of", "grounded", "human-human", "conversations", ",", "where", "speakers", "are", "asked", "to", "play", "roles", "given", "a", "provided", "emotional", "mood", "or", "style", ",", "as", "the", "use", "of", "such", "traits", "is", "also", "a", "key", "factor", "in", "engagingness", "(", "Guo", "et", "al.", ",", "2019", ")", ".", "Our", "dataset", ",", "Image-Chat", ",", "consists", "of", "202k", "dialogues", "over", "202k", "images", "using", "215", "possible", "style", "traits", ".", "Automatic", "metrics", "and", "human", "evaluations", "of", "engagingness", "show", "the", "efficacy", "of", "our", "approach", ";", "in", "particular", ",", "we", "obtain", "state-of-the-art", "performance", "on", "the", "existing", "IGC", "task", ",", "and", "our", "best", "performing", "model", "is", "almost", "on", "par", "with", "humans", "on", "the", "Image-Chat", "test", "set", "(", "preferred", "47.7", "%", "of", "the", "time", ")", "."], "entities": [{"type": "Operation", "start": 77, "end": 83, "text": "using state-of-the-art image and text representations", "sent_idx": 3}, {"type": "Effect", "start": 180, "end": 181, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--219"}
{"text": "In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers' opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers' opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers' opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of document-level sentiment classification, and improve the performance significantly.", "tokens": ["In", "this", "paper", ",", "we", "dedicate", "to", "the", "topic", "of", "aspect", "ranking", ",", "which", "aims", "to", "automatically", "identify", "important", "product", "aspects", "from", "online", "consumer", "reviews", ".", "The", "important", "aspects", "are", "identified", "according", "to", "two", "observations", ":", "(", "a", ")", "the", "important", "aspects", "of", "a", "product", "are", "usually", "commented", "by", "a", "large", "number", "of", "consumers", ";", "and", "(", "b", ")", "consumers", "'", "opinions", "on", "the", "important", "aspects", "greatly", "influence", "their", "overall", "opinions", "on", "the", "product", ".", "In", "particular", ",", "given", "consumer", "reviews", "of", "a", "product", ",", "we", "first", "identify", "the", "product", "aspects", "by", "a", "shallow", "dependency", "parser", "and", "determine", "consumers", "'", "opinions", "on", "these", "aspects", "via", "a", "sentiment", "classifier", ".", "We", "then", "develop", "an", "aspect", "ranking", "algorithm", "to", "identify", "the", "important", "aspects", "by", "simultaneously", "considering", "the", "aspect", "frequency", "and", "the", "influence", "of", "consumers", "'", "opinions", "given", "to", "each", "aspect", "on", "their", "overall", "opinions", ".", "The", "experimental", "results", "on", "11", "popular", "products", "in", "four", "domains", "demonstrate", "the", "effectiveness", "of", "our", "approach", ".", "We", "further", "apply", "the", "aspect", "ranking", "results", "to", "the", "application", "of", "document-level", "sentiment", "classification", ",", "and", "improve", "the", "performance", "significantly", "."], "entities": [{"type": "Operation", "start": 162, "end": 167, "text": "apply the aspect ranking results", "sent_idx": 5}, {"type": "Effect", "start": 178, "end": 179, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1150"}
{"text": "Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries.", "tokens": ["Showing", "items", "that", "do", "not", "match", "search", "query", "intent", "degrades", "customer", "experience", "in", "e-commerce", ".", "These", "mismatches", "result", "from", "counterfactual", "biases", "of", "the", "ranking", "algorithms", "toward", "noisy", "behavioral", "signals", "such", "as", "clicks", "and", "purchases", "in", "the", "search", "logs", ".", "Mitigating", "the", "problem", "requires", "a", "large", "labeled", "dataset", ",", "which", "is", "expensive", "and", "time-consuming", "to", "obtain", ".", "In", "this", "paper", ",", "we", "develop", "a", "deep", ",", "end-to-end", "model", "that", "learns", "to", "effectively", "classify", "mismatches", "and", "to", "generate", "hard", "mismatched", "examples", "to", "improve", "the", "classifier", ".", "We", "train", "the", "model", "end-to-end", "by", "introducing", "a", "latent", "variable", "into", "the", "cross-entropy", "loss", "that", "alternates", "between", "using", "the", "real", "and", "generated", "samples", ".", "This", "not", "only", "makes", "the", "classifier", "more", "robust", "but", "also", "boosts", "the", "overall", "ranking", "performance", ".", "Our", "model", "achieves", "a", "relative", "gain", "compared", "to", "baselines", "by", "over", "26", "%", "in", "F-score", ",", "and", "over", "17", "%", "in", "Area", "Under", "PR", "curve", ".", "On", "live", "search", "traffic", ",", "our", "model", "gains", "significant", "improvement", "in", "multiple", "countries", "."], "entities": [{"type": "Operation", "start": 65, "end": 67, "text": "end-to-end model", "sent_idx": 3}, {"type": "Effect", "start": 138, "end": 139, "text": "F-score", "sent_idx": 6}, {"type": "Effect", "start": 145, "end": 149, "text": "Area Under PR curve", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--614"}
{"text": "One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.", "tokens": ["One", "of", "the", "most", "crucial", "challenges", "in", "question", "answering", "(", "QA", ")", "is", "the", "scarcity", "of", "labeled", "data", ",", "since", "it", "is", "costly", "to", "obtain", "question-answer", "(", "QA", ")", "pairs", "for", "a", "target", "text", "domain", "with", "human", "annotation", ".", "An", "alternative", "approach", "to", "tackle", "the", "problem", "is", "to", "use", "automatically", "generated", "QA", "pairs", "from", "either", "the", "problem", "context", "or", "from", "large", "amount", "of", "unstructured", "texts", "(", "e.g.", "Wikipedia", ")", ".", "In", "this", "work", ",", "we", "propose", "a", "hierarchical", "conditional", "variational", "autoencoder", "(", "HCVAE", ")", "for", "generating", "QA", "pairs", "given", "unstructured", "texts", "as", "contexts", ",", "while", "maximizing", "the", "mutual", "information", "between", "generated", "QA", "pairs", "to", "ensure", "their", "consistency", ".", "We", "validate", "our", "Information", "Maximizing", "Hierarchical", "Conditional", "Variational", "AutoEncoder", "(", "Info-HCVAE", ")", "on", "several", "benchmark", "datasets", "by", "evaluating", "the", "performance", "of", "the", "QA", "model", "(", "BERT-base", ")", "using", "only", "the", "generated", "QA", "pairs", "(", "QA-based", "evaluation", ")", "or", "by", "using", "both", "the", "generated", "and", "human-labeled", "pairs", "(", "semi-supervised", "learning", ")", "for", "training", ",", "against", "state-of-the-art", "baseline", "models", ".", "The", "results", "show", "that", "our", "model", "obtains", "impressive", "performance", "gains", "over", "all", "baselines", "on", "both", "tasks", ",", "using", "only", "a", "fraction", "of", "data", "for", "training", "."], "entities": [{"type": "Operation", "start": 77, "end": 84, "text": "hierarchical conditional variational autoencoder (HCVAE)", "sent_idx": 2}, {"type": "Effect", "start": 174, "end": 175, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--20"}
{"text": "Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.", "tokens": ["Current", "state-of-the-art", "neural", "dialogue", "models", "learn", "from", "human", "conversations", "following", "the", "data-driven", "paradigm", ".", "As", "such", ",", "a", "reliable", "training", "corpus", "is", "the", "crux", "of", "building", "a", "robust", "and", "well-behaved", "dialogue", "model", ".", "However", ",", "due", "to", "the", "open-ended", "nature", "of", "human", "conversations", ",", "the", "quality", "of", "user-generated", "training", "data", "varies", "greatly", ",", "and", "effective", "training", "samples", "are", "typically", "insufficient", "while", "noisy", "samples", "frequently", "appear", ".", "This", "impedes", "the", "learning", "of", "those", "data-driven", "neural", "dialogue", "models", ".", "Therefore", ",", "effective", "dialogue", "learning", "requires", "not", "only", "more", "reliable", "learning", "samples", ",", "but", "also", "fewer", "noisy", "samples", ".", "In", "this", "paper", ",", "we", "propose", "a", "data", "manipulation", "framework", "to", "proactively", "reshape", "the", "data", "distribution", "towards", "reliable", "samples", "by", "augmenting", "and", "highlighting", "effective", "learning", "samples", "as", "well", "as", "reducing", "the", "effect", "of", "inefficient", "samples", "simultaneously", ".", "In", "particular", ",", "the", "data", "manipulation", "model", "selectively", "augments", "the", "training", "samples", "and", "assigns", "an", "importance", "weight", "to", "each", "instance", "to", "reform", "the", "training", "data", ".", "Note", "that", ",", "the", "proposed", "data", "manipulation", "framework", "is", "fully", "data-driven", "and", "learnable", ".", "It", "not", "only", "manipulates", "training", "samples", "to", "optimize", "the", "dialogue", "generation", "model", ",", "but", "also", "learns", "to", "increase", "its", "manipulation", "skills", "through", "gradient", "descent", "with", "validation", "samples", ".", "Extensive", "experiments", "show", "that", "our", "framework", "can", "improve", "the", "dialogue", "generation", "performance", "with", "respect", "to", "various", "automatic", "evaluation", "metrics", "and", "human", "judgments", "."], "entities": [{"type": "Operation", "start": 103, "end": 106, "text": "data manipulation framework", "sent_idx": 5}, {"type": "Effect", "start": 212, "end": 213, "text": "performance", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--564"}
{"text": "Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.", "tokens": ["Recent", "advances", "in", "Named", "Entity", "Recognition", "(", "NER", ")", "show", "that", "document-level", "contexts", "can", "significantly", "improve", "model", "performance", ".", "In", "many", "application", "scenarios", ",", "however", ",", "such", "contexts", "are", "not", "available", ".", "In", "this", "paper", ",", "we", "propose", "to", "find", "external", "contexts", "of", "a", "sentence", "by", "retrieving", "and", "selecting", "a", "set", "of", "semantically", "relevant", "texts", "through", "a", "search", "engine", ",", "with", "the", "original", "sentence", "as", "the", "query", ".", "We", "find", "empirically", "that", "the", "contextual", "representations", "computed", "on", "the", "retrieval-based", "input", "view", ",", "constructed", "through", "the", "concatenation", "of", "a", "sentence", "and", "its", "external", "contexts", ",", "can", "achieve", "significantly", "improved", "performance", "compared", "to", "the", "original", "input", "view", "based", "only", "on", "the", "sentence", ".", "Furthermore", ",", "we", "can", "improve", "the", "model", "performance", "of", "both", "input", "views", "by", "Cooperative", "Learning", ",", "a", "training", "method", "that", "encourages", "the", "two", "input", "views", "to", "produce", "similar", "contextual", "representations", "or", "output", "label", "distributions", ".", "Experiments", "show", "that", "our", "approach", "can", "achieve", "new", "state-of-the-art", "performance", "on", "8", "NER", "data", "sets", "across", "5", "domains", "."], "entities": [{"type": "Operation", "start": 39, "end": 45, "text": "find external contexts of a sentence", "sent_idx": 2}, {"type": "Effect", "start": 155, "end": 156, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--142"}
{"text": "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT\u2019s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.", "tokens": ["Answering", "natural", "language", "questions", "over", "tables", "is", "usually", "seen", "as", "a", "semantic", "parsing", "task", ".", "To", "alleviate", "the", "collection", "cost", "of", "full", "logical", "forms", ",", "one", "popular", "approach", "focuses", "on", "weak", "supervision", "consisting", "of", "denotations", "instead", "of", "logical", "forms", ".", "However", ",", "training", "semantic", "parsers", "from", "weak", "supervision", "poses", "difficulties", ",", "and", "in", "addition", ",", "the", "generated", "logical", "forms", "are", "only", "used", "as", "an", "intermediate", "step", "prior", "to", "retrieving", "the", "denotation", ".", "In", "this", "paper", ",", "we", "present", "TaPas", ",", "an", "approach", "to", "question", "answering", "over", "tables", "without", "generating", "logical", "forms", ".", "TaPas", "trains", "from", "weak", "supervision", ",", "and", "predicts", "the", "denotation", "by", "selecting", "table", "cells", "and", "optionally", "applying", "a", "corresponding", "aggregation", "operator", "to", "such", "selection", ".", "TaPas", "extends", "BERT", "\u2019s", "architecture", "to", "encode", "tables", "as", "input", ",", "initializes", "from", "an", "effective", "joint", "pre-training", "of", "text", "segments", "and", "tables", "crawled", "from", "Wikipedia", ",", "and", "is", "trained", "end-to-end", ".", "We", "experiment", "with", "three", "different", "semantic", "parsing", "datasets", ",", "and", "find", "that", "TaPas", "outperforms", "or", "rivals", "semantic", "parsing", "models", "by", "improving", "state-of-the-art", "accuracy", "on", "SQA", "from", "55.1", "to", "67.2", "and", "performing", "on", "par", "with", "the", "state-of-the-art", "on", "WikiSQL", "and", "WikiTQ", ",", "but", "with", "a", "simpler", "model", "architecture", ".", "We", "additionally", "find", "that", "transfer", "learning", ",", "which", "is", "trivial", "in", "our", "setting", ",", "from", "WikiSQL", "to", "WikiTQ", ",", "yields", "48.7", "accuracy", ",", "4.2", "points", "above", "the", "state-of-the-art", "."], "entities": [{"type": "Operation", "start": 160, "end": 161, "text": "TaPas", "sent_idx": 6}, {"type": "Effect", "start": 170, "end": 171, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--398"}
{"text": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.", "tokens": ["Transformers", "are", "ubiquitous", "in", "Natural", "Language", "Processing", "(", "NLP", ")", "tasks", ",", "but", "they", "are", "difficult", "to", "be", "deployed", "on", "hardware", "due", "to", "the", "intensive", "computation", ".", "To", "enable", "low-latency", "inference", "on", "resource-constrained", "hardware", "platforms", ",", "we", "propose", "to", "design", "Hardware-Aware", "Transformers", "(", "HAT", ")", "with", "neural", "architecture", "search", ".", "We", "first", "construct", "a", "large", "design", "space", "with", "arbitrary", "encoder-decoder", "attention", "and", "heterogeneous", "layers", ".", "Then", "we", "train", "a", "SuperTransformer", "that", "covers", "all", "candidates", "in", "the", "design", "space", ",", "and", "efficiently", "produces", "many", "SubTransformers", "with", "weight", "sharing", ".", "Finally", ",", "we", "perform", "an", "evolutionary", "search", "with", "a", "hardware", "latency", "constraint", "to", "find", "a", "specialized", "SubTransformer", "dedicated", "to", "run", "fast", "on", "the", "target", "hardware", ".", "Extensive", "experiments", "on", "four", "machine", "translation", "tasks", "demonstrate", "that", "HAT", "can", "discover", "efficient", "models", "for", "different", "hardware", "(", "CPU", ",", "GPU", ",", "IoT", "device", ")", ".", "When", "running", "WMT\u201914", "translation", "task", "on", "Raspberry", "Pi-4", ",", "HAT", "can", "achieve", "3", "\u00d7", "speedup", ",", "3.7", "\u00d7", "smaller", "size", "over", "baseline", "Transformer", ";", "2.7", "\u00d7", "speedup", ",", "3.6", "\u00d7", "smaller", "size", "over", "Evolved", "Transformer", "with", "12,041", "\u00d7", "less", "search", "cost", "and", "no", "performance", "loss", ".", "HAT", "is", "open-sourced", "at", "https://github.com/mit-han-lab/hardware-aware-transformers", "."], "entities": [{"type": "Operation", "start": 149, "end": 150, "text": "HAT", "sent_idx": 6}, {"type": "Effect", "start": 154, "end": 155, "text": "speedup", "sent_idx": 6}, {"type": "Effect", "start": 159, "end": 160, "text": "size", "sent_idx": 6}, {"type": "Effect", "start": 183, "end": 184, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--686"}
{"text": "Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.", "tokens": ["Neural", "abstractive", "summarization", "models", "are", "able", "to", "generate", "summaries", "which", "have", "high", "overlap", "with", "human", "references", ".", "However", ",", "existing", "models", "are", "not", "optimized", "for", "factual", "correctness", ",", "a", "critical", "metric", "in", "real-world", "applications", ".", "In", "this", "work", ",", "we", "develop", "a", "general", "framework", "where", "we", "evaluate", "the", "factual", "correctness", "of", "a", "generated", "summary", "by", "fact-checking", "it", "automatically", "against", "its", "reference", "using", "an", "information", "extraction", "module", ".", "We", "further", "propose", "a", "training", "strategy", "which", "optimizes", "a", "neural", "summarization", "model", "with", "a", "factual", "correctness", "reward", "via", "reinforcement", "learning", ".", "We", "apply", "the", "proposed", "method", "to", "the", "summarization", "of", "radiology", "reports", ",", "where", "factual", "correctness", "is", "a", "key", "requirement", ".", "On", "two", "separate", "datasets", "collected", "from", "hospitals", ",", "we", "show", "via", "both", "automatic", "and", "human", "evaluation", "that", "the", "proposed", "approach", "substantially", "improves", "the", "factual", "correctness", "and", "overall", "quality", "of", "outputs", "over", "a", "competitive", "neural", "summarization", "system", ",", "producing", "radiology", "summaries", "that", "approach", "the", "quality", "of", "human-authored", "ones", "."], "entities": [{"type": "Operation", "start": 42, "end": 44, "text": "general framework", "sent_idx": 2}, {"type": "Effect", "start": 131, "end": 133, "text": "factual correctness", "sent_idx": 5}, {"type": "Effect", "start": 134, "end": 136, "text": "overall quality", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--458"}
{"text": "The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code\u2013text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.", "tokens": ["The", "ability", "to", "match", "pieces", "of", "code", "to", "their", "corresponding", "natural", "language", "descriptions", "and", "vice", "versa", "is", "fundamental", "for", "natural", "language", "search", "interfaces", "to", "software", "repositories", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "multi-perspective", "cross-lingual", "neural", "framework", "for", "code", "\u2013", "text", "matching", ",", "inspired", "in", "part", "by", "a", "previous", "model", "for", "monolingual", "text-to-text", "matching", ",", "to", "capture", "both", "global", "and", "local", "similarities", ".", "Our", "experiments", "on", "the", "CoNaLa", "dataset", "show", "that", "our", "proposed", "model", "yields", "better", "performance", "on", "this", "cross-lingual", "text-to-code", "matching", "task", "than", "previous", "approaches", "that", "map", "code", "and", "text", "to", "a", "single", "joint", "embedding", "space", "."], "entities": [{"type": "Operation", "start": 35, "end": 39, "text": "multi-perspective cross-lingual neural framework", "sent_idx": 1}, {"type": "Effect", "start": 78, "end": 79, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--758"}
{"text": "Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.", "tokens": ["Most", "sentiment", "analysis", "approaches", "use", "as", "baseline", "a", "support", "vector", "machines", "(", "SVM", ")", "classifier", "with", "binary", "unigram", "weights", ".", "In", "this", "paper", ",", "we", "explore", "whether", "more", "sophisticated", "feature", "weighting", "schemes", "from", "Information", "Retrieval", "can", "enhance", "classification", "accuracy", ".", "We", "show", "that", "variants", "of", "the", "classic", "tf.idf", "scheme", "adapted", "to", "sentiment", "analysis", "provide", "significant", "increases", "in", "accuracy", ",", "especially", "when", "using", "a", "sublinear", "function", "for", "term", "frequency", "weights", "and", "document", "frequency", "smoothing", ".", "The", "techniques", "are", "tested", "on", "a", "wide", "selection", "of", "data", "sets", "and", "produce", "the", "best", "accuracy", "to", "our", "knowledge", "."], "entities": [{"type": "Operation", "start": 29, "end": 32, "text": "feature weighting schemes", "sent_idx": 1}, {"type": "Effect", "start": 38, "end": 39, "text": "accuracy", "sent_idx": 1}, {"type": "Operation", "start": 46, "end": 49, "text": "classic tf.idf scheme", "sent_idx": 2}, {"type": "Effect", "start": 57, "end": 58, "text": "accuracy", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "P10-1141"}
{"text": "We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.", "tokens": ["We", "address", "the", "problem", "of", "extractive", "question", "answering", "using", "document-level", "distant", "super-vision", ",", "pairing", "questions", "and", "relevant", "documents", "with", "answer", "strings", ".", "We", "compare", "previously", "used", "probability", "space", "and", "distant", "supervision", "assumptions", "(", "assumptions", "on", "the", "correspondence", "between", "the", "weak", "answer", "string", "labels", "and", "possible", "answer", "mention", "spans", ")", ".", "We", "show", "that", "these", "assumptions", "interact", ",", "and", "that", "different", "configurations", "provide", "complementary", "benefits", ".", "We", "demonstrate", "that", "a", "multi-objective", "model", "can", "efficiently", "combine", "the", "advantages", "of", "multiple", "assumptions", "and", "outperform", "the", "best", "individual", "formulation", ".", "Our", "approach", "outperforms", "previous", "state-of-the-art", "models", "by", "4.3", "points", "in", "F1", "on", "TriviaQA-Wiki", "and", "1.7", "points", "in", "Rouge-L", "on", "NarrativeQA", "summaries", "."], "entities": [{"type": "Operation", "start": 8, "end": 18, "text": "using document-level distant super-vision, pairing questions and relevant documents", "sent_idx": 0}, {"type": "Effect", "start": 96, "end": 97, "text": "F1", "sent_idx": 4}, {"type": "Effect", "start": 103, "end": 104, "text": "Rouge-L", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--501"}
{"text": "Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.", "tokens": ["Off-topic", "spoken", "response", "detection", ",", "the", "task", "aiming", "at", "predicting", "whether", "a", "response", "is", "off-topic", "for", "the", "corresponding", "prompt", ",", "is", "important", "for", "an", "automated", "speaking", "assessment", "system", ".", "In", "many", "real-world", "educational", "applications", ",", "off-topic", "spoken", "response", "detectors", "are", "required", "to", "achieve", "high", "recall", "for", "off-topic", "responses", "not", "only", "on", "seen", "prompts", "but", "also", "on", "prompts", "that", "are", "unseen", "during", "training", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "approach", "for", "off-topic", "spoken", "response", "detection", "with", "high", "off-topic", "recall", "on", "both", "seen", "and", "unseen", "prompts", ".", "We", "introduce", "a", "new", "model", ",", "Gated", "Convolutional", "Bidirectional", "Attention-based", "Model", "(", "GCBiA", ")", ",", "which", "applies", "bi-attention", "mechanism", "and", "convolutions", "to", "extract", "topic", "words", "of", "prompts", "and", "key-phrases", "of", "responses", ",", "and", "introduces", "gated", "unit", "and", "residual", "connections", "between", "major", "layers", "to", "better", "represent", "the", "relevance", "of", "responses", "and", "prompts", ".", "Moreover", ",", "a", "new", "negative", "sampling", "method", "is", "proposed", "to", "augment", "training", "data", ".", "Experiment", "results", "demonstrate", "that", "our", "novel", "approach", "can", "achieve", "significant", "improvements", "in", "detecting", "off-topic", "responses", "with", "extremely", "high", "on-topic", "recall", ",", "for", "both", "seen", "and", "unseen", "prompts", "."], "entities": [{"type": "Operation", "start": 94, "end": 102, "text": "Gated Convolutional Bidirectional Attention-based Model (GCBiA)", "sent_idx": 3}, {"type": "Effect", "start": 172, "end": 174, "text": "on-topic recall", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--56"}
{"text": "We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentence-level hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces word-error rates significantly by 10.2% relative to the best baseline models.", "tokens": ["We", "introduce", "a", "novel", "mechanism", "for", "incorporating", "articulatory", "dynamics", "into", "speech", "recognition", "with", "the", "theory", "of", "task", "dynamics", ".", "This", "system", "reranks", "sentence-level", "hypotheses", "by", "the", "likelihoods", "of", "their", "hypothetical", "articulatory", "realizations", "which", "are", "derived", "from", "relationships", "learned", "with", "aligned", "acoustic/articulatory", "data", ".", "Experiments", "compare", "this", "with", "two", "baseline", "systems", ",", "namely", "an", "acoustic", "hidden", "Markov", "model", "and", "a", "dynamic", "Bayes", "network", "augmented", "with", "discretized", "representations", "of", "the", "vocal", "tract", ".", "Our", "system", "based", "on", "task", "dynamics", "reduces", "word-error", "rates", "significantly", "by", "10.2", "%", "relative", "to", "the", "best", "baseline", "models", "."], "entities": [{"type": "Operation", "start": 71, "end": 73, "text": "Our system", "sent_idx": 3}, {"type": "Effect", "start": 78, "end": 80, "text": "word-error rates", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1007"}
{"text": "Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ~7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.", "tokens": ["Scoring", "sentences", "in", "documents", "given", "abstract", "summaries", "created", "by", "humans", "is", "important", "in", "extractive", "multi-document", "summarization", ".", "In", "this", "paper", ",", "we", "formulate", "extractive", "summarization", "as", "a", "two", "step", "learning", "problem", "building", "a", "generative", "model", "for", "pattern", "discovery", "and", "a", "regression", "model", "for", "inference", ".", "We", "calculate", "scores", "for", "sentences", "in", "document", "clusters", "based", "on", "their", "latent", "characteristics", "using", "a", "hierarchical", "topic", "model", ".", "Then", ",", "using", "these", "scores", ",", "we", "train", "a", "regression", "model", "based", "on", "the", "lexical", "and", "structural", "characteristics", "of", "the", "sentences", ",", "and", "use", "the", "model", "to", "score", "sentences", "of", "new", "documents", "to", "form", "a", "summary", ".", "Our", "system", "advances", "current", "state-of-the-art", "improving", "ROUGE", "scores", "by", "~7", "%", ".", "Generated", "summaries", "are", "less", "redundant", "and", "more", "coherent", "based", "upon", "manual", "quality", "evaluations", "."], "entities": [{"type": "Operation", "start": 32, "end": 35, "text": "a generative model", "sent_idx": 1}, {"type": "Effect", "start": 107, "end": 109, "text": "ROUGE scores", "sent_idx": 4}, {"type": "Operation", "start": 39, "end": 42, "text": "a regression model", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P10-1084"}
{"text": "We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image\u2013caption coherence relations, we annotate 10,000 instances from publicly-available image\u2013caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.", "tokens": ["We", "use", "coherence", "relations", "inspired", "by", "computational", "models", "of", "discourse", "to", "study", "the", "information", "needs", "and", "goals", "of", "image", "captioning", ".", "Using", "an", "annotation", "protocol", "specifically", "devised", "for", "capturing", "image", "\u2013", "caption", "coherence", "relations", ",", "we", "annotate", "10,000", "instances", "from", "publicly-available", "image", "\u2013", "caption", "pairs", ".", "We", "introduce", "a", "new", "task", "for", "learning", "inferences", "in", "imagery", "and", "text", ",", "coherence", "relation", "prediction", ",", "and", "show", "that", "these", "coherence", "annotations", "can", "be", "exploited", "to", "learn", "relation", "classifiers", "as", "an", "intermediary", "step", ",", "and", "also", "train", "coherence-aware", ",", "controllable", "image", "captioning", "models", ".", "The", "results", "show", "a", "dramatic", "improvement", "in", "the", "consistency", "and", "quality", "of", "the", "generated", "captions", "with", "respect", "to", "information", "needs", "specified", "via", "coherence", "relations", "."], "entities": [{"type": "Operation", "start": 67, "end": 69, "text": "coherence annotations", "sent_idx": 2}, {"type": "Effect", "start": 99, "end": 100, "text": "consistency", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 102, "text": "quality", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--583"}
{"text": "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference.", "tokens": ["Due", "to", "recent", "pretrained", "multilingual", "representation", "models", ",", "it", "has", "become", "feasible", "to", "exploit", "labeled", "data", "from", "one", "language", "to", "train", "a", "cross-lingual", "model", "that", "can", "then", "be", "applied", "to", "multiple", "new", "languages", ".", "In", "practice", ",", "however", ",", "we", "still", "face", "the", "problem", "of", "scarce", "labeled", "data", ",", "leading", "to", "subpar", "results", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "data", "augmentation", "strategy", "for", "better", "cross-lingual", "natural", "language", "inference", "by", "enriching", "the", "data", "to", "reflect", "more", "diversity", "in", "a", "semantically", "faithful", "way", ".", "To", "this", "end", ",", "we", "propose", "two", "methods", "of", "training", "a", "generative", "model", "to", "induce", "synthesized", "examples", ",", "and", "then", "leverage", "the", "resulting", "data", "using", "an", "adversarial", "training", "regimen", "for", "more", "robustness", ".", "In", "a", "series", "of", "detailed", "experiments", ",", "we", "show", "that", "this", "fruitful", "combination", "leads", "to", "substantial", "gains", "in", "cross-lingual", "inference", "."], "entities": [{"type": "Operation", "start": 111, "end": 114, "text": "adversarial training regimen", "sent_idx": 3}, {"type": "Effect", "start": 116, "end": 117, "text": "robustness", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--401"}
{"text": "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with \u201ccopy\u201d information while translations based on embedding similarities tend to extend the translation \u201ccontext\u201d. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.", "tokens": ["This", "paper", "explores", "data", "augmentation", "methods", "for", "training", "Neural", "Machine", "Translation", "to", "make", "use", "of", "similar", "translations", ",", "in", "a", "comparable", "way", "a", "human", "translator", "employs", "fuzzy", "matches", ".", "In", "particular", ",", "we", "show", "how", "we", "can", "simply", "present", "the", "neural", "model", "with", "information", "of", "both", "source", "and", "target", "sides", "of", "the", "fuzzy", "matches", ",", "we", "also", "extend", "the", "similarity", "to", "include", "semantically", "related", "translations", "retrieved", "using", "sentence", "distributed", "representations", ".", "We", "show", "that", "translations", "based", "on", "fuzzy", "matching", "provide", "the", "model", "with", "\u201c", "copy", "\u201d", "information", "while", "translations", "based", "on", "embedding", "similarities", "tend", "to", "extend", "the", "translation", "\u201c", "context", "\u201d", ".", "Results", "indicate", "that", "the", "effect", "from", "both", "similar", "sentences", "are", "adding", "up", "to", "further", "boost", "accuracy", ",", "combine", "naturally", "with", "model", "fine-tuning", "and", "are", "providing", "dynamic", "adaptation", "for", "unseen", "translation", "pairs", ".", "Tests", "on", "multiple", "data", "sets", "and", "domains", "show", "consistent", "accuracy", "improvements", ".", "To", "foster", "research", "around", "these", "techniques", ",", "we", "also", "release", "an", "Open-Source", "toolkit", "with", "efficient", "and", "flexible", "fuzzy-match", "implementation", "."], "entities": [{"type": "Operation", "start": 3, "end": 6, "text": "data augmentation methods", "sent_idx": 0}, {"type": "Effect", "start": 143, "end": 144, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--144"}
{"text": "Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation. We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.", "tokens": ["Recent", "years", "have", "witnessed", "various", "types", "of", "generative", "models", "for", "natural", "language", "generation", "(", "NLG", ")", ",", "especially", "RNNs", "or", "transformer", "based", "sequence-to-sequence", "models", ",", "as", "well", "as", "variational", "autoencoder", "(", "VAE", ")", "and", "generative", "adversarial", "network", "(", "GAN", ")", "based", "models", ".", "However", ",", "flow-based", "generative", "models", ",", "which", "achieve", "strong", "performance", "in", "image", "generation", "due", "to", "their", "invertibility", "and", "exact", "density", "estimation", "properties", ",", "have", "been", "less", "explored", "for", "NLG", ".", "In", "this", "paper", ",", "we", "propose", "a", "flow-based", "language", "generation", "model", "by", "adapting", "previous", "flow", "generative", "models", "to", "language", "generation", "via", "continuous", "input", "embeddings", ",", "adapted", "affine", "coupling", "structures", ",", "and", "a", "novel", "architecture", "for", "autoregressive", "text", "generation", ".", "We", "also", "apply", "our", "framework", "to", "Sequence-to-Sequence", "generation", ",", "including", "text-", "and", "video-based", "Question", "Generation", "(", "QG", ")", "and", "Neural", "Machine", "Translation", "(", "NMT", ")", ",", "and", "data", "augmentation", "for", "Question", "Answering", "(", "QA", ")", ".", "We", "use", "our", "language", "flow", "model", "to", "provide", "extra", "input", "features", "for", "QG", "and", "NMT", ",", "which", "achieves", "improvements", "over", "the", "strong", "QG", "baselines", "on", "SQuAD", "and", "TVQA", "and", "NMT", "baseline", "on", "WMT16", ".", "We", "also", "augment", "QA", "data", "with", "new", "context", "by", "injecting", "noise", "to", "the", "latent", "features", "of", "the", "language", "flow", "and", "show", "this", "augmentation", "leads", "to", "a", "large", "performance", "improvement", "from", "strong", "baselines", "on", "SQuAD", "and", "TVQA", "."], "entities": [{"type": "Operation", "start": 191, "end": 201, "text": "injecting noise to the latent features of the language flow", "sent_idx": 5}, {"type": "Effect", "start": 209, "end": 210, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 184, "end": 190, "text": "augment QA data with new context", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--355"}
{"text": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.", "tokens": ["Despite", "excellent", "performance", "on", "many", "tasks", ",", "NLP", "systems", "are", "easily", "fooled", "by", "small", "adversarial", "perturbations", "of", "inputs", ".", "Existing", "procedures", "to", "defend", "against", "such", "perturbations", "are", "either", "(", "i", ")", "heuristic", "in", "nature", "and", "susceptible", "to", "stronger", "attacks", "or", "(", "ii", ")", "provide", "guaranteed", "robustness", "to", "worst-case", "attacks", ",", "but", "are", "incompatible", "with", "state-of-the-art", "models", "like", "BERT", ".", "In", "this", "work", ",", "we", "introduce", "robust", "encodings", "(", "RobEn", "):", "a", "simple", "framework", "that", "confers", "guaranteed", "robustness", ",", "without", "making", "compromises", "on", "model", "architecture", ".", "The", "core", "component", "of", "RobEn", "is", "an", "encoding", "function", ",", "which", "maps", "sentences", "to", "a", "smaller", ",", "discrete", "space", "of", "encodings", ".", "Systems", "using", "these", "encodings", "as", "a", "bottleneck", "confer", "guaranteed", "robustness", "with", "standard", "training", ",", "and", "the", "same", "encodings", "can", "be", "used", "across", "multiple", "tasks", ".", "We", "identify", "two", "desiderata", "to", "construct", "robust", "encoding", "functions", ":", "perturbations", "of", "a", "sentence", "should", "map", "to", "a", "small", "set", "of", "encodings", "(", "stability", ")", ",", "and", "models", "using", "encodings", "should", "still", "perform", "well", "(", "fidelity", ")", ".", "We", "instantiate", "RobEn", "to", "defend", "against", "a", "large", "family", "of", "adversarial", "typos", ".", "Across", "six", "tasks", "from", "GLUE", ",", "our", "instantiation", "of", "RobEn", "paired", "with", "BERT", "achieves", "an", "average", "robust", "accuracy", "of", "71.3", "%", "against", "all", "adversarial", "typos", "in", "the", "family", "considered", ",", "while", "previous", "work", "using", "a", "typo-corrector", "achieves", "only", "35.3", "%", "accuracy", "against", "a", "simple", "greedy", "attack", "."], "entities": [{"type": "Operation", "start": 89, "end": 90, "text": "RobEn", "sent_idx": 3}, {"type": "Effect", "start": 116, "end": 117, "text": "robustness", "sent_idx": 4}, {"type": "Operation", "start": 190, "end": 196, "text": "instantiation of RobEn paired with BERT", "sent_idx": 7}, {"type": "Effect", "start": 198, "end": 201, "text": "average robust accuracy", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--245"}
{"text": "Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as \u201cList/Count all female athletes who were born in 20th century\u201d, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.", "tokens": ["Neural", "models", "have", "shown", "impressive", "performance", "gains", "in", "answering", "queries", "from", "natural", "language", "text", ".", "However", ",", "existing", "works", "are", "unable", "to", "support", "database", "queries", ",", "such", "as", "\u201c", "List/Count", "all", "female", "athletes", "who", "were", "born", "in", "20th", "century", "\u201d", ",", "which", "require", "reasoning", "over", "sets", "of", "relevant", "facts", "with", "operations", "such", "as", "join", ",", "filtering", "and", "aggregation", ".", "We", "show", "that", "while", "state-of-the-art", "transformer", "models", "perform", "very", "well", "for", "small", "databases", ",", "they", "exhibit", "limitations", "in", "processing", "noisy", "data", ",", "numerical", "operations", ",", "and", "queries", "that", "aggregate", "facts", ".", "We", "propose", "a", "modular", "architecture", "to", "answer", "these", "database-style", "queries", "over", "multiple", "spans", "from", "text", "and", "aggregating", "these", "at", "scale", ".", "We", "evaluate", "the", "architecture", "using", "WikiNLDB", ",", "a", "novel", "dataset", "for", "exploring", "such", "queries", ".", "Our", "architecture", "scales", "to", "databases", "containing", "thousands", "of", "facts", "whereas", "contemporary", "models", "are", "limited", "by", "how", "many", "facts", "can", "be", "encoded", ".", "In", "direct", "comparison", "on", "small", "databases", ",", "our", "approach", "increases", "overall", "answer", "accuracy", "from", "85", "%", "to", "90", "%", ".", "On", "larger", "databases", ",", "our", "approach", "retains", "its", "accuracy", "whereas", "transformer", "baselines", "could", "not", "encode", "the", "context", "."], "entities": [{"type": "Operation", "start": 93, "end": 95, "text": "modular architecture", "sent_idx": 3}, {"type": "Effect", "start": 160, "end": 161, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--241"}
{"text": "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline.", "tokens": ["We", "describe", "a", "new", "approach", "to", "disambiguating", "semantic", "frames", "evoked", "by", "lexical", "predicates", "previously", "unseen", "in", "a", "lexicon", "or", "annotated", "data", ".", "Our", "approach", "makes", "use", "of", "large", "amounts", "of", "unlabeled", "data", "in", "a", "graph-based", "semi-supervised", "learning", "framework", ".", "We", "construct", "a", "large", "graph", "where", "vertices", "correspond", "to", "potential", "predicates", "and", "use", "label", "propagation", "to", "learn", "possible", "semantic", "frames", "for", "new", "ones", ".", "The", "label-propagated", "graph", "is", "used", "within", "a", "frame-semantic", "parser", "and", ",", "for", "unknown", "predicates", ",", "results", "in", "over", "15", "%", "absolute", "improvement", "in", "frame", "identification", "accuracy", "and", "over", "13", "%", "absolute", "improvement", "in", "full", "frame-semantic", "parsing", "F1", "score", "on", "a", "blind", "test", "set", ",", "over", "a", "state-of-the-art", "supervised", "baseline", "."], "entities": [{"type": "Operation", "start": 24, "end": 32, "text": "makes use of large amounts of unlabeled data", "sent_idx": 1}, {"type": "Effect", "start": 96, "end": 101, "text": "full frame-semantic parsing F1 score", "sent_idx": 3}, {"type": "Effect", "start": 86, "end": 89, "text": "frame identification accuracy", "sent_idx": 3}, {"type": "Operation", "start": 34, "end": 38, "text": "graph-based semi-supervised learning framework", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 1}], "id": "P11-1144"}
{"text": "A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator. With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus. Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain. We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation. Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.", "tokens": ["A", "Dialogue", "State", "Tracker", "(", "DST", ")", "is", "a", "core", "component", "of", "a", "modular", "task-oriented", "dialogue", "system", ".", "Tremendous", "progress", "has", "been", "made", "in", "recent", "years", ".", "However", ",", "the", "major", "challenges", "remain", ".", "The", "state-of-the-art", "accuracy", "for", "DST", "is", "below", "50", "%", "for", "a", "multi-domain", "dialogue", "task", ".", "A", "learnable", "DST", "for", "any", "new", "domain", "requires", "a", "large", "amount", "of", "labeled", "in-domain", "data", "and", "training", "from", "scratch", ".", "In", "this", "paper", ",", "we", "propose", "a", "Meta-Reinforced", "Multi-Domain", "State", "Generator", "(", "MERET", ")", ".", "Our", "first", "contribution", "is", "to", "improve", "the", "DST", "accuracy", ".", "We", "enhance", "a", "neural", "model", "based", "DST", "generator", "with", "a", "reward", "manager", ",", "which", "is", "built", "on", "policy", "gradient", "reinforcement", "learning", "(", "RL", ")", "to", "fine-tune", "the", "generator", ".", "With", "this", "change", ",", "we", "are", "able", "to", "improve", "the", "joint", "accuracy", "of", "DST", "from", "48.79", "%", "to", "50.91", "%", "on", "the", "MultiWOZ", "corpus", ".", "Second", ",", "we", "explore", "to", "train", "a", "DST", "meta-learning", "model", "with", "a", "few", "domains", "as", "source", "domains", "and", "a", "new", "domain", "as", "target", "domain", ".", "We", "apply", "the", "model-agnostic", "meta-learning", "algorithm", "(", "MAML", ")", "to", "DST", "and", "the", "obtained", "meta-learning", "model", "is", "used", "for", "new", "domain", "adaptation", ".", "Our", "experimental", "results", "show", "this", "solution", "is", "able", "to", "outperform", "the", "traditional", "training", "approach", "with", "extremely", "less", "training", "data", "in", "target", "domain", "."], "entities": [{"type": "Operation", "start": 95, "end": 107, "text": "enhance a neural model based DST generator with a reward manager,", "sent_idx": 7}, {"type": "Effect", "start": 133, "end": 135, "text": "joint accuracy", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--636"}
{"text": "We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen\u2019s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.", "tokens": ["We", "present", "BERTGen", ",", "a", "novel", ",", "generative", ",", "decoder-only", "model", "which", "extends", "BERT", "by", "fusing", "multimodal", "and", "multilingual", "pre-trained", "models", "VL-BERT", "and", "M-BERT", ",", "respectively", ".", "BERTGen", "is", "auto-regressively", "trained", "for", "language", "generation", "tasks", ",", "namely", "image", "captioning", ",", "machine", "translation", "and", "multimodal", "machine", "translation", ",", "under", "a", "multi-task", "setting", ".", "With", "a", "comprehensive", "set", "of", "evaluations", ",", "we", "show", "that", "BERTGen", "outperforms", "many", "strong", "baselines", "across", "the", "tasks", "explored", ".", "We", "also", "show", "BERTGen", "\u2019s", "ability", "for", "zero-shot", "language", "generation", ",", "where", "it", "exhibits", "competitive", "performance", "to", "supervised", "counterparts", ".", "Finally", ",", "we", "conduct", "ablation", "studies", "which", "demonstrate", "that", "BERTGen", "substantially", "benefits", "from", "multi-tasking", "and", "effectively", "transfers", "relevant", "inductive", "biases", "from", "the", "pre-trained", "models", "."], "entities": [{"type": "Operation", "start": 75, "end": 76, "text": "BERTGen", "sent_idx": 3}, {"type": "Effect", "start": 87, "end": 88, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--503"}
{"text": "This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper.", "tokens": ["This", "paper", "introduces", "the", "Webis", "Gmane", "Email", "Corpus", "2019", ",", "the", "largest", "publicly", "available", "and", "fully", "preprocessed", "email", "corpus", "to", "date", ".", "We", "crawled", "more", "than", "153", "million", "emails", "from", "14,699", "mailing", "lists", "and", "segmented", "them", "into", "semantically", "consistent", "components", "using", "a", "new", "neural", "segmentation", "model", ".", "With", "96", "%", "accuracy", "on", "15", "classes", "of", "email", "segments", ",", "our", "model", "achieves", "state-of-the-art", "performance", "while", "being", "more", "efficient", "to", "train", "than", "previous", "ones", ".", "All", "data", ",", "code", ",", "and", "trained", "models", "are", "made", "freely", "available", "alongside", "the", "paper", "."], "entities": [{"type": "Operation", "start": 40, "end": 46, "text": "using a new neural segmentation model", "sent_idx": 1}, {"type": "Effect", "start": 50, "end": 51, "text": "accuracy", "sent_idx": 2}, {"type": "Effect", "start": 62, "end": 63, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--108"}
{"text": "Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach. Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.", "tokens": ["Detecting", "rumors", "on", "social", "media", "is", "a", "very", "critical", "task", "with", "significant", "implications", "to", "the", "economy", ",", "public", "health", ",", "etc", ".", "Previous", "works", "generally", "capture", "effective", "features", "from", "texts", "and", "the", "propagation", "structure", ".", "However", ",", "the", "uncertainty", "caused", "by", "unreliable", "relations", "in", "the", "propagation", "structure", "is", "common", "and", "inevitable", "due", "to", "wily", "rumor", "producers", "and", "the", "limited", "collection", "of", "spread", "data", ".", "Most", "approaches", "neglect", "it", "and", "may", "seriously", "limit", "the", "learning", "of", "features", ".", "Towards", "this", "issue", ",", "this", "paper", "makes", "the", "first", "attempt", "to", "explore", "propagation", "uncertainty", "for", "rumor", "detection", ".", "Specifically", ",", "we", "propose", "a", "novel", "Edge-enhanced", "Bayesian", "Graph", "Convolutional", "Network", "(", "EBGCN", ")", "to", "capture", "robust", "structural", "features", ".", "The", "model", "adaptively", "rethinks", "the", "reliability", "of", "latent", "relations", "by", "adopting", "a", "Bayesian", "approach", ".", "Besides", ",", "we", "design", "a", "new", "edge-wise", "consistency", "training", "framework", "to", "optimize", "the", "model", "by", "enforcing", "consistency", "on", "relations", ".", "Experiments", "on", "three", "public", "benchmark", "datasets", "demonstrate", "that", "the", "proposed", "model", "achieves", "better", "performance", "than", "baseline", "methods", "on", "both", "rumor", "detection", "and", "early", "rumor", "detection", "tasks", "."], "entities": [{"type": "Operation", "start": 101, "end": 109, "text": "Edge-enhanced Bayesian Graph Convolutional Network (EBGCN)", "sent_idx": 5}, {"type": "Effect", "start": 163, "end": 164, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--297"}
{"text": "Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.", "tokens": ["Evidence", "retrieval", "is", "a", "critical", "stage", "of", "question", "answering", "(", "QA", ")", ",", "necessary", "not", "only", "to", "improve", "performance", ",", "but", "also", "to", "explain", "the", "decisions", "of", "the", "QA", "method", ".", "We", "introduce", "a", "simple", ",", "fast", ",", "and", "unsupervised", "iterative", "evidence", "retrieval", "method", ",", "which", "relies", "on", "three", "ideas", ":", "(", "a", ")", "an", "unsupervised", "alignment", "approach", "to", "soft-align", "questions", "and", "answers", "with", "justification", "sentences", "using", "only", "GloVe", "embeddings", ",", "(", "b", ")", "an", "iterative", "process", "that", "reformulates", "queries", "focusing", "on", "terms", "that", "are", "not", "covered", "by", "existing", "justifications", ",", "which", "(", "c", ")", "stops", "when", "the", "terms", "in", "the", "given", "question", "and", "candidate", "answers", "are", "covered", "by", "the", "retrieved", "justifications", ".", "Despite", "its", "simplicity", ",", "our", "approach", "outperforms", "all", "the", "previous", "methods", "(", "including", "supervised", "methods", ")", "on", "the", "evidence", "selection", "task", "on", "two", "datasets", ":", "MultiRC", "and", "QASC", ".", "When", "these", "evidence", "sentences", "are", "fed", "into", "a", "RoBERTa", "answer", "classification", "component", ",", "we", "achieve", "state-of-the-art", "QA", "performance", "on", "these", "two", "datasets", "."], "entities": [{"type": "Operation", "start": 150, "end": 154, "text": "RoBERTa answer classification component", "sent_idx": 3}, {"type": "Effect", "start": 159, "end": 160, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--414"}
{"text": "To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model\u2019s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model\u2019s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model\u2019s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model\u2019s probabilities only answers 48% at 80% accuracy.", "tokens": ["To", "avoid", "giving", "wrong", "answers", ",", "question", "answering", "(", "QA", ")", "models", "need", "to", "know", "when", "to", "abstain", "from", "answering", ".", "Moreover", ",", "users", "often", "ask", "questions", "that", "diverge", "from", "the", "model", "\u2019s", "training", "data", ",", "making", "errors", "more", "likely", "and", "thus", "abstention", "more", "critical", ".", "In", "this", "work", ",", "we", "propose", "the", "setting", "of", "selective", "question", "answering", "under", "domain", "shift", ",", "in", "which", "a", "QA", "model", "is", "tested", "on", "a", "mixture", "of", "in-domain", "and", "out-of-domain", "data", ",", "and", "must", "answer", "(", "i.e.", ",", "not", "abstain", "on", ")", "as", "many", "questions", "as", "possible", "while", "maintaining", "high", "accuracy", ".", "Abstention", "policies", "based", "solely", "on", "the", "model", "\u2019s", "softmax", "probabilities", "fare", "poorly", ",", "since", "models", "are", "overconfident", "on", "out-of-domain", "inputs", ".", "Instead", ",", "we", "train", "a", "calibrator", "to", "identify", "inputs", "on", "which", "the", "QA", "model", "errs", ",", "and", "abstain", "when", "it", "predicts", "an", "error", "is", "likely", ".", "Crucially", ",", "the", "calibrator", "benefits", "from", "observing", "the", "model", "\u2019s", "behavior", "on", "out-of-domain", "data", ",", "even", "if", "from", "a", "different", "domain", "than", "the", "test", "data", ".", "We", "combine", "this", "method", "with", "a", "SQuAD-trained", "QA", "model", "and", "evaluate", "on", "mixtures", "of", "SQuAD", "and", "five", "other", "QA", "datasets", ".", "Our", "method", "answers", "56", "%", "of", "questions", "while", "maintaining", "80", "%", "accuracy", ";", "in", "contrast", ",", "directly", "using", "the", "model", "\u2019s", "probabilities", "only", "answers", "48", "%", "at", "80", "%", "accuracy", "."], "entities": [{"type": "Operation", "start": 53, "end": 61, "text": "setting of selective question answering under domain shift", "sent_idx": 2}, {"type": "Effect", "start": 203, "end": 204, "text": "accuracy", "sent_idx": 7}, {"type": "Operation", "start": 208, "end": 214, "text": "directly using the model\u2019s probabilities", "sent_idx": 7}, {"type": "Effect", "start": 221, "end": 222, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--503"}
{"text": "This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER.", "tokens": ["This", "paper", "presents", "Pyramid", ",", "a", "novel", "layered", "model", "for", "Nested", "Named", "Entity", "Recognition", "(", "nested", "NER", ")", ".", "In", "our", "approach", ",", "token", "or", "text", "region", "embeddings", "are", "recursively", "inputted", "into", "L", "flat", "NER", "layers", ",", "from", "bottom", "to", "top", ",", "stacked", "in", "a", "pyramid", "shape", ".", "Each", "time", "an", "embedding", "passes", "through", "a", "layer", "of", "the", "pyramid", ",", "its", "length", "is", "reduced", "by", "one", ".", "Its", "hidden", "state", "at", "layer", "l", "represents", "an", "l-gram", "in", "the", "input", "text", ",", "which", "is", "labeled", "only", "if", "its", "corresponding", "text", "region", "represents", "a", "complete", "entity", "mention", ".", "We", "also", "design", "an", "inverse", "pyramid", "to", "allow", "bidirectional", "interaction", "between", "layers", ".", "The", "proposed", "method", "achieves", "state-of-the-art", "F1", "scores", "in", "nested", "NER", "on", "ACE-2004", ",", "ACE-2005", ",", "GENIA", ",", "and", "NNE", ",", "which", "are", "80.27", ",", "79.42", ",", "77.78", ",", "and", "93.70", "with", "conventional", "embeddings", ",", "and", "87.74", ",", "86.34", ",", "79.31", ",", "and", "94.68", "with", "pre-trained", "contextualized", "embeddings", ".", "In", "addition", ",", "our", "model", "can", "be", "used", "for", "the", "more", "general", "task", "of", "Overlapping", "Named", "Entity", "Recognition", ".", "A", "preliminary", "experiment", "confirms", "the", "effectiveness", "of", "our", "method", "in", "overlapping", "NER", "."], "entities": [{"type": "Operation", "start": 7, "end": 9, "text": "layered model", "sent_idx": 0}, {"type": "Effect", "start": 114, "end": 116, "text": "F1 scores", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--525"}
{"text": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.", "tokens": ["State-of-the-art", "parameter-efficient", "fine-tuning", "methods", "rely", "on", "introducing", "adapter", "modules", "between", "the", "layers", "of", "a", "pretrained", "language", "model", ".", "However", ",", "such", "modules", "are", "trained", "separately", "for", "each", "task", "and", "thus", "do", "not", "enable", "sharing", "information", "across", "tasks", ".", "In", "this", "paper", ",", "we", "show", "that", "we", "can", "learn", "adapter", "parameters", "for", "all", "layers", "and", "tasks", "by", "generating", "them", "using", "shared", "hypernetworks", ",", "which", "condition", "on", "task", ",", "adapter", "position", ",", "and", "layer", "i", "d", "in", "a", "transformer", "model", ".", "This", "parameter-efficient", "multi-task", "learning", "framework", "allows", "us", "to", "achieve", "the", "best", "of", "both", "worlds", "by", "sharing", "knowledge", "across", "tasks", "via", "hypernetworks", "while", "enabling", "the", "model", "to", "adapt", "to", "each", "individual", "task", "through", "task-specific", "adapters", ".", "Experiments", "on", "the", "well-known", "GLUE", "benchmark", "show", "improved", "performance", "in", "multi-task", "learning", "while", "adding", "only", "0.29", "%", "parameters", "per", "task", ".", "We", "additionally", "demonstrate", "substantial", "performance", "improvements", "in", "few-shot", "domain", "generalization", "across", "a", "variety", "of", "tasks", ".", "Our", "code", "is", "publicly", "available", "in", "https://github.com/rabeehk/hyperformer", "."], "entities": [{"type": "Operation", "start": 47, "end": 55, "text": "learn adapter parameters for all layers and tasks", "sent_idx": 2}, {"type": "Effect", "start": 144, "end": 145, "text": "generalization", "sent_idx": 5}, {"type": "Effect", "start": 131, "end": 132, "text": "parameters", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--47"}
{"text": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.", "tokens": ["Neural", "machine", "translation", "(", "NMT", ")", "has", "proven", "to", "be", "facilitated", "by", "curriculum", "learning", "which", "presents", "examples", "in", "an", "easy-to-hard", "order", "at", "different", "training", "stages", ".", "The", "keys", "lie", "in", "the", "assessment", "of", "data", "difficulty", "and", "model", "competence", ".", "We", "propose", "uncertainty-aware", "curriculum", "learning", ",", "which", "is", "motivated", "by", "the", "intuition", "that", ":", "1", ")", "the", "higher", "the", "uncertainty", "in", "a", "translation", "pair", ",", "the", "more", "complex", "and", "rarer", "the", "information", "it", "contains", ";", "and", "2", ")", "the", "end", "of", "the", "decline", "in", "model", "uncertainty", "indicates", "the", "completeness", "of", "current", "training", "stage", ".", "Specifically", ",", "we", "serve", "cross-entropy", "of", "an", "example", "as", "its", "data", "difficulty", "and", "exploit", "the", "variance", "of", "distributions", "over", "the", "weights", "of", "the", "network", "to", "present", "the", "model", "uncertainty", ".", "Extensive", "experiments", "on", "various", "translation", "tasks", "reveal", "that", "our", "approach", "outperforms", "the", "strong", "baseline", "and", "related", "methods", "on", "both", "translation", "quality", "and", "convergence", "speed", ".", "Quantitative", "analyses", "reveal", "that", "the", "proposed", "strategy", "offers", "NMT", "the", "ability", "to", "automatically", "govern", "its", "learning", "schedule", "."], "entities": [{"type": "Operation", "start": 41, "end": 44, "text": "uncertainty-aware curriculum learning", "sent_idx": 2}, {"type": "Effect", "start": 142, "end": 144, "text": "translation quality", "sent_idx": 4}, {"type": "Effect", "start": 145, "end": 147, "text": "convergence speed", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--620"}
{"text": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.", "tokens": ["In", "this", "paper", ",", "we", "propose", "a", "multi-granularity", "interaction", "network", "for", "extractive", "and", "abstractive", "multi-document", "summarization", ",", "which", "jointly", "learn", "semantic", "representations", "for", "words", ",", "sentences", ",", "and", "documents", ".", "The", "word", "representations", "are", "used", "to", "generate", "an", "abstractive", "summary", "while", "the", "sentence", "representations", "are", "used", "to", "produce", "an", "extractive", "summary", ".", "We", "employ", "attention", "mechanisms", "to", "interact", "between", "different", "granularity", "of", "semantic", "representations", ",", "which", "helps", "to", "capture", "multi-granularity", "key", "information", "and", "improves", "the", "performance", "of", "both", "abstractive", "and", "extractive", "summarization", ".", "Experiment", "results", "show", "that", "our", "proposed", "model", "substantially", "outperforms", "all", "strong", "baseline", "methods", "and", "achieves", "the", "best", "results", "on", "the", "Multi-News", "dataset", "."], "entities": [{"type": "Operation", "start": 53, "end": 64, "text": "employ attention mechanisms to interact between different granularity of semantic representations", "sent_idx": 2}, {"type": "Effect", "start": 75, "end": 76, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--556"}
{"text": "The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional class-based approaches, it produces human-interpretable classes describing each relation's preferences, but it is competitive with non-class-based methods in predictive power. \n \nWe compare LDA-SP to several state-of-the-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP's effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.'s system (Pantel et al., 2007).", "tokens": ["The", "computation", "of", "selectional", "preferences", ",", "the", "admissible", "argument", "values", "for", "a", "relation", ",", "is", "a", "well-known", "NLP", "task", "with", "broad", "applicability", ".", "We", "present", "LDA-SP", ",", "which", "utilizes", "LinkLDA", "(", "Erosheva", "et", "al.", ",", "2004", ")", "to", "model", "selectional", "preferences", ".", "By", "simultaneously", "inferring", "latent", "topics", "and", "topic", "distributions", "over", "relations", ",", "LDA-SP", "combines", "the", "benefits", "of", "previous", "approaches", ":", "like", "traditional", "class-based", "approaches", ",", "it", "produces", "human-interpretable", "classes", "describing", "each", "relation", "'s", "preferences", ",", "but", "it", "is", "competitive", "with", "non-class-based", "methods", "in", "predictive", "power", ".", "\n \n", "We", "compare", "LDA-SP", "to", "several", "state-of-the-art", "methods", "achieving", "an", "85", "%", "increase", "in", "recall", "at", "0.9", "precision", "over", "mutual", "information", "(", "Erk", ",", "2007", ")", ".", "We", "also", "evaluate", "LDA-SP", "'s", "effectiveness", "at", "filtering", "improper", "applications", "of", "inference", "rules", ",", "where", "we", "show", "substantial", "improvement", "over", "Pantel", "et", "al.", "'s", "system", "(", "Pantel", "et", "al.", ",", "2007", ")", "."], "entities": [{"type": "Operation", "start": 90, "end": 91, "text": "LDA-SP", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 102, "text": "recall", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1044"}
{"text": "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F 1 score, property generation model achieves a respectable F 1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.", "tokens": ["CommonsenseQA", "(", "CQA", ")", "(", "Talmor", "et", "al.", ",", "2019", ")", "dataset", "was", "recently", "released", "to", "advance", "the", "research", "on", "common-sense", "question", "answering", "(", "QA", ")", "task", ".", "Whereas", "the", "prior", "work", "has", "mostly", "focused", "on", "proposing", "QA", "models", "for", "this", "dataset", ",", "our", "aim", "is", "to", "retrieve", "as", "well", "as", "generate", "explanation", "for", "a", "given", "(", "question", ",", "correct", "answer", "choice", ",", "incorrect", "answer", "choices", ")", "tuple", "from", "this", "dataset", ".", "Our", "explanation", "definition", "is", "based", "on", "certain", "desiderata", ",", "and", "translates", "an", "explanation", "into", "a", "set", "of", "positive", "and", "negative", "common-sense", "properties", "(", "aka", "facts", ")", "which", "not", "only", "explain", "the", "correct", "answer", "choice", "but", "also", "refute", "the", "incorrect", "ones", ".", "We", "human-annotate", "a", "first-of-its-kind", "dataset", "(", "called", "ECQA", ")", "of", "positive", "and", "negative", "properties", ",", "as", "well", "as", "free-flow", "explanations", ",", "for", "11", "K", "QA", "pairs", "taken", "from", "the", "CQA", "dataset", ".", "We", "propose", "a", "latent", "representation", "based", "property", "retrieval", "model", "as", "well", "as", "a", "GPT-2", "based", "property", "generation", "model", "with", "a", "novel", "two", "step", "fine-tuning", "procedure", ".", "We", "also", "propose", "a", "free-flow", "explanation", "generation", "model", ".", "Extensive", "experiments", "show", "that", "our", "retrieval", "model", "beats", "BM25", "baseline", "by", "a", "relative", "gain", "of", "100", "%", "in", "F", "1", "score", ",", "property", "generation", "model", "achieves", "a", "respectable", "F", "1", "score", "of", "36.4", ",", "and", "free-flow", "generation", "model", "achieves", "a", "similarity", "score", "of", "61.9", ",", "where", "last", "two", "scores", "are", "based", "on", "a", "human", "correlated", "semantic", "similarity", "metric", "."], "entities": [{"type": "Operation", "start": 185, "end": 187, "text": "retrieval model", "sent_idx": 6}, {"type": "Effect", "start": 198, "end": 201, "text": "F 1 score", "sent_idx": 6}, {"type": "Effect", "start": 220, "end": 222, "text": "similarity score", "sent_idx": 6}, {"type": "Operation", "start": 158, "end": 163, "text": "GPT-2 based property generation model", "sent_idx": 4}, {"type": "Operation", "start": 148, "end": 154, "text": "latent representation based property retrieval model", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 1}, {"type": "Pos_Affect", "head": 4, "tail": 1}, {"type": "Pos_Affect", "head": 4, "tail": 2}], "id": "abstract-2021--acl-long--238"}
{"text": "Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-the-art approaches.", "tokens": ["Visual", "question", "answering", "aims", "to", "answer", "the", "natural", "language", "question", "about", "a", "given", "image", ".", "Existing", "graph-based", "methods", "only", "focus", "on", "the", "relations", "between", "objects", "in", "an", "image", "and", "neglect", "the", "importance", "of", "the", "syntactic", "dependency", "relations", "between", "words", "in", "a", "question", ".", "To", "simultaneously", "capture", "the", "relations", "between", "objects", "in", "an", "image", "and", "the", "syntactic", "dependency", "relations", "between", "words", "in", "a", "question", ",", "we", "propose", "a", "novel", "dual", "channel", "graph", "convolutional", "network", "(", "DC-GCN", ")", "for", "better", "combining", "visual", "and", "textual", "advantages", ".", "The", "DC-GCN", "model", "consists", "of", "three", "parts", ":", "an", "I-GCN", "module", "to", "capture", "the", "relations", "between", "objects", "in", "an", "image", ",", "a", "Q-GCN", "module", "to", "capture", "the", "syntactic", "dependency", "relations", "between", "words", "in", "a", "question", ",", "and", "an", "attention", "alignment", "module", "to", "align", "image", "representations", "and", "question", "representations", ".", "Experimental", "results", "show", "that", "our", "model", "achieves", "comparable", "performance", "with", "the", "state-of-the-art", "approaches", "."], "entities": [{"type": "Operation", "start": 68, "end": 76, "text": "dual channel graph convolutional network (DC-GCN)", "sent_idx": 2}, {"type": "Effect", "start": 141, "end": 142, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--642"}
{"text": "With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both semantics and neighborhood information are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model. To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning. Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.", "tokens": ["With", "the", "need", "of", "fast", "retrieval", "speed", "and", "small", "memory", "footprint", ",", "document", "hashing", "has", "been", "playing", "a", "crucial", "role", "in", "large-scale", "information", "retrieval", ".", "To", "generate", "high-quality", "hashing", "code", ",", "both", "semantics", "and", "neighborhood", "information", "are", "crucial", ".", "However", ",", "most", "existing", "methods", "leverage", "only", "one", "of", "them", "or", "simply", "combine", "them", "via", "some", "intuitive", "criteria", ",", "lacking", "a", "theoretical", "principle", "to", "guide", "the", "integration", "process", ".", "In", "this", "paper", ",", "we", "encode", "the", "neighborhood", "information", "with", "a", "graph-induced", "Gaussian", "distribution", ",", "and", "propose", "to", "integrate", "the", "two", "types", "of", "information", "with", "a", "graph-driven", "generative", "model", ".", "To", "deal", "with", "the", "complicated", "correlations", "among", "documents", ",", "we", "further", "propose", "a", "tree-structured", "approximation", "method", "for", "learning", ".", "Under", "the", "approximation", ",", "we", "prove", "that", "the", "training", "objective", "can", "be", "decomposed", "into", "terms", "involving", "only", "singleton", "or", "pairwise", "documents", ",", "enabling", "the", "model", "to", "be", "trained", "as", "efficiently", "as", "uncorrelated", "ones", ".", "Extensive", "experimental", "results", "on", "three", "benchmark", "datasets", "show", "that", "our", "method", "achieves", "superior", "performance", "over", "state-of-the-art", "methods", ",", "demonstrating", "the", "effectiveness", "of", "the", "proposed", "model", "for", "simultaneously", "preserving", "semantic", "and", "neighborhood", "information", "."], "entities": [{"type": "Operation", "start": 86, "end": 97, "text": "integrate the two types of information with a graph-driven generative model", "sent_idx": 3}, {"type": "Effect", "start": 164, "end": 165, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--174"}
{"text": "In online debates, users express different levels of agreement/disagreement with one another\u2019s arguments and ideas. Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. Existing stance detection methods predict the polarity of a post\u2019s stance toward a topic or post, but don\u2019t consider the stance\u2019s degree of intensity. We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding. Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis. We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation. These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform. The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596. This work is the first to train models for predicting a post\u2019s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.", "tokens": ["In", "online", "debates", ",", "users", "express", "different", "levels", "of", "agreement/disagreement", "with", "one", "another", "\u2019s", "arguments", "and", "ideas", ".", "Often", "levels", "of", "agreement/disagreement", "are", "implicit", "in", "the", "text", ",", "and", "must", "be", "predicted", "to", "analyze", "collective", "opinions", ".", "Existing", "stance", "detection", "methods", "predict", "the", "polarity", "of", "a", "post", "\u2019s", "stance", "toward", "a", "topic", "or", "post", ",", "but", "do", "n\u2019t", "consider", "the", "stance", "\u2019s", "degree", "of", "intensity", ".", "We", "introduce", "a", "new", "research", "problem", ",", "stance", "polarity", "and", "intensity", "prediction", "in", "response", "relationships", "between", "posts", ".", "This", "problem", "is", "challenging", "because", "differences", "in", "stance", "intensity", "are", "often", "subtle", "and", "require", "nuanced", "language", "understanding", ".", "Cyber", "argumentation", "research", "has", "shown", "that", "incorporating", "both", "stance", "polarity", "and", "intensity", "data", "in", "online", "debates", "leads", "to", "better", "discussion", "analysis", ".", "We", "explore", "five", "different", "learning", "models", ":", "Ridge-M", "regression", ",", "Ridge-S", "regression", ",", "SVR-RF-R", ",", "pkudblab-PIP", ",", "and", "T-PAN-PIP", "for", "predicting", "stance", "polarity", "and", "intensity", "in", "argumentation", ".", "These", "models", "are", "evaluated", "using", "a", "new", "dataset", "for", "stance", "polarity", "and", "intensity", "prediction", "collected", "using", "a", "cyber", "argumentation", "platform", ".", "The", "SVR-RF-R", "model", "performs", "best", "for", "prediction", "of", "stance", "polarity", "with", "an", "accuracy", "of", "70.43", "%", "and", "intensity", "with", "RMSE", "of", "0.596", ".", "This", "work", "is", "the", "first", "to", "train", "models", "for", "predicting", "a", "post", "\u2019s", "stance", "polarity", "and", "intensity", "in", "one", "combined", "value", "in", "cyber", "argumentation", "with", "reasonably", "good", "accuracy", "."], "entities": [{"type": "Operation", "start": 174, "end": 176, "text": "SVR-RF-R model", "sent_idx": 8}, {"type": "Effect", "start": 185, "end": 186, "text": "accuracy", "sent_idx": 8}, {"type": "Effect", "start": 192, "end": 193, "text": "RMSE", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--509"}
{"text": "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.", "tokens": ["The", "goal", "of", "database", "question", "answering", "is", "to", "enable", "natural", "language", "querying", "of", "real-life", "relational", "databases", "in", "diverse", "application", "domains", ".", "Recently", ",", "large-scale", "datasets", "such", "as", "Spider", "and", "WikiSQL", "facilitated", "novel", "modeling", "techniques", "for", "text-to-SQL", "parsing", ",", "improving", "zero-shot", "generalization", "to", "unseen", "databases", ".", "In", "this", "work", ",", "we", "examine", "the", "challenges", "that", "still", "prevent", "these", "techniques", "from", "practical", "deployment", ".", "First", ",", "we", "present", "KaggleDBQA", ",", "a", "new", "cross-domain", "evaluation", "dataset", "of", "real", "Web", "databases", ",", "with", "domain-specific", "data", "types", ",", "original", "formatting", ",", "and", "unrestricted", "questions", ".", "Second", ",", "we", "re-examine", "the", "choice", "of", "evaluation", "tasks", "for", "text-to-SQL", "parsers", "as", "applied", "in", "real-life", "settings", ".", "Finally", ",", "we", "augment", "our", "in-domain", "evaluation", "task", "with", "database", "documentation", ",", "a", "naturally", "occurring", "source", "of", "implicit", "domain", "knowledge", ".", "We", "show", "that", "KaggleDBQA", "presents", "a", "challenge", "to", "state-of-the-art", "zero-shot", "parsers", "but", "a", "more", "realistic", "evaluation", "setting", "and", "creative", "use", "of", "associated", "database", "documentation", "boosts", "their", "accuracy", "by", "over", "13.2", "%", ",", "doubling", "their", "performance", "."], "entities": [{"type": "Operation", "start": 144, "end": 146, "text": "evaluation setting", "sent_idx": 6}, {"type": "Effect", "start": 155, "end": 156, "text": "accuracy", "sent_idx": 6}, {"type": "Operation", "start": 147, "end": 153, "text": "creative use of associated database documentation", "sent_idx": 6}, {"type": "Effect", "start": 163, "end": 164, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--176"}
{"text": "This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro\u2194En and De\u2194En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).", "tokens": ["This", "paper", "proposes", "a", "simple", "and", "effective", "approach", "to", "address", "the", "problem", "of", "posterior", "collapse", "in", "conditional", "variational", "autoencoders", "(", "CVAEs", ")", ".", "It", "thus", "improves", "performance", "of", "machine", "translation", "models", "that", "use", "noisy", "or", "monolingual", "data", ",", "as", "well", "as", "in", "conventional", "settings", ".", "Extending", "Transformer", "and", "conditional", "VAEs", ",", "our", "proposed", "latent", "variable", "model", "measurably", "prevents", "posterior", "collapse", "by", "(", "1", ")", "using", "a", "modified", "evidence", "lower", "bound", "(", "ELBO", ")", "objective", "which", "promotes", "mutual", "information", "between", "the", "latent", "variable", "and", "the", "target", ",", "and", "(", "2", ")", "guiding", "the", "latent", "variable", "with", "an", "auxiliary", "bag-of-words", "prediction", "task", ".", "As", "a", "result", ",", "the", "proposed", "model", "yields", "improved", "translation", "quality", "compared", "to", "existing", "variational", "NMT", "models", "on", "WMT", "Ro\u2194En", "and", "De\u2194En", ".", "With", "latent", "variables", "being", "effectively", "utilized", ",", "our", "model", "demonstrates", "improved", "robustness", "over", "non-latent", "Transformer", "in", "handling", "uncertainty", ":", "exploiting", "noisy", "source-side", "monolingual", "data", "(", "up", "to", "+", "3.2", "BLEU", ")", ",", "and", "training", "with", "weakly", "aligned", "web-mined", "parallel", "data", "(", "up", "to", "+", "4.7", "BLEU", ")", "."], "entities": [{"type": "Operation", "start": 125, "end": 127, "text": "latent variables", "sent_idx": 4}, {"type": "Effect", "start": 169, "end": 170, "text": "BLEU", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--753"}
{"text": "Resolving coordination ambiguity is a classic hard problem. This paper looks at co-ordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don't do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training. The co-trained classifier achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations.", "tokens": ["Resolving", "coordination", "ambiguity", "is", "a", "classic", "hard", "problem", ".", "This", "paper", "looks", "at", "co-ordination", "disambiguation", "in", "complex", "noun", "phrases", "(", "NPs", ")", ".", "Parsers", "trained", "on", "the", "Penn", "Treebank", "are", "reporting", "impressive", "numbers", "these", "days", ",", "but", "they", "do", "n't", "do", "very", "well", "on", "this", "problem", "(", "79", "%", ")", ".", "We", "explore", "systems", "trained", "using", "three", "types", "of", "corpora", ":", "(", "1", ")", "annotated", "(", "e.g.", "the", "Penn", "Treebank", ")", ",", "(", "2", ")", "bitexts", "(", "e.g.", "Europarl", ")", ",", "and", "(", "3", ")", "unannotated", "monolingual", "(", "e.g.", "Google", "N-grams", ")", ".", "Size", "matters", ":", "(", "1", ")", "is", "a", "million", "words", ",", "(", "2", ")", "is", "potentially", "billions", "of", "words", "and", "(", "3", ")", "is", "potentially", "trillions", "of", "words", ".", "The", "unannotated", "monolingual", "data", "is", "helpful", "when", "the", "ambiguity", "can", "be", "resolved", "through", "associations", "among", "the", "lexical", "items", ".", "The", "bilingual", "data", "is", "helpful", "when", "the", "ambiguity", "can", "be", "resolved", "by", "the", "order", "of", "words", "in", "the", "translation", ".", "We", "train", "separate", "classifiers", "with", "monolingual", "and", "bilingual", "features", "and", "iteratively", "improve", "them", "via", "co-training", ".", "The", "co-trained", "classifier", "achieves", "close", "to", "96", "%", "accuracy", "on", "Treebank", "data", "and", "makes", "20", "%", "fewer", "errors", "than", "a", "supervised", "system", "trained", "with", "Treebank", "annotations", "."], "entities": [{"type": "Operation", "start": 178, "end": 180, "text": "co-trained classifier", "sent_idx": 8}, {"type": "Effect", "start": 185, "end": 186, "text": "accuracy", "sent_idx": 8}, {"type": "Effect", "start": 194, "end": 195, "text": "errors", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "P11-1135"}
{"text": "Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.", "tokens": ["Maintaining", "a", "consistent", "persona", "is", "essential", "for", "dialogue", "agents", ".", "Although", "tremendous", "advancements", "have", "been", "brought", ",", "the", "limited-scale", "of", "annotated", "personalized", "dialogue", "datasets", "is", "still", "a", "barrier", "towards", "training", "robust", "and", "consistent", "persona-based", "dialogue", "models", ".", "This", "work", "shows", "how", "this", "challenge", "can", "be", "addressed", "by", "disentangling", "persona-based", "dialogue", "generation", "into", "two", "sub-tasks", "with", "a", "novel", "BERT-over-BERT", "(", "BoB", ")", "model", ".", "Specifically", ",", "the", "model", "consists", "of", "a", "BERT-based", "encoder", "and", "two", "BERT-based", "decoders", ",", "where", "one", "decoder", "is", "for", "response", "generation", ",", "and", "another", "is", "for", "consistency", "understanding", ".", "In", "particular", ",", "to", "learn", "the", "ability", "of", "consistency", "understanding", "from", "large-scale", "non-dialogue", "inference", "data", ",", "we", "train", "the", "second", "decoder", "in", "an", "unlikelihood", "manner", ".", "Under", "different", "limited", "data", "settings", ",", "both", "automatic", "and", "human", "evaluations", "demonstrate", "that", "the", "proposed", "model", "outperforms", "strong", "baselines", "in", "response", "quality", "and", "persona", "consistency", "."], "entities": [{"type": "Operation", "start": 47, "end": 54, "text": "disentangling persona-based dialogue generation into two sub-tasks", "sent_idx": 2}, {"type": "Effect", "start": 138, "end": 140, "text": "response quality", "sent_idx": 5}, {"type": "Effect", "start": 141, "end": 143, "text": "persona consistency", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--14"}
{"text": "Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.", "tokens": ["Speech", "translation", "(", "ST", ")", "aims", "to", "learn", "transformations", "from", "speech", "in", "the", "source", "language", "to", "the", "text", "in", "the", "target", "language", ".", "Previous", "works", "show", "that", "multitask", "learning", "improves", "the", "ST", "performance", ",", "in", "which", "the", "recognition", "decoder", "generates", "the", "text", "of", "the", "source", "language", ",", "and", "the", "translation", "decoder", "obtains", "the", "final", "translations", "based", "on", "the", "output", "of", "the", "recognition", "decoder", ".", "Because", "whether", "the", "output", "of", "the", "recognition", "decoder", "has", "the", "correct", "semantics", "is", "more", "critical", "than", "its", "accuracy", ",", "we", "propose", "to", "improve", "the", "multitask", "ST", "model", "by", "utilizing", "word", "embedding", "as", "the", "intermediate", "."], "entities": [{"type": "Operation", "start": 27, "end": 29, "text": "multitask learning", "sent_idx": 1}, {"type": "Effect", "start": 32, "end": 33, "text": "performance", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--533"}
{"text": "Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.", "tokens": ["Unsupervised", "commonsense", "question", "answering", "is", "appealing", "since", "it", "does", "not", "rely", "on", "any", "labeled", "task", "data", ".", "Among", "existing", "work", ",", "a", "popular", "solution", "is", "to", "use", "pre-trained", "language", "models", "to", "score", "candidate", "choices", "directly", "conditioned", "on", "the", "question", "or", "context", ".", "However", ",", "such", "scores", "from", "language", "models", "can", "be", "easily", "affected", "by", "irrelevant", "factors", ",", "such", "as", "word", "frequencies", ",", "sentence", "structures", ",", "etc", ".", "These", "distracting", "factors", "may", "not", "only", "mislead", "the", "model", "to", "choose", "a", "wrong", "answer", "but", "also", "make", "it", "oversensitive", "to", "lexical", "perturbations", "in", "candidate", "answers", ".", "In", "this", "paper", ",", "we", "present", "a", "novel", "SEmantic-based", "Question", "Answering", "method", "(", "SEQA", ")", "for", "unsupervised", "commonsense", "question", "answering", ".", "Instead", "of", "directly", "scoring", "each", "answer", "choice", ",", "our", "method", "first", "generates", "a", "set", "of", "plausible", "answers", "with", "generative", "models", "(", "e.g.", ",", "GPT-2", ")", ",", "and", "then", "uses", "these", "plausible", "answers", "to", "select", "the", "correct", "choice", "by", "considering", "the", "semantic", "similarity", "between", "each", "plausible", "answer", "and", "each", "choice", ".", "We", "devise", "a", "simple", ",", "yet", "sound", "formalism", "for", "this", "idea", "and", "verify", "its", "effectiveness", "and", "robustness", "with", "extensive", "experiments", ".", "We", "evaluate", "the", "proposed", "method", "on", "four", "benchmark", "datasets", ",", "and", "our", "method", "achieves", "the", "best", "results", "in", "unsupervised", "settings", ".", "Moreover", ",", "when", "attacked", "by", "TextFooler", "with", "synonym", "replacement", ",", "SEQA", "demonstrates", "much", "less", "performance", "drops", "than", "baselines", ",", "thereby", "indicating", "stronger", "robustness", "."], "entities": [{"type": "Operation", "start": 216, "end": 217, "text": "SEQA", "sent_idx": 8}, {"type": "Effect", "start": 220, "end": 221, "text": "performance", "sent_idx": 8}, {"type": "Effect", "start": 228, "end": 229, "text": "robustness", "sent_idx": 8}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--237"}
{"text": "Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.", "tokens": ["Entity", "linking", "(", "EL", ")", "is", "the", "task", "of", "disambiguating", "mentions", "appearing", "in", "text", "by", "linking", "them", "to", "entities", "in", "a", "knowledge", "graph", ",", "a", "crucial", "task", "for", "text", "understanding", ",", "question", "answering", "or", "conversational", "systems", ".", "In", "the", "special", "case", "of", "short-text", "EL", ",", "which", "poses", "additional", "challenges", "due", "to", "limited", "context", ",", "prior", "approaches", "have", "reached", "good", "performance", "by", "employing", "heuristics-based", "methods", "or", "purely", "neural", "approaches", ".", "Here", ",", "we", "take", "a", "different", ",", "neuro-symbolic", "approach", "that", "combines", "the", "advantages", "of", "using", "interpretable", "rules", "based", "on", "first-order", "logic", "with", "the", "performance", "of", "neural", "learning", ".", "Even", "though", "constrained", "to", "use", "rules", ",", "we", "show", "that", "we", "reach", "competitive", "or", "better", "performance", "with", "SoTA", "black-box", "neural", "approaches", ".", "Furthermore", ",", "our", "framework", "has", "the", "benefits", "of", "extensibility", "and", "transferability", ".", "We", "show", "that", "we", "can", "easily", "blend", "existing", "rule", "templates", "given", "by", "a", "human", "expert", ",", "with", "multiple", "types", "of", "features", "(", "priors", ",", "BERT", "encodings", ",", "box", "embeddings", ",", "etc", ")", ",", "and", "even", "with", "scores", "resulting", "from", "previous", "EL", "methods", ",", "thus", "improving", "on", "such", "methods", ".", "As", "an", "example", "of", "improvement", ",", "on", "the", "LC-QuAD-1.0", "dataset", ",", "we", "show", "more", "than", "3", "%", "increase", "in", "F1", "score", "relative", "to", "previous", "SoTA", ".", "Finally", ",", "we", "show", "that", "the", "inductive", "bias", "offered", "by", "using", "logic", "results", "in", "a", "set", "of", "learned", "rules", "that", "transfers", "from", "one", "dataset", "to", "another", ",", "sometimes", "without", "finetuning", ",", "while", "still", "having", "high", "accuracy", "."], "entities": [{"type": "Operation", "start": 76, "end": 78, "text": "neuro-symbolic approach", "sent_idx": 2}, {"type": "Effect", "start": 199, "end": 201, "text": "F1 score", "sent_idx": 6}, {"type": "Operation", "start": 212, "end": 225, "text": "inductive bias offered by using logic results in a set of learned rules", "sent_idx": 7}, {"type": "Effect", "start": 241, "end": 242, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--64"}
{"text": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.", "tokens": ["A", "central", "problem", "in", "historical", "linguistics", "is", "the", "identification", "of", "historically", "related", "cognate", "words", ".", "We", "present", "a", "generative", "phylogenetic", "model", "for", "automatically", "inducing", "cognate", "group", "structure", "from", "unaligned", "word", "lists", ".", "Our", "model", "represents", "the", "process", "of", "transformation", "and", "transmission", "from", "ancestor", "word", "to", "daughter", "word", ",", "as", "well", "as", "the", "alignment", "between", "the", "words", "lists", "of", "the", "observed", "languages", ".", "We", "also", "present", "a", "novel", "method", "for", "simplifying", "complex", "weighted", "automata", "created", "during", "inference", "to", "counteract", "the", "otherwise", "exponential", "growth", "of", "message", "sizes", ".", "On", "the", "task", "of", "identifying", "cognates", "in", "a", "dataset", "of", "Romance", "words", ",", "our", "model", "significantly", "outperforms", "a", "baseline", "approach", ",", "increasing", "accuracy", "by", "as", "much", "as", "80", "%", ".", "Finally", ",", "we", "demonstrate", "that", "our", "automatically", "induced", "groups", "can", "be", "used", "to", "successfully", "reconstruct", "ancestral", "words", "."], "entities": [{"type": "Operation", "start": 17, "end": 21, "text": "a generative phylogenetic model", "sent_idx": 1}, {"type": "Effect", "start": 108, "end": 109, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1105"}
{"text": "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.", "tokens": ["Recent", "evidence", "reveals", "that", "Neural", "Machine", "Translation", "(", "NMT", ")", "models", "with", "deeper", "neural", "networks", "can", "be", "more", "effective", "but", "are", "difficult", "to", "train", ".", "In", "this", "paper", ",", "we", "present", "a", "MultiScale", "Collaborative", "(", "MSC", ")", "framework", "to", "ease", "the", "training", "of", "NMT", "models", "that", "are", "substantially", "deeper", "than", "those", "used", "previously", ".", "We", "explicitly", "boost", "the", "gradient", "back-propagation", "from", "top", "to", "bottom", "levels", "by", "introducing", "a", "block-scale", "collaboration", "mechanism", "into", "deep", "NMT", "models", ".", "Then", ",", "instead", "of", "forcing", "the", "whole", "encoder", "stack", "directly", "learns", "a", "desired", "representation", ",", "we", "let", "each", "encoder", "block", "learns", "a", "fine-grained", "representation", "and", "enhance", "it", "by", "encoding", "spatial", "dependencies", "using", "a", "context-scale", "collaboration", ".", "We", "provide", "empirical", "evidence", "showing", "that", "the", "MSC", "nets", "are", "easy", "to", "optimize", "and", "can", "obtain", "improvements", "of", "translation", "quality", "from", "considerably", "increased", "depth", ".", "On", "IWSLT", "translation", "tasks", "with", "three", "translation", "directions", ",", "our", "extremely", "deep", "models", "(", "with", "72-layer", "encoders", ")", "surpass", "strong", "baselines", "by", "+", "2.2~+3.1", "BLEU", "points", ".", "In", "addition", ",", "our", "deep", "MSC", "achieves", "a", "BLEU", "score", "of", "30.56", "on", "WMT14", "English-to-German", "task", "that", "significantly", "outperforms", "state-of-the-art", "deep", "NMT", "models", ".", "We", "have", "included", "the", "source", "code", "in", "supplementary", "materials", "."], "entities": [{"type": "Operation", "start": 147, "end": 155, "text": "extremely deep models (with 72-layer encoders)", "sent_idx": 5}, {"type": "Effect", "start": 161, "end": 163, "text": "BLEU points", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--40"}
{"text": "The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.", "tokens": ["The", "connection", "between", "the", "maximum", "spanning", "tree", "in", "a", "directed", "graph", "and", "the", "best", "dependency", "tree", "of", "a", "sentence", "has", "been", "exploited", "by", "the", "NLP", "community", ".", "However", ",", "for", "many", "dependency", "parsing", "schemes", ",", "an", "important", "detail", "of", "this", "approach", "is", "that", "the", "spanning", "tree", "must", "have", "exactly", "one", "edge", "emanating", "from", "the", "root", ".", "While", "work", "has", "been", "done", "to", "efficiently", "solve", "this", "problem", "for", "finding", "the", "one-best", "dependency", "tree", ",", "no", "research", "has", "attempted", "to", "extend", "this", "solution", "to", "finding", "the", "K-best", "dependency", "trees", ".", "This", "is", "arguably", "a", "more", "important", "extension", "as", "a", "larger", "proportion", "of", "decoded", "trees", "will", "not", "be", "subject", "to", "the", "root", "constraint", "of", "dependency", "trees", ".", "Indeed", ",", "we", "show", "that", "the", "rate", "of", "root", "constraint", "violations", "increases", "by", "an", "average", "of", "13", "times", "when", "decoding", "with", "K=50", "as", "opposed", "to", "K=1", ".", "In", "this", "paper", ",", "we", "provide", "a", "simplification", "of", "the", "K-best", "spanning", "tree", "algorithm", "of", "Camerini", "et", "al.", "(", "1980", ")", ".", "Our", "simplification", "allows", "us", "to", "obtain", "a", "constant", "time", "speed-up", "over", "the", "original", "algorithm", ".", "Furthermore", ",", "we", "present", "a", "novel", "extension", "of", "the", "algorithm", "for", "decoding", "the", "K-best", "dependency", "trees", "of", "a", "graph", "which", "are", "subject", "to", "a", "root", "constraint", "."], "entities": [{"type": "Operation", "start": 148, "end": 155, "text": "simplification of the K-best spanning tree algorithm", "sent_idx": 5}, {"type": "Effect", "start": 172, "end": 173, "text": "speed-up", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--106"}
{"text": "We present a new human-human dialogue dataset - PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% F1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community.", "tokens": ["We", "present", "a", "new", "human-human", "dialogue", "dataset", "-", "PhotoChat", ",", "the", "first", "dataset", "that", "casts", "light", "on", "the", "photo", "sharing", "behavior", "in", "online", "messaging", ".", "PhotoChat", "contains", "12k", "dialogues", ",", "each", "of", "which", "is", "paired", "with", "a", "user", "photo", "that", "is", "shared", "during", "the", "conversation", ".", "Based", "on", "this", "dataset", ",", "we", "propose", "two", "tasks", "to", "facilitate", "research", "on", "image-text", "modeling", ":", "a", "photo-sharing", "intent", "prediction", "task", "that", "predicts", "whether", "one", "intends", "to", "share", "a", "photo", "in", "the", "next", "conversation", "turn", ",", "and", "a", "photo", "retrieval", "task", "that", "retrieves", "the", "most", "relevant", "photo", "according", "to", "the", "dialogue", "context", ".", "In", "addition", ",", "for", "both", "tasks", ",", "we", "provide", "baseline", "models", "using", "the", "state-of-the-art", "models", "and", "report", "their", "benchmark", "performances", ".", "The", "best", "image", "retrieval", "model", "achieves", "10.4", "%", "recall@1", "(", "out", "of", "1000", "candidates", ")", "and", "the", "best", "photo", "intent", "prediction", "model", "achieves", "58.1", "%", "F1", "score", ",", "indicating", "that", "the", "dataset", "presents", "interesting", "yet", "challenging", "real-world", "problems", ".", "We", "are", "releasing", "PhotoChat", "to", "facilitate", "future", "research", "work", "among", "the", "community", "."], "entities": [{"type": "Operation", "start": 122, "end": 125, "text": "image retrieval model", "sent_idx": 4}, {"type": "Effect", "start": 128, "end": 129, "text": "recall@1", "sent_idx": 4}, {"type": "Effect", "start": 145, "end": 147, "text": "F1 score", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--479"}
{"text": "Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.", "tokens": ["Streaming", "cross", "document", "entity", "coreference", "(", "CDC", ")", "systems", "disambiguate", "mentions", "of", "named", "entities", "in", "a", "scalable", "manner", "via", "incremental", "clustering", ".", "Unlike", "other", "approaches", "for", "named", "entity", "disambiguation", "(", "e.g.", ",", "entity", "linking", ")", ",", "streaming", "CDC", "allows", "for", "the", "disambiguation", "of", "entities", "that", "are", "unknown", "at", "inference", "time", ".", "Thus", ",", "it", "is", "well-suited", "for", "processing", "streams", "of", "data", "where", "new", "entities", "are", "frequently", "introduced", ".", "Despite", "these", "benefits", ",", "this", "task", "is", "currently", "difficult", "to", "study", ",", "as", "existing", "approaches", "are", "either", "evaluated", "on", "datasets", "that", "are", "no", "longer", "available", ",", "or", "omit", "other", "crucial", "details", "needed", "to", "ensure", "fair", "comparison", ".", "In", "this", "work", ",", "we", "address", "this", "issue", "by", "compiling", "a", "large", "benchmark", "adapted", "from", "existing", "free", "datasets", ",", "and", "performing", "a", "comprehensive", "evaluation", "of", "a", "number", "of", "novel", "and", "existing", "baseline", "models", ".", "We", "investigate", ":", "how", "to", "best", "encode", "mentions", ",", "which", "clustering", "algorithms", "are", "most", "effective", "for", "grouping", "mentions", ",", "how", "models", "transfer", "to", "different", "domains", ",", "and", "how", "bounding", "the", "number", "of", "mentions", "tracked", "during", "inference", "impacts", "performance", ".", "Our", "results", "show", "that", "the", "relative", "performance", "of", "neural", "and", "feature-based", "mention", "encoders", "varies", "across", "different", "domains", ",", "and", "in", "most", "cases", "the", "best", "performance", "is", "achieved", "using", "a", "combination", "of", "both", "approaches", ".", "We", "also", "find", "that", "performance", "is", "minimally", "impacted", "by", "limiting", "the", "number", "of", "tracked", "mentions", "."], "entities": [{"type": "Operation", "start": 221, "end": 227, "text": "limiting the number of tracked mentions", "sent_idx": 7}, {"type": "Effect", "start": 216, "end": 217, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--364"}
{"text": "Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.", "tokens": ["Weakly", "supervised", "question", "answering", "usually", "has", "only", "the", "final", "answers", "as", "supervision", "signals", "while", "the", "correct", "solutions", "to", "derive", "the", "answers", "are", "not", "provided", ".", "This", "setting", "gives", "rise", "to", "the", "spurious", "solution", "problem", ":", "there", "may", "exist", "many", "spurious", "solutions", "that", "coincidentally", "derive", "the", "correct", "answer", ",", "but", "training", "on", "such", "solutions", "can", "hurt", "model", "performance", "(", "e.g.", ",", "producing", "wrong", "solutions", "or", "answers", ")", ".", "For", "example", ",", "for", "discrete", "reasoning", "tasks", "as", "on", "DROP", ",", "there", "may", "exist", "many", "equations", "to", "derive", "a", "numeric", "answer", ",", "and", "typically", "only", "one", "of", "them", "is", "correct", ".", "Previous", "learning", "methods", "mostly", "filter", "out", "spurious", "solutions", "with", "heuristics", "or", "using", "model", "confidence", ",", "but", "do", "not", "explicitly", "exploit", "the", "semantic", "correlations", "between", "a", "question", "and", "its", "solution", ".", "In", "this", "paper", ",", "to", "alleviate", "the", "spurious", "solution", "problem", ",", "we", "propose", "to", "explicitly", "exploit", "such", "semantic", "correlations", "by", "maximizing", "the", "mutual", "information", "between", "question-answer", "pairs", "and", "predicted", "solutions", ".", "Extensive", "experiments", "on", "four", "question", "answering", "datasets", "show", "that", "our", "method", "significantly", "outperforms", "previous", "learning", "methods", "in", "terms", "of", "task", "performance", "and", "is", "more", "effective", "in", "training", "models", "to", "produce", "correct", "solutions", "."], "entities": [{"type": "Operation", "start": 148, "end": 158, "text": "maximizing the mutual information between question-answer pairs and predicted solutions", "sent_idx": 4}, {"type": "Effect", "start": 179, "end": 180, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--318"}
{"text": "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait- k  strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.", "tokens": ["In", "this", "work", ",", "we", "develop", "SimulSpeech", ",", "an", "end-to-end", "simultaneous", "speech", "to", "text", "translation", "system", "which", "translates", "speech", "in", "source", "language", "to", "text", "in", "target", "language", "concurrently", ".", "SimulSpeech", "consists", "of", "a", "speech", "encoder", ",", "a", "speech", "segmenter", "and", "a", "text", "decoder", ",", "where", "1", ")", "the", "segmenter", "builds", "upon", "the", "encoder", "and", "leverages", "a", "connectionist", "temporal", "classification", "(", "CTC", ")", "loss", "to", "split", "the", "input", "streaming", "speech", "in", "real", "time", ",", "2", ")", "the", "encoder-decoder", "attention", "adopts", "a", "wait-", "k", " ", "strategy", "for", "simultaneous", "translation", ".", "SimulSpeech", "is", "more", "challenging", "than", "previous", "cascaded", "systems", "(", "with", "simultaneous", "automatic", "speech", "recognition", "(", "ASR", ")", "and", "simultaneous", "neural", "machine", "translation", "(", "NMT", ")", ")", ".", "We", "introduce", "two", "novel", "knowledge", "distillation", "methods", "to", "ensure", "the", "performance", ":", "1", ")", "Attention-level", "knowledge", "distillation", "transfers", "the", "knowledge", "from", "the", "multiplication", "of", "the", "attention", "matrices", "of", "simultaneous", "NMT", "and", "ASR", "models", "to", "help", "the", "training", "of", "the", "attention", "mechanism", "in", "SimulSpeech", ";", "2", ")", "Data-level", "knowledge", "distillation", "transfers", "the", "knowledge", "from", "the", "full-sentence", "NMT", "model", "and", "also", "reduces", "the", "complexity", "of", "data", "distribution", "to", "help", "on", "the", "optimization", "of", "SimulSpeech", ".", "Experiments", "on", "MuST-C", "English-Spanish", "and", "English-German", "spoken", "language", "translation", "datasets", "show", "that", "SimulSpeech", "achieves", "reasonable", "BLEU", "scores", "and", "lower", "delay", "compared", "to", "full-sentence", "end-to-end", "speech", "to", "text", "translation", "(", "without", "simultaneous", "translation", ")", ",", "and", "better", "performance", "than", "the", "two-stage", "cascaded", "simultaneous", "translation", "model", "in", "terms", "of", "BLEU", "scores", "and", "translation", "delay", "."], "entities": [{"type": "Operation", "start": 200, "end": 201, "text": "SimulSpeech", "sent_idx": 4}, {"type": "Effect", "start": 203, "end": 205, "text": "BLEU scores", "sent_idx": 4}, {"type": "Effect", "start": 207, "end": 208, "text": "delay", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--350"}
{"text": "Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.", "tokens": ["Recently", ",", "many", "works", "have", "tried", "to", "augment", "the", "performance", "of", "Chinese", "named", "entity", "recognition", "(", "NER", ")", "using", "word", "lexicons", ".", "As", "a", "representative", ",", "Lattice-LSTM", "has", "achieved", "new", "benchmark", "results", "on", "several", "public", "Chinese", "NER", "datasets", ".", "However", ",", "Lattice-LSTM", "has", "a", "complex", "model", "architecture", ".", "This", "limits", "its", "application", "in", "many", "industrial", "areas", "where", "real-time", "NER", "responses", "are", "needed", ".", "In", "this", "work", ",", "we", "propose", "a", "simple", "but", "effective", "method", "for", "incorporating", "the", "word", "lexicon", "into", "the", "character", "representations", ".", "This", "method", "avoids", "designing", "a", "complicated", "sequence", "modeling", "architecture", ",", "and", "for", "any", "neural", "NER", "model", ",", "it", "requires", "only", "subtle", "adjustment", "of", "the", "character", "representation", "layer", "to", "introduce", "the", "lexicon", "information", ".", "Experimental", "studies", "on", "four", "benchmark", "Chinese", "NER", "datasets", "show", "that", "our", "method", "achieves", "an", "inference", "speed", "up", "to", "6.15", "times", "faster", "than", "those", "of", "state-of-the-art", "methods", ",", "along", "with", "a", "better", "performance", ".", "The", "experimental", "results", "also", "show", "that", "the", "proposed", "method", "can", "be", "easily", "incorporated", "with", "pre-trained", "models", "like", "BERT", "."], "entities": [{"type": "Operation", "start": 75, "end": 83, "text": "incorporating the word lexicon into the character representations", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 133, "text": "inference speed", "sent_idx": 6}, {"type": "Effect", "start": 148, "end": 149, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--528"}
{"text": "We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user/item explainability. Our experiments illustrate that ESCOFILT\u2019s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types.", "tokens": ["We", "pioneer", "the", "first", "extractive", "summarization-based", "collaborative", "filtering", "model", "called", "ESCOFILT", ".", "Our", "proposed", "model", "specifically", "produces", "extractive", "summaries", "for", "each", "item", "and", "user", ".", "Unlike", "other", "types", "of", "explanations", ",", "summary-level", "explanations", "closely", "resemble", "real-life", "explanations", ".", "The", "strength", "of", "ESCOFILT", "lies", "in", "the", "fact", "that", "it", "unifies", "representation", "and", "explanation", ".", "In", "other", "words", ",", "extractive", "summaries", "both", "represent", "and", "explain", "the", "items", "and", "users", ".", "Our", "model", "uniquely", "integrates", "BERT", ",", "K-Means", "embedding", "clustering", ",", "and", "multilayer", "perceptron", "to", "learn", "sentence", "embeddings", ",", "representation-explanations", ",", "and", "user-item", "interactions", ",", "respectively", ".", "We", "argue", "that", "our", "approach", "enhances", "both", "rating", "prediction", "accuracy", "and", "user/item", "explainability", ".", "Our", "experiments", "illustrate", "that", "ESCOFILT", "\u2019s", "prediction", "accuracy", "is", "better", "than", "the", "other", "state-of-the-art", "recommender", "models", ".", "Furthermore", ",", "we", "propose", "a", "comprehensive", "set", "of", "criteria", "that", "assesses", "the", "real-life", "explainability", "of", "explanations", ".", "Our", "explainability", "study", "demonstrates", "the", "superiority", "of", "and", "preference", "for", "summary-level", "explanations", "over", "other", "explanation", "types", "."], "entities": [{"type": "Operation", "start": 4, "end": 9, "text": "extractive summarization-based collaborative filtering model", "sent_idx": 0}, {"type": "Effect", "start": 115, "end": 116, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--232"}
{"text": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.", "tokens": ["The", "ability", "to", "control", "for", "the", "kinds", "of", "information", "encoded", "in", "neural", "representation", "has", "a", "variety", "of", "use", "cases", ",", "especially", "in", "light", "of", "the", "challenge", "of", "interpreting", "these", "models", ".", "We", "present", "Iterative", "Null-space", "Projection", "(", "INLP", ")", ",", "a", "novel", "method", "for", "removing", "information", "from", "neural", "representations", ".", "Our", "method", "is", "based", "on", "repeated", "training", "of", "linear", "classifiers", "that", "predict", "a", "certain", "property", "we", "aim", "to", "remove", ",", "followed", "by", "projection", "of", "the", "representations", "on", "their", "null-space", ".", "By", "doing", "so", ",", "the", "classifiers", "become", "oblivious", "to", "that", "target", "property", ",", "making", "it", "hard", "to", "linearly", "separate", "the", "data", "according", "to", "it", ".", "While", "applicable", "for", "multiple", "uses", ",", "we", "evaluate", "our", "method", "on", "bias", "and", "fairness", "use-cases", ",", "and", "show", "that", "our", "method", "is", "able", "to", "mitigate", "bias", "in", "word", "embeddings", ",", "as", "well", "as", "to", "increase", "fairness", "in", "a", "setting", "of", "multi-class", "classification", "."], "entities": [{"type": "Operation", "start": 33, "end": 39, "text": "Iterative Null-space Projection (INLP)", "sent_idx": 1}, {"type": "Effect", "start": 130, "end": 131, "text": "bias", "sent_idx": 4}, {"type": "Effect", "start": 140, "end": 141, "text": "fairness", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--647"}
{"text": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their probability distributions. We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction. Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy. We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.", "tokens": ["Suspense", "is", "a", "crucial", "ingredient", "of", "narrative", "fiction", ",", "engaging", "readers", "and", "making", "stories", "compelling", ".", "While", "there", "is", "a", "vast", "theoretical", "literature", "on", "suspense", ",", "it", "is", "computationally", "not", "well", "understood", ".", "We", "compare", "two", "ways", "for", "modelling", "suspense", ":", "surprise", ",", "a", "backward-looking", "measure", "of", "how", "unexpected", "the", "current", "state", "is", "given", "the", "story", "so", "far", ";", "and", "uncertainty", "reduction", ",", "a", "forward-looking", "measure", "of", "how", "unexpected", "the", "continuation", "of", "the", "story", "is", ".", "Both", "can", "be", "computed", "either", "directly", "over", "story", "representations", "or", "over", "their", "probability", "distributions", ".", "We", "propose", "a", "hierarchical", "language", "model", "that", "encodes", "stories", "and", "computes", "surprise", "and", "uncertainty", "reduction", ".", "Evaluating", "against", "short", "stories", "annotated", "with", "human", "suspense", "judgements", ",", "we", "find", "that", "uncertainty", "reduction", "over", "representations", "is", "the", "best", "predictor", ",", "resulting", "in", "near", "human", "accuracy", ".", "We", "also", "show", "that", "uncertainty", "reduction", "can", "be", "used", "to", "predict", "suspenseful", "events", "in", "movie", "synopses", "."], "entities": [{"type": "Operation", "start": 120, "end": 124, "text": "uncertainty reduction over representations", "sent_idx": 5}, {"type": "Effect", "start": 133, "end": 134, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--161"}
{"text": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 \u2013> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.", "tokens": ["Large", "pre-trained", "language", "models", "(", "LMs", ")", "are", "known", "to", "encode", "substantial", "amounts", "of", "linguistic", "information", ".", "However", ",", "high-level", "reasoning", "skills", ",", "such", "as", "numerical", "reasoning", ",", "are", "difficult", "to", "learn", "from", "a", "language-modeling", "objective", "only", ".", "Consequently", ",", "existing", "models", "for", "numerical", "reasoning", "have", "used", "specialized", "architectures", "with", "limited", "flexibility", ".", "In", "this", "work", ",", "we", "show", "that", "numerical", "reasoning", "is", "amenable", "to", "automatic", "data", "generation", ",", "and", "thus", "one", "can", "inject", "this", "skill", "into", "pre-trained", "LMs", ",", "by", "generating", "large", "amounts", "of", "data", ",", "and", "training", "in", "a", "multi-task", "setup", ".", "We", "show", "that", "pre-training", "our", "model", ",", "GenBERT", ",", "on", "this", "data", ",", "dramatically", "improves", "performance", "on", "DROP", "(", "49.3", "\u2013", ">", "72.3", "F1", ")", ",", "reaching", "performance", "that", "matches", "state-of-the-art", "models", "of", "comparable", "size", ",", "while", "using", "a", "simple", "and", "general-purpose", "encoder-decoder", "architecture", ".", "Moreover", ",", "GenBERT", "generalizes", "well", "to", "math", "word", "problem", "datasets", ",", "while", "maintaining", "high", "performance", "on", "standard", "RC", "tasks", ".", "Our", "approach", "provides", "a", "general", "recipe", "for", "injecting", "skills", "into", "large", "pre-trained", "LMs", ",", "whenever", "the", "skill", "is", "amenable", "to", "automatic", "data", "augmentation", "."], "entities": [{"type": "Operation", "start": 141, "end": 142, "text": "GenBERT", "sent_idx": 5}, {"type": "Effect", "start": 153, "end": 154, "text": "performance", "sent_idx": 5}, {"type": "Operation", "start": 97, "end": 100, "text": "pre-training our model", "sent_idx": 4}, {"type": "Effect", "start": 109, "end": 110, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--89"}
{"text": "Not all documents are equally important. Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. In this paper, we present a novel model that uses message-level attention to learn the relative weight of users\u2019 social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%). In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.", "tokens": ["Not", "all", "documents", "are", "equally", "important", ".", "Language", "processing", "is", "increasingly", "finding", "use", "as", "a", "supplement", "for", "questionnaires", "to", "assess", "psychological", "attributes", "of", "consenting", "individuals", ",", "but", "most", "approaches", "neglect", "to", "consider", "whether", "all", "documents", "of", "an", "individual", "are", "equally", "informative", ".", "In", "this", "paper", ",", "we", "present", "a", "novel", "model", "that", "uses", "message-level", "attention", "to", "learn", "the", "relative", "weight", "of", "users", "\u2019", "social", "media", "posts", "for", "assessing", "their", "five", "factor", "personality", "traits", ".", "We", "demonstrate", "that", "models", "with", "message-level", "attention", "outperform", "those", "with", "word-level", "attention", ",", "and", "ultimately", "yield", "state-of-the-art", "accuracies", "for", "all", "five", "traits", "by", "using", "both", "word", "and", "message", "attention", "in", "combination", "with", "past", "approaches", "(", "an", "average", "increase", "in", "Pearson", "r", "of", "2.5", "%", ")", ".", "In", "addition", ",", "examination", "of", "the", "high-signal", "posts", "identified", "by", "our", "model", "provides", "insight", "into", "the", "relationship", "between", "language", "and", "personality", ",", "helping", "to", "inform", "future", "work", "."], "entities": [{"type": "Operation", "start": 97, "end": 103, "text": "using both word and message attention", "sent_idx": 3}, {"type": "Effect", "start": 91, "end": 92, "text": "accuracies", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--472"}
{"text": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.", "tokens": ["Named", "Entity", "Recognition", "(", "NER", ")", "is", "the", "task", "of", "identifying", "spans", "that", "represent", "entities", "in", "sentences", ".", "Whether", "the", "entity", "spans", "are", "nested", "or", "discontinuous", ",", "the", "NER", "task", "can", "be", "categorized", "into", "the", "flat", "NER", ",", "nested", "NER", ",", "and", "discontinuous", "NER", "subtasks", ".", "These", "subtasks", "have", "been", "mainly", "solved", "by", "the", "token-level", "sequence", "labelling", "or", "span-level", "classification", ".", "However", ",", "these", "solutions", "can", "hardly", "tackle", "the", "three", "kinds", "of", "NER", "subtasks", "concurrently", ".", "To", "that", "end", ",", "we", "propose", "to", "formulate", "the", "NER", "subtasks", "as", "an", "entity", "span", "sequence", "generation", "task", ",", "which", "can", "be", "solved", "by", "a", "unified", "sequence-to-sequence", "(", "Seq2Seq", ")", "framework", ".", "Based", "on", "our", "unified", "framework", ",", "we", "can", "leverage", "the", "pre-trained", "Seq2Seq", "model", "to", "solve", "all", "three", "kinds", "of", "NER", "subtasks", "without", "the", "special", "design", "of", "the", "tagging", "schema", "or", "ways", "to", "enumerate", "spans", ".", "We", "exploit", "three", "types", "of", "entity", "representations", "to", "linearize", "entities", "into", "a", "sequence", ".", "Our", "proposed", "framework", "is", "easy-to-implement", "and", "achieves", "state-of-the-art", "(", "SoTA", ")", "or", "near", "SoTA", "performance", "on", "eight", "English", "NER", "datasets", ",", "including", "two", "flat", "NER", "datasets", ",", "three", "nested", "NER", "datasets", ",", "and", "three", "discontinuous", "NER", "datasets", "."], "entities": [{"type": "Operation", "start": 83, "end": 94, "text": "formulate the NER subtasks as an entity span sequence generation task", "sent_idx": 4}, {"type": "Effect", "start": 171, "end": 172, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--451"}
{"text": "Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from N-best lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.", "tokens": ["Automatic", "error", "detection", "is", "desired", "in", "the", "post-processing", "to", "improve", "machine", "translation", "quality", ".", "The", "previous", "work", "is", "largely", "based", "on", "confidence", "estimation", "using", "system-based", "features", ",", "such", "as", "word", "posterior", "probabilities", "calculated", "from", "N-best", "lists", "or", "word", "lattices", ".", "We", "propose", "to", "incorporate", "two", "groups", "of", "linguistic", "features", ",", "which", "convey", "information", "from", "outside", "machine", "translation", "systems", ",", "into", "error", "detection", ":", "lexical", "and", "syntactic", "features", ".", "We", "use", "a", "maximum", "entropy", "classifier", "to", "predict", "translation", "errors", "by", "integrating", "word", "posterior", "probability", "feature", "and", "linguistic", "features", ".", "The", "experimental", "results", "show", "that", "1", ")", "linguistic", "features", "alone", "outperform", "word", "posterior", "probability", "based", "confidence", "estimation", "in", "error", "detection", ";", "and", "2", ")", "linguistic", "features", "can", "further", "provide", "complementary", "information", "when", "combined", "with", "word", "confidence", "scores", ",", "which", "collectively", "reduce", "the", "classification", "error", "rate", "by", "18.52", "%", "and", "improve", "the", "F", "measure", "by", "16.37", "%", "."], "entities": [{"type": "Operation", "start": 112, "end": 114, "text": "linguistic features", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 133, "text": "error rate", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1062"}
{"text": "Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.", "tokens": ["Naively", "collecting", "translations", "by", "crowd-sourcing", "the", "task", "to", "non-professional", "translators", "yields", "disfluent", ",", "low-quality", "results", "if", "no", "quality", "control", "is", "exercised", ".", "We", "demonstrate", "a", "variety", "of", "mechanisms", "that", "increase", "the", "translation", "quality", "to", "near", "professional", "levels", ".", "Specifically", ",", "we", "solicit", "redundant", "translations", "and", "edits", "to", "them", ",", "and", "automatically", "select", "the", "best", "output", "among", "them", ".", "We", "propose", "a", "set", "of", "features", "that", "model", "both", "the", "translations", "and", "the", "translators", ",", "such", "as", "country", "of", "residence", ",", "LM", "perplexity", "of", "the", "translation", ",", "edit", "rate", "from", "the", "other", "translations", ",", "and", "(", "optionally", ")", "calibration", "against", "professional", "translators", ".", "Using", "these", "features", "to", "score", "the", "collected", "translations", ",", "we", "are", "able", "to", "discriminate", "between", "acceptable", "and", "unacceptable", "translations", ".", "We", "recreate", "the", "NIST", "2009", "Urdu-to-English", "evaluation", "set", "with", "Mechanical", "Turk", ",", "and", "quantitatively", "show", "that", "our", "models", "are", "able", "to", "select", "translations", "within", "the", "range", "of", "quality", "that", "we", "expect", "from", "professional", "translators", ".", "The", "total", "cost", "is", "more", "than", "an", "order", "of", "magnitude", "lower", "than", "professional", "translation", "."], "entities": [{"type": "Operation", "start": 24, "end": 28, "text": "a variety of mechanisms", "sent_idx": 1}, {"type": "Effect", "start": 31, "end": 33, "text": "translation quality", "sent_idx": 1}, {"type": "Operation", "start": 60, "end": 72, "text": "a set of features that model both the translations and the translators", "sent_idx": 3}, {"type": "Effect", "start": 157, "end": 159, "text": "total cost", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 3}], "id": "P11-1122"}
{"text": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.", "tokens": ["Document-level", "relation", "extraction", "requires", "integrating", "information", "within", "and", "across", "multiple", "sentences", "of", "a", "document", "and", "capturing", "complex", "interactions", "between", "inter-sentence", "entities", ".", "However", ",", "effective", "aggregation", "of", "relevant", "information", "in", "the", "document", "remains", "a", "challenging", "research", "question", ".", "Existing", "approaches", "construct", "static", "document-level", "graphs", "based", "on", "syntactic", "trees", ",", "co-references", "or", "heuristics", "from", "the", "unstructured", "text", "to", "model", "the", "dependencies", ".", "Unlike", "previous", "methods", "that", "may", "not", "be", "able", "to", "capture", "rich", "non-local", "interactions", "for", "inference", ",", "we", "propose", "a", "novel", "model", "that", "empowers", "the", "relational", "reasoning", "across", "sentences", "by", "automatically", "inducing", "the", "latent", "document-level", "graph", ".", "We", "further", "develop", "a", "refinement", "strategy", ",", "which", "enables", "the", "model", "to", "incrementally", "aggregate", "relevant", "information", "for", "multi-hop", "reasoning", ".", "Specifically", ",", "our", "model", "achieves", "an", "F1", "score", "of", "59.05", "on", "a", "large-scale", "document-level", "dataset", "(", "DocRED", ")", ",", "significantly", "improving", "over", "the", "previous", "results", ",", "and", "also", "yields", "new", "state-of-the-art", "results", "on", "the", "CDR", "and", "GDA", "dataset", ".", "Furthermore", ",", "extensive", "analyses", "show", "that", "the", "model", "is", "able", "to", "discover", "more", "accurate", "inter-sentence", "relations", "."], "entities": [{"type": "Operation", "start": 83, "end": 96, "text": "empowers the relational reasoning across sentences by automatically inducing the latent document-level graph", "sent_idx": 3}, {"type": "Effect", "start": 123, "end": 125, "text": "F1 score", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--141"}
{"text": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Net work (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.", "tokens": ["We", "consider", "the", "problem", "of", "collectively", "detecting", "multiple", "events", ",", "particularly", "in", "cross-sentence", "settings", ".", "The", "key", "to", "dealing", "with", "the", "problem", "is", "to", "encode", "semantic", "information", "and", "model", "event", "inter-dependency", "at", "a", "document-level", ".", "In", "this", "paper", ",", "we", "reformulate", "it", "as", "a", "Seq2Seq", "task", "and", "propose", "a", "Multi-Layer", "Bidirectional", "Net", "work", "(", "MLBiNet", ")", "to", "capture", "the", "document-level", "association", "of", "events", "and", "semantic", "information", "simultaneously", ".", "Specifically", ",", "a", "bidirectional", "decoder", "is", "firstly", "devised", "to", "model", "event", "inter-dependency", "within", "a", "sentence", "when", "decoding", "the", "event", "tag", "vector", "sequence", ".", "Secondly", ",", "an", "information", "aggregation", "module", "is", "employed", "to", "aggregate", "sentence-level", "semantic", "and", "event", "tag", "information", ".", "Finally", ",", "we", "stack", "multiple", "bidirectional", "decoders", "and", "feed", "cross-sentence", "information", ",", "forming", "a", "multi-layer", "bidirectional", "tagging", "architecture", "to", "iteratively", "propagate", "information", "across", "sentences", ".", "We", "show", "that", "our", "approach", "provides", "significant", "improvement", "in", "performance", "compared", "to", "the", "current", "state-of-the-art", "results", "."], "entities": [{"type": "Operation", "start": 49, "end": 56, "text": "Multi-Layer Bidirectional Net work (MLBiNet)", "sent_idx": 2}, {"type": "Effect", "start": 142, "end": 143, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--373"}
{"text": "In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.", "tokens": ["In", "this", "paper", ",", "we", "observe", "that", "there", "exists", "a", "second", "dimension", "to", "the", "relation", "extraction", "(", "RE", ")", "problem", "that", "is", "orthogonal", "to", "the", "relation", "type", "dimension", ".", "We", "show", "that", "most", "of", "these", "second", "dimensional", "structures", "are", "relatively", "constrained", "and", "not", "difficult", "to", "identify", ".", "We", "propose", "a", "novel", "algorithmic", "approach", "to", "RE", "that", "starts", "by", "first", "identifying", "these", "structures", "and", "then", ",", "within", "these", ",", "identifying", "the", "semantic", "type", "of", "the", "relation", ".", "In", "the", "real", "RE", "problem", "where", "relation", "arguments", "need", "to", "be", "identified", ",", "exploiting", "these", "structures", "also", "allows", "reducing", "pipelined", "propagated", "errors", ".", "We", "show", "that", "this", "RE", "framework", "provides", "significant", "improvement", "in", "RE", "performance", "."], "entities": [{"type": "Operation", "start": 103, "end": 105, "text": "RE framework", "sent_idx": 4}, {"type": "Effect", "start": 110, "end": 111, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1056"}
{"text": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a  graph autoencoder  (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.", "tokens": ["Document", "clustering", "requires", "a", "deep", "understanding", "of", "the", "complex", "structure", "of", "long-text", ";", "in", "particular", ",", "the", "intra-sentential", "(", "local", ")", "and", "inter-sentential", "features", "(", "global", ")", ".", "Existing", "representation", "learning", "models", "do", "not", "fully", "capture", "these", "features", ".", "To", "address", "this", ",", "we", "present", "a", "novel", "graph-based", "representation", "for", "document", "clustering", "that", "builds", "a", " ", "graph", "autoencoder", " ", "(", "GAE", ")", "on", "a", "Keyword", "Correlation", "Graph", ".", "The", "graph", "is", "constructed", "with", "topical", "keywords", "as", "nodes", "and", "multiple", "local", "and", "global", "features", "as", "edges", ".", "A", "GAE", "is", "employed", "to", "aggregate", "the", "two", "sets", "of", "features", "by", "learning", "a", "latent", "representation", "which", "can", "jointly", "reconstruct", "them", ".", "Clustering", "is", "then", "performed", "on", "the", "learned", "representations", ",", "using", "vector", "dimensions", "as", "features", "for", "inducing", "document", "classes", ".", "Extensive", "experiments", "on", "two", "datasets", "show", "that", "the", "features", "learned", "by", "our", "approach", "can", "achieve", "better", "clustering", "performance", "than", "other", "existing", "features", ",", "including", "term", "frequency-inverse", "document", "frequency", "and", "average", "embedding", "."], "entities": [{"type": "Operation", "start": 53, "end": 67, "text": "builds a  graph autoencoder  (GAE) on a Keyword Correlation Graph", "sent_idx": 2}, {"type": "Effect", "start": 144, "end": 145, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--366"}
{"text": "Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their representations using each other\u2019s representations through attention. In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences\u2019 Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives.", "tokens": ["Argument", "pair", "extraction", "(", "APE", ")", "is", "a", "research", "task", "for", "extracting", "arguments", "from", "two", "passages", "and", "identifying", "potential", "argument", "pairs", ".", "Prior", "research", "work", "treats", "this", "task", "as", "a", "sequence", "labeling", "problem", "and", "a", "binary", "classification", "problem", "on", "two", "passages", "that", "are", "directly", "concatenated", "together", ",", "which", "has", "a", "limitation", "of", "not", "fully", "utilizing", "the", "unique", "characteristics", "and", "inherent", "relations", "of", "two", "different", "passages", ".", "This", "paper", "proposes", "a", "novel", "attention-guided", "multi-layer", "multi-cross", "encoding", "scheme", "to", "address", "the", "challenges", ".", "The", "new", "model", "processes", "two", "passages", "with", "two", "individual", "sequence", "encoders", "and", "updates", "their", "representations", "using", "each", "other", "\u2019s", "representations", "through", "attention", ".", "In", "addition", ",", "the", "pair", "prediction", "part", "is", "formulated", "as", "a", "table-filling", "problem", "by", "updating", "the", "representations", "of", "two", "sequences", "\u2019", "Cartesian", "product", ".", "Furthermore", ",", "an", "auxiliary", "attention", "loss", "is", "introduced", "to", "guide", "each", "argument", "to", "align", "to", "its", "paired", "argument", ".", "An", "extensive", "set", "of", "experiments", "show", "that", "the", "new", "model", "significantly", "improves", "the", "APE", "performance", "over", "several", "alternatives", "."], "entities": [{"type": "Operation", "start": 71, "end": 76, "text": "attention-guided multi-layer multi-cross encoding scheme", "sent_idx": 2}, {"type": "Effect", "start": 161, "end": 162, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--496"}
{"text": "Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.", "tokens": ["Neural", "generative", "models", "have", "achieved", "promising", "performance", "on", "dialog", "generation", "tasks", "if", "given", "a", "huge", "data", "set", ".", "However", ",", "the", "lack", "of", "high-quality", "dialog", "data", "and", "the", "expensive", "data", "annotation", "process", "greatly", "limit", "their", "application", "in", "real", "world", "settings", ".", "We", "propose", "a", "paraphrase", "augmented", "response", "generation", "(", "PARG", ")", "framework", "that", "jointly", "trains", "a", "paraphrase", "model", "and", "a", "response", "generation", "model", "to", "improve", "the", "dialog", "generation", "performance", ".", "We", "also", "design", "a", "method", "to", "automatically", "construct", "paraphrase", "training", "data", "set", "based", "on", "dialog", "state", "and", "dialog", "act", "labels", ".", "PARG", "is", "applicable", "to", "various", "dialog", "generation", "models", ",", "such", "as", "TSCP", "(", "Lei", "et", "al.", ",", "2018", ")", "and", "DAMD", "(", "Zhang", "et", "al.", ",", "2019", ")", ".", "Experimental", "results", "show", "that", "the", "proposed", "framework", "improves", "these", "state-of-the-art", "dialog", "models", "further", "on", "CamRest676", "and", "MultiWOZ", ".", "PARG", "also", "outperforms", "other", "data", "augmentation", "methods", "significantly", "in", "dialog", "generation", "tasks", ",", "especially", "under", "low", "resource", "settings", "."], "entities": [{"type": "Operation", "start": 44, "end": 52, "text": "paraphrase augmented response generation (PARG) framework", "sent_idx": 2}, {"type": "Effect", "start": 68, "end": 69, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--60"}
{"text": "Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1% and incorporating CoCLR brings a further improvement of 10.5%.", "tokens": ["Finding", "codes", "given", "natural", "language", "query", "is", "beneficial", "to", "the", "productivity", "of", "software", "developers", ".", "Future", "progress", "towards", "better", "semantic", "matching", "between", "query", "and", "code", "requires", "richer", "supervised", "training", "resources", ".", "To", "remedy", "this", ",", "we", "introduce", "CoSQA", "dataset", ".", "It", "includes", "20,604", "labels", "for", "pairs", "of", "natural", "language", "queries", "and", "codes", ",", "each", "annotated", "by", "at", "least", "3", "human", "annotators", ".", "We", "further", "introduce", "a", "contrastive", "learning", "method", "dubbed", "CoCLR", "to", "enhance", "text-code", "matching", ",", "which", "works", "as", "a", "data", "augmenter", "to", "bring", "more", "artificially", "generated", "training", "instances", ".", "We", "show", "that", ",", "evaluated", "on", "CodeXGLUE", "with", "the", "same", "CodeBERT", "model", ",", "training", "on", "CoSQA", "improves", "the", "accuracy", "of", "code", "question", "answering", "by", "5.1", "%", "and", "incorporating", "CoCLR", "brings", "a", "further", "improvement", "of", "10.5", "%", "."], "entities": [{"type": "Operation", "start": 117, "end": 119, "text": "incorporating CoCLR", "sent_idx": 5}, {"type": "Effect", "start": 108, "end": 109, "text": "accuracy", "sent_idx": 5}, {"type": "Operation", "start": 103, "end": 106, "text": "training on CoSQA", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--442"}
{"text": "Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. Experimental results confirm that our method alleviates the data scarcity problem significantly. It also effectively boosts the performances of several current models for aspect term extraction.", "tokens": ["Aspect", "term", "extraction", "aims", "to", "extract", "aspect", "terms", "from", "review", "texts", "as", "opinion", "targets", "for", "sentiment", "analysis", ".", "One", "of", "the", "big", "challenges", "with", "this", "task", "is", "the", "lack", "of", "sufficient", "annotated", "data", ".", "While", "data", "augmentation", "is", "potentially", "an", "effective", "technique", "to", "address", "the", "above", "issue", ",", "it", "is", "uncontrollable", "as", "it", "may", "change", "aspect", "words", "and", "aspect", "labels", "unexpectedly", ".", "In", "this", "paper", ",", "we", "formulate", "the", "data", "augmentation", "as", "a", "conditional", "generation", "task", ":", "generating", "a", "new", "sentence", "while", "preserving", "the", "original", "opinion", "targets", "and", "labels", ".", "We", "propose", "a", "masked", "sequence-to-sequence", "method", "for", "conditional", "augmentation", "of", "aspect", "term", "extraction", ".", "Unlike", "existing", "augmentation", "approaches", ",", "ours", "is", "controllable", "and", "allows", "to", "generate", "more", "diversified", "sentences", ".", "Experimental", "results", "confirm", "that", "our", "method", "alleviates", "the", "data", "scarcity", "problem", "significantly", ".", "It", "also", "effectively", "boosts", "the", "performances", "of", "several", "current", "models", "for", "aspect", "term", "extraction", "."], "entities": [{"type": "Operation", "start": 93, "end": 96, "text": "masked sequence-to-sequence method", "sent_idx": 4}, {"type": "Effect", "start": 138, "end": 139, "text": "performances", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--631"}
{"text": "Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/GT-SALT/HiddenCut.", "tokens": ["Fine-tuning", "large", "pre-trained", "models", "with", "task-specific", "data", "has", "achieved", "great", "success", "in", "NLP", ".", "However", ",", "it", "has", "been", "demonstrated", "that", "the", "majority", "of", "information", "within", "the", "self-attention", "networks", "is", "redundant", "and", "not", "utilized", "effectively", "during", "the", "fine-tuning", "stage", ".", "This", "leads", "to", "inferior", "results", "when", "generalizing", "the", "obtained", "models", "to", "out-of-domain", "distributions", ".", "To", "this", "end", ",", "we", "propose", "a", "simple", "yet", "effective", "data", "augmentation", "technique", ",", "HiddenCut", ",", "to", "better", "regularize", "the", "model", "and", "encourage", "it", "to", "learn", "more", "generalizable", "features", ".", "Specifically", ",", "contiguous", "spans", "within", "the", "hidden", "space", "are", "dynamically", "and", "strategically", "dropped", "during", "training", ".", "Experiments", "show", "that", "our", "HiddenCut", "method", "outperforms", "the", "state-of-the-art", "augmentation", "methods", "on", "the", "GLUE", "benchmark", ",", "and", "consistently", "exhibits", "superior", "generalization", "performances", "on", "out-of-distribution", "and", "challenging", "counterexamples", ".", "We", "have", "publicly", "released", "our", "code", "at", "https://github.com/GT-SALT/HiddenCut", "."], "entities": [{"type": "Operation", "start": 104, "end": 105, "text": "HiddenCut", "sent_idx": 5}, {"type": "Effect", "start": 120, "end": 121, "text": "generalization", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--338"}
{"text": "One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context. ExCorD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that ExCorD significantly improves the QA models\u2019 performance by up to 1.2 F1 on QuAC, and 5.2 F1 on CANARD, while addressing the limitations of the existing approaches.", "tokens": ["One", "of", "the", "main", "challenges", "in", "conversational", "question", "answering", "(", "CQA", ")", "is", "to", "resolve", "the", "conversational", "dependency", ",", "such", "as", "anaphora", "and", "ellipsis", ".", "However", ",", "existing", "approaches", "do", "not", "explicitly", "train", "QA", "models", "on", "how", "to", "resolve", "the", "dependency", ",", "and", "thus", "these", "models", "are", "limited", "in", "understanding", "human", "dialogues", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "framework", ",", "ExCorD", "(", "Explicit", "guidance", "on", "how", "to", "resolve", "Conversational", "Dependency", ")", "to", "enhance", "the", "abilities", "of", "QA", "models", "in", "comprehending", "conversational", "context", ".", "ExCorD", "first", "generates", "self-contained", "questions", "that", "can", "be", "understood", "without", "the", "conversation", "history", ",", "then", "trains", "a", "QA", "model", "with", "the", "pairs", "of", "original", "and", "self-contained", "questions", "using", "a", "consistency-based", "regularizer", ".", "In", "our", "experiments", ",", "we", "demonstrate", "that", "ExCorD", "significantly", "improves", "the", "QA", "models", "\u2019", "performance", "by", "up", "to", "1.2", "F1", "on", "QuAC", ",", "and", "5.2", "F1", "on", "CANARD", ",", "while", "addressing", "the", "limitations", "of", "the", "existing", "approaches", "."], "entities": [{"type": "Operation", "start": 125, "end": 126, "text": "ExCorD", "sent_idx": 4}, {"type": "Effect", "start": 132, "end": 133, "text": "performance", "sent_idx": 4}, {"type": "Effect", "start": 137, "end": 138, "text": "F1", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--478"}
{"text": "Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.", "tokens": ["Sequence-based", "neural", "networks", "show", "significant", "sensitivity", "to", "syntactic", "structure", ",", "but", "they", "still", "perform", "less", "well", "on", "syntactic", "tasks", "than", "tree-based", "networks", ".", "Such", "tree-based", "networks", "can", "be", "provided", "with", "a", "constituency", "parse", ",", "a", "dependency", "parse", ",", "or", "both", ".", "We", "evaluate", "which", "of", "these", "two", "representational", "schemes", "more", "effectively", "introduces", "biases", "for", "syntactic", "structure", "that", "increase", "performance", "on", "the", "subject-verb", "agreement", "prediction", "task", ".", "We", "find", "that", "a", "constituency-based", "network", "generalizes", "more", "robustly", "than", "a", "dependency-based", "one", ",", "and", "that", "combining", "the", "two", "types", "of", "structure", "does", "not", "yield", "further", "improvement", ".", "Finally", ",", "we", "show", "that", "the", "syntactic", "robustness", "of", "sequential", "models", "can", "be", "substantially", "improved", "by", "fine-tuning", "on", "a", "small", "amount", "of", "constructed", "data", ",", "suggesting", "that", "data", "augmentation", "is", "a", "viable", "alternative", "to", "explicit", "constituency", "structure", "for", "imparting", "the", "syntactic", "biases", "that", "sequential", "models", "are", "lacking", "."], "entities": [{"type": "Operation", "start": 110, "end": 118, "text": "fine-tuning on a small amount of constructed data", "sent_idx": 4}, {"type": "Effect", "start": 100, "end": 102, "text": "syntactic robustness", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--303"}
{"text": "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English\u21d2German and English\u21d2Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.", "tokens": ["Self-training", "has", "proven", "effective", "for", "improving", "NMT", "performance", "by", "augmenting", "model", "training", "with", "synthetic", "parallel", "data", ".", "The", "common", "practice", "is", "to", "construct", "synthetic", "data", "based", "on", "a", "randomly", "sampled", "subset", "of", "large-scale", "monolingual", "data", ",", "which", "we", "empirically", "show", "is", "sub-optimal", ".", "In", "this", "work", ",", "we", "propose", "to", "improve", "the", "sampling", "procedure", "by", "selecting", "the", "most", "informative", "monolingual", "sentences", "to", "complement", "the", "parallel", "data", ".", "To", "this", "end", ",", "we", "compute", "the", "uncertainty", "of", "monolingual", "sentences", "using", "the", "bilingual", "dictionary", "extracted", "from", "the", "parallel", "data", ".", "Intuitively", ",", "monolingual", "sentences", "with", "lower", "uncertainty", "generally", "correspond", "to", "easy-to-translate", "patterns", "which", "may", "not", "provide", "additional", "gains", ".", "Accordingly", ",", "we", "design", "an", "uncertainty-based", "sampling", "strategy", "to", "efficiently", "exploit", "the", "monolingual", "data", "for", "self-training", ",", "in", "which", "monolingual", "sentences", "with", "higher", "uncertainty", "would", "be", "sampled", "with", "higher", "probability", ".", "Experimental", "results", "on", "large-scale", "WMT", "English\u21d2German", "and", "English\u21d2Chinese", "datasets", "demonstrate", "the", "effectiveness", "of", "the", "proposed", "approach", ".", "Extensive", "analyses", "suggest", "that", "emphasizing", "the", "learning", "on", "uncertain", "monolingual", "sentences", "by", "our", "approach", "does", "improve", "the", "translation", "quality", "of", "high-uncertainty", "sentences", "and", "also", "benefits", "the", "prediction", "of", "low-frequency", "words", "at", "the", "target", "side", "."], "entities": [{"type": "Operation", "start": 159, "end": 166, "text": "emphasizing the learning on uncertain monolingual sentences", "sent_idx": 7}, {"type": "Effect", "start": 172, "end": 174, "text": "translation quality", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--221"}
{"text": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.", "tokens": ["Distance-based", "knowledge", "graph", "embeddings", "have", "shown", "substantial", "improvement", "on", "the", "knowledge", "graph", "link", "prediction", "task", ",", "from", "TransE", "to", "the", "latest", "state-of-the-art", "RotatE.", "However", ",", "complex", "relations", "such", "as", "N-to-1", ",", "1-to-N", "and", "N-to-N", "still", "remain", "challenging", "to", "predict", ".", "In", "this", "work", ",", "we", "propose", "a", "novel", "distance-based", "approach", "for", "knowledge", "graph", "link", "prediction", ".", "First", ",", "we", "extend", "the", "RotatE", "from", "2D", "complex", "domain", "to", "high", "dimensional", "space", "with", "orthogonal", "transforms", "to", "model", "relations", ".", "The", "orthogonal", "transform", "embedding", "for", "relations", "keeps", "the", "capability", "for", "modeling", "symmetric/anti-symmetric", ",", "inverse", "and", "compositional", "relations", "while", "achieves", "better", "modeling", "capacity", ".", "Second", ",", "the", "graph", "context", "is", "integrated", "into", "distance", "scoring", "functions", "directly", ".", "Specifically", ",", "graph", "context", "is", "explicitly", "modeled", "via", "two", "directed", "context", "representations", ".", "Each", "node", "embedding", "in", "knowledge", "graph", "is", "augmented", "with", "two", "context", "representations", ",", "which", "are", "computed", "from", "the", "neighboring", "outgoing", "and", "incoming", "nodes/edges", "respectively", ".", "The", "proposed", "approach", "improves", "prediction", "accuracy", "on", "the", "difficult", "N-to-1", ",", "1-to-N", "and", "N-to-N", "cases", ".", "Our", "experimental", "results", "show", "that", "it", "achieves", "state-of-the-art", "results", "on", "two", "common", "benchmarks", "FB15k-237", "and", "WNRR-18", ",", "especially", "on", "FB15k-237", "which", "has", "many", "high", "in-degree", "nodes", "."], "entities": [{"type": "Operation", "start": 48, "end": 50, "text": "distance-based approach", "sent_idx": 1}, {"type": "Effect", "start": 156, "end": 157, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--241"}
{"text": "This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a target-side tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English.", "tokens": ["This", "paper", "proposes", "a", "dependency", "parsing", "method", "that", "uses", "bilingual", "constraints", "to", "improve", "the", "accuracy", "of", "parsing", "bilingual", "texts", "(", "bitexts", ")", ".", "In", "our", "method", ",", "a", "target-side", "tree", "fragment", "that", "corresponds", "to", "a", "source-side", "tree", "fragment", "is", "identified", "via", "word", "alignment", "and", "mapping", "rules", "that", "are", "automatically", "learned", ".", "Then", "it", "is", "verified", "by", "checking", "the", "subtree", "list", "that", "is", "collected", "from", "large", "scale", "automatically", "parsed", "data", "on", "the", "target", "side", ".", "Our", "method", ",", "thus", ",", "requires", "gold", "standard", "trees", "only", "on", "the", "source", "side", "of", "a", "bilingual", "corpus", "in", "the", "training", "phase", ",", "unlike", "the", "joint", "parsing", "model", ",", "which", "requires", "gold", "standard", "trees", "on", "the", "both", "sides", ".", "Compared", "to", "the", "reordering", "constraint", "model", ",", "which", "requires", "the", "same", "training", "data", "as", "ours", ",", "our", "method", "achieved", "higher", "accuracy", "because", "of", "richer", "bilingual", "constraints", ".", "Experiments", "on", "the", "translated", "portion", "of", "the", "Chinese", "Treebank", "show", "that", "our", "system", "outperforms", "monolingual", "parsers", "by", "2.93", "points", "for", "Chinese", "and", "1.64", "points", "for", "English", "."], "entities": [{"type": "Operation", "start": 4, "end": 7, "text": "dependency parsing method", "sent_idx": 0}, {"type": "Effect", "start": 133, "end": 134, "text": "accuracy", "sent_idx": 4}, {"type": "Operation", "start": 9, "end": 11, "text": "bilingual constraints", "sent_idx": 0}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P10-1003"}
{"text": "Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow \u201cone-to-many\u201d conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.", "tokens": ["Conditional", "Text", "Generation", "has", "drawn", "much", "attention", "as", "a", "topic", "of", "Natural", "Language", "Generation", "(", "NLG", ")", "which", "provides", "the", "possibility", "for", "humans", "to", "control", "the", "properties", "of", "generated", "contents", ".", "Current", "conditional", "generation", "models", "can", "not", "handle", "emerging", "conditions", "due", "to", "their", "joint", "end-to-end", "learning", "fashion", ".", "When", "a", "new", "condition", "added", ",", "these", "techniques", "require", "full", "retraining", ".", "In", "this", "paper", ",", "we", "present", "a", "new", "framework", "named", "Pre-train", "and", "Plug-in", "Variational", "Auto-Encoder", "(", "PPVAE", ")", "towards", "flexible", "conditional", "text", "generation", ".", "PPVAE", "decouples", "the", "text", "generation", "module", "from", "the", "condition", "representation", "module", "to", "allow", "\u201c", "one-to-many", "\u201d", "conditional", "generation", ".", "When", "a", "fresh", "condition", "emerges", ",", "only", "a", "lightweight", "network", "needs", "to", "be", "trained", "and", "works", "as", "a", "plug-in", "for", "PPVAE", ",", "which", "is", "efficient", "and", "desirable", "for", "real-world", "applications", ".", "Extensive", "experiments", "demonstrate", "the", "superiority", "of", "PPVAE", "against", "the", "existing", "alternatives", "with", "better", "conditionality", "and", "diversity", "but", "less", "training", "effort", "."], "entities": [{"type": "Operation", "start": 140, "end": 141, "text": "PPVAE", "sent_idx": 6}, {"type": "Effect", "start": 147, "end": 148, "text": "conditionality", "sent_idx": 6}, {"type": "Effect", "start": 149, "end": 150, "text": "diversity", "sent_idx": 6}, {"type": "Effect", "start": 152, "end": 154, "text": "training effort", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Neg_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--23"}
{"text": "Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora. To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. To this end, we propose two pre-training tasks. One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents.", "tokens": ["Context-aware", "neural", "machine", "translation", "(", "NMT", ")", "remains", "challenging", "due", "to", "the", "lack", "of", "large-scale", "document-level", "parallel", "corpora", ".", "To", "break", "the", "corpus", "bottleneck", ",", "in", "this", "paper", "we", "aim", "to", "improve", "context-aware", "NMT", "by", "taking", "the", "advantage", "of", "the", "availability", "of", "both", "large-scale", "sentence-level", "parallel", "dataset", "and", "source-side", "monolingual", "documents", ".", "To", "this", "end", ",", "we", "propose", "two", "pre-training", "tasks", ".", "One", "learns", "to", "translate", "a", "sentence", "from", "source", "language", "to", "target", "language", "on", "the", "sentence-level", "parallel", "dataset", "while", "the", "other", "learns", "to", "translate", "a", "document", "from", "deliberately", "noised", "to", "original", "on", "the", "monolingual", "documents", ".", "Importantly", ",", "the", "two", "pre-training", "tasks", "are", "jointly", "and", "simultaneously", "learned", "via", "the", "same", "model", ",", "thereafter", "fine-tuned", "on", "scale-limited", "parallel", "documents", "from", "both", "sentence-level", "and", "document-level", "perspectives", ".", "Experimental", "results", "on", "four", "translation", "tasks", "show", "that", "our", "approach", "significantly", "improves", "translation", "performance", ".", "One", "nice", "property", "of", "our", "approach", "is", "that", "the", "fine-tuned", "model", "can", "be", "used", "to", "translate", "both", "sentences", "and", "documents", "."], "entities": [{"type": "Operation", "start": 58, "end": 61, "text": "two pre-training tasks", "sent_idx": 2}, {"type": "Effect", "start": 138, "end": 140, "text": "translation performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--222"}
{"text": "Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.", "tokens": ["Recently", ",", "there", "is", "an", "effort", "to", "extend", "fine-grained", "entity", "typing", "by", "using", "a", "richer", "and", "ultra-fine", "set", "of", "types", ",", "and", "labeling", "noun", "phrases", "including", "pronouns", "and", "nominal", "nouns", "instead", "of", "just", "named", "entity", "mentions", ".", "A", "key", "challenge", "for", "this", "ultra-fine", "entity", "typing", "task", "is", "that", "human", "annotated", "data", "are", "extremely", "scarce", ",", "and", "the", "annotation", "ability", "of", "existing", "distant", "or", "weak", "supervision", "approaches", "is", "very", "limited", ".", "To", "remedy", "this", "problem", ",", "in", "this", "paper", ",", "we", "propose", "to", "obtain", "training", "data", "for", "ultra-fine", "entity", "typing", "by", "using", "a", "BERT", "Masked", "Language", "Model", "(", "MLM", ")", ".", "Given", "a", "mention", "in", "a", "sentence", ",", "our", "approach", "constructs", "an", "input", "for", "the", "BERT", "MLM", "so", "that", "it", "predicts", "context", "dependent", "hypernyms", "of", "the", "mention", ",", "which", "can", "be", "used", "as", "type", "labels", ".", "Experimental", "results", "demonstrate", "that", ",", "with", "the", "help", "of", "these", "automatically", "generated", "labels", ",", "the", "performance", "of", "an", "ultra-fine", "entity", "typing", "model", "can", "be", "improved", "substantially", ".", "We", "also", "show", "that", "our", "approach", "can", "be", "applied", "to", "improve", "traditional", "fine-grained", "entity", "typing", "after", "performing", "simple", "type", "mapping", "."], "entities": [{"type": "Operation", "start": 145, "end": 148, "text": "automatically generated labels", "sent_idx": 4}, {"type": "Effect", "start": 150, "end": 151, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--141"}
{"text": "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.", "tokens": ["Over", "the", "last", "few", "years", "two", "promising", "research", "directions", "in", "low-resource", "neural", "machine", "translation", "(", "NMT", ")", "have", "emerged", ".", "The", "first", "focuses", "on", "utilizing", "high-resource", "languages", "to", "improve", "the", "quality", "of", "low-resource", "languages", "via", "multilingual", "NMT", ".", "The", "second", "direction", "employs", "monolingual", "data", "with", "self-supervision", "to", "pre-train", "translation", "models", ",", "followed", "by", "fine-tuning", "on", "small", "amounts", "of", "supervised", "data", ".", "In", "this", "work", ",", "we", "join", "these", "two", "lines", "of", "research", "and", "demonstrate", "the", "efficacy", "of", "monolingual", "data", "with", "self-supervision", "in", "multilingual", "NMT", ".", "We", "offer", "three", "major", "results", ":", "(", "i", ")", "Using", "monolingual", "data", "significantly", "boosts", "the", "translation", "quality", "of", "low-resource", "languages", "in", "multilingual", "models", ".", "(", "ii", ")", "Self-supervision", "improves", "zero-shot", "translation", "quality", "in", "multilingual", "models", ".", "(", "iii", ")", "Leveraging", "monolingual", "data", "with", "self-supervision", "provides", "a", "viable", "path", "towards", "adding", "new", "languages", "to", "multilingual", "models", ",", "getting", "up", "to", "33", "BLEU", "on", "ro-en", "translation", "without", "any", "parallel", "data", "or", "back-translation", "."], "entities": [{"type": "Operation", "start": 94, "end": 97, "text": "Using monolingual data", "sent_idx": 4}, {"type": "Effect", "start": 100, "end": 102, "text": "translation quality", "sent_idx": 4}, {"type": "Operation", "start": 124, "end": 129, "text": "Leveraging monolingual data with self-supervision", "sent_idx": 6}, {"type": "Effect", "start": 145, "end": 146, "text": "BLEU", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--252"}
{"text": "Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. However, this problem is less studied in open-domain dialogue. In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence.", "tokens": ["Learning", "discrete", "dialog", "structure", "graph", "from", "human-human", "dialogs", "yields", "basic", "insights", "into", "the", "structure", "of", "conversation", ",", "and", "also", "provides", "background", "knowledge", "to", "facilitate", "dialog", "generation", ".", "However", ",", "this", "problem", "is", "less", "studied", "in", "open-domain", "dialogue", ".", "In", "this", "paper", ",", "we", "conduct", "unsupervised", "discovery", "of", "discrete", "dialog", "structure", "from", "chitchat", "corpora", ",", "and", "then", "leverage", "it", "to", "facilitate", "coherent", "dialog", "generation", "in", "downstream", "systems", ".", "To", "this", "end", ",", "we", "present", "an", "unsupervised", "model", ",", "Discrete", "Variational", "Auto-Encoder", "with", "Graph", "Neural", "Network", "(", "DVAE-GNN", ")", ",", "to", "discover", "discrete", "hierarchical", "latent", "dialog", "states", "(", "at", "the", "level", "of", "both", "session", "and", "utterance", ")", "and", "their", "transitions", "from", "corpus", "as", "a", "dialog", "structure", "graph", ".", "Then", "we", "leverage", "it", "as", "background", "knowledge", "to", "facilitate", "dialog", "management", "in", "a", "RL", "based", "dialog", "system", ".", "Experimental", "results", "on", "two", "benchmark", "corpora", "confirm", "that", "DVAE-GNN", "can", "discover", "meaningful", "dialog", "structure", "graph", ",", "and", "the", "use", "of", "dialog", "structure", "as", "background", "knowledge", "can", "significantly", "improve", "multi-turn", "coherence", "."], "entities": [{"type": "Operation", "start": 152, "end": 159, "text": "use of dialog structure as background knowledge", "sent_idx": 4}, {"type": "Effect", "start": 162, "end": 164, "text": "multi-turn coherence", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--136"}
{"text": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.", "tokens": ["Fine-tuning", "of", "pre-trained", "transformer", "models", "has", "become", "the", "standard", "approach", "for", "solving", "common", "NLP", "tasks", ".", "Most", "of", "the", "existing", "approaches", "rely", "on", "a", "randomly", "initialized", "classifier", "on", "top", "of", "such", "networks", ".", "We", "argue", "that", "this", "fine-tuning", "procedure", "is", "sub-optimal", "as", "the", "pre-trained", "model", "has", "no", "prior", "on", "the", "specific", "classifier", "labels", ",", "while", "it", "might", "have", "already", "learned", "an", "intrinsic", "textual", "representation", "of", "the", "task", ".", "In", "this", "paper", ",", "we", "introduce", "a", "new", "scoring", "method", "that", "casts", "a", "plausibility", "ranking", "task", "in", "a", "full-text", "format", "and", "leverages", "the", "masked", "language", "modeling", "head", "tuned", "during", "the", "pre-training", "phase", ".", "We", "study", "commonsense", "reasoning", "tasks", "where", "the", "model", "must", "rank", "a", "set", "of", "hypotheses", "given", "a", "premise", ",", "focusing", "on", "the", "COPA", ",", "Swag", ",", "HellaSwag", "and", "CommonsenseQA", "datasets", ".", "By", "exploiting", "our", "scoring", "method", "without", "fine-tuning", ",", "we", "are", "able", "to", "produce", "strong", "baselines", "(", "e.g.", "80", "%", "test", "accuracy", "on", "COPA", ")", "that", "are", "comparable", "to", "supervised", "approaches", ".", "Moreover", ",", "when", "fine-tuning", "directly", "on", "the", "proposed", "scoring", "function", ",", "we", "show", "that", "our", "method", "provides", "a", "much", "more", "stable", "training", "phase", "across", "random", "restarts", "(", "e.g", "x10", "standard", "deviation", "reduction", "on", "COPA", "test", "accuracy", ")", "and", "requires", "less", "annotated", "data", "than", "the", "standard", "classifier", "approach", "to", "reach", "equivalent", "performances", "."], "entities": [{"type": "Operation", "start": 79, "end": 88, "text": "casts a plausibility ranking task in a full-text format", "sent_idx": 3}, {"type": "Effect", "start": 151, "end": 152, "text": "accuracy", "sent_idx": 5}, {"type": "Operation", "start": 89, "end": 100, "text": "leverages the masked language modeling head tuned during the pre-training phase", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--357"}
{"text": "The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements.", "tokens": ["The", "goal", "of", "dialogue", "state", "tracking", "(", "DST", ")", "is", "to", "predict", "the", "current", "dialogue", "state", "given", "all", "previous", "dialogue", "contexts", ".", "Existing", "approaches", "generally", "predict", "the", "dialogue", "state", "at", "every", "turn", "from", "scratch", ".", "However", ",", "the", "overwhelming", "majority", "of", "the", "slots", "in", "each", "turn", "should", "simply", "inherit", "the", "slot", "values", "from", "the", "previous", "turn", ".", "Therefore", ",", "the", "mechanism", "of", "treating", "slots", "equally", "in", "each", "turn", "not", "only", "is", "inefficient", "but", "also", "may", "lead", "to", "additional", "errors", "because", "of", "the", "redundant", "slot", "value", "generation", ".", "To", "address", "this", "problem", ",", "we", "devise", "the", "two-stage", "DSS-DST", "which", "consists", "of", "the", "Dual", "Slot", "Selector", "based", "on", "the", "current", "turn", "dialogue", ",", "and", "the", "Slot", "Value", "Generator", "based", "on", "the", "dialogue", "history", ".", "The", "Dual", "Slot", "Selector", "determines", "each", "slot", "whether", "to", "update", "slot", "value", "or", "to", "inherit", "the", "slot", "value", "from", "the", "previous", "turn", "from", "two", "aspects", ":", "(", "1", ")", "if", "there", "is", "a", "strong", "relationship", "between", "it", "and", "the", "current", "turn", "dialogue", "utterances", ";", "(", "2", ")", "if", "a", "slot", "value", "with", "high", "reliability", "can", "be", "obtained", "for", "it", "through", "the", "current", "turn", "dialogue", ".", "The", "slots", "selected", "to", "be", "updated", "are", "permitted", "to", "enter", "the", "Slot", "Value", "Generator", "to", "update", "values", "by", "a", "hybrid", "method", ",", "while", "the", "other", "slots", "directly", "inherit", "the", "values", "from", "the", "previous", "turn", ".", "Empirical", "results", "show", "that", "our", "method", "achieves", "56.93", "%", ",", "60.73", "%", ",", "and", "58.04", "%", "joint", "accuracy", "on", "MultiWOZ", "2.0", ",", "MultiWOZ", "2.1", ",", "and", "MultiWOZ", "2.2", "datasets", "respectively", "and", "achieves", "a", "new", "state-of-the-art", "performance", "with", "significant", "improvements", "."], "entities": [{"type": "Operation", "start": 95, "end": 97, "text": "two-stage DSS-DST", "sent_idx": 4}, {"type": "Effect", "start": 238, "end": 240, "text": "joint accuracy", "sent_idx": 7}, {"type": "Effect", "start": 257, "end": 258, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--12"}
{"text": "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.", "tokens": ["Pre-trained", "language", "models", "(", "LMs", ")", "are", "currently", "integral", "to", "many", "natural", "language", "processing", "systems", ".", "Although", "multilingual", "LMs", "were", "also", "introduced", "to", "serve", "many", "languages", ",", "these", "have", "limitations", "such", "as", "being", "costly", "at", "inference", "time", "and", "the", "size", "and", "diversity", "of", "non-English", "data", "involved", "in", "their", "pre-training", ".", "We", "remedy", "these", "issues", "for", "a", "collection", "of", "diverse", "Arabic", "varieties", "by", "introducing", "two", "powerful", "deep", "bidirectional", "transformer-based", "models", ",", "ARBERT", "and", "MARBERT", ".", "To", "evaluate", "our", "models", ",", "we", "also", "introduce", "ARLUE", ",", "a", "new", "benchmark", "for", "multi-dialectal", "Arabic", "language", "understanding", "evaluation", ".", "ARLUE", "is", "built", "using", "42", "datasets", "targeting", "six", "different", "task", "clusters", ",", "allowing", "us", "to", "offer", "a", "series", "of", "standardized", "experiments", "under", "rich", "conditions", ".", "When", "fine-tuned", "on", "ARLUE", ",", "our", "models", "collectively", "achieve", "new", "state-of-the-art", "results", "across", "the", "majority", "of", "tasks", "(", "37", "out", "of", "48", "classification", "tasks", ",", "on", "the", "42", "datasets", ")", ".", "Our", "best", "model", "acquires", "the", "highest", "ARLUE", "score", "(", "77.40", ")", "across", "all", "six", "task", "clusters", ",", "outperforming", "all", "other", "models", "including", "XLM-R", "Large", "(", "3.4x", "larger", "size", ")", ".", "Our", "models", "are", "publicly", "available", "at", "https://github.com/UBC-NLP/marbert", "and", "ARLUE", "will", "be", "released", "through", "the", "same", "repository", "."], "entities": [{"type": "Operation", "start": 62, "end": 69, "text": "introducing two powerful deep bidirectional transformer-based models", "sent_idx": 2}, {"type": "Effect", "start": 156, "end": 158, "text": "ARLUE score", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--551"}
{"text": "CCGs are directly compatible with binary-branching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.", "tokens": ["CCGs", "are", "directly", "compatible", "with", "binary-branching", "bottom-up", "parsing", "algorithms", ",", "in", "particular", "CKY", "and", "shift-reduce", "algorithms", ".", "While", "the", "chart-based", "approach", "has", "been", "the", "dominant", "approach", "for", "CCG", ",", "the", "shift-reduce", "method", "has", "been", "little", "explored", ".", "In", "this", "paper", ",", "we", "develop", "a", "shift-reduce", "CCG", "parser", "using", "a", "discriminative", "model", "and", "beam", "search", ",", "and", "compare", "its", "strengths", "and", "weaknesses", "with", "the", "chart-based", "C&C", "parser", ".", "We", "study", "different", "errors", "made", "by", "the", "two", "parsers", ",", "and", "show", "that", "the", "shift-reduce", "parser", "gives", "competitive", "accuracies", "compared", "to", "C&C.", "Considering", "our", "use", "of", "a", "small", "beam", ",", "and", "given", "the", "high", "ambiguity", "levels", "in", "an", "automatically-extracted", "grammar", "and", "the", "amount", "of", "information", "in", "the", "CCG", "lexical", "categories", "which", "form", "the", "shift", "actions", ",", "this", "is", "a", "surprising", "result", "."], "entities": [{"type": "Operation", "start": 81, "end": 83, "text": "shift-reduce parser", "sent_idx": 3}, {"type": "Effect", "start": 85, "end": 86, "text": "accuracies", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1069"}
{"text": "The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub.", "tokens": ["The", "prevalence", "of", "the", "COVID-19", "pandemic", "in", "day-to-day", "life", "has", "yielded", "large", "amounts", "of", "stance", "detection", "data", "on", "social", "media", "sites", ",", "as", "users", "turn", "to", "social", "media", "to", "share", "their", "views", "regarding", "various", "issues", "related", "to", "the", "pandemic", ",", "e.g.", "stay", "at", "home", "mandates", "and", "wearing", "face", "masks", "when", "out", "in", "public", ".", "We", "set", "out", "to", "make", "use", "of", "this", "data", "by", "collecting", "the", "stance", "expressed", "by", "Twitter", "users", ",", "with", "respect", "to", "topics", "revolving", "around", "the", "pandemic", ".", "We", "annotate", "a", "new", "stance", "detection", "dataset", ",", "called", "COVID-19-Stance", ".", "Using", "this", "newly", "annotated", "dataset", ",", "we", "train", "several", "established", "stance", "detection", "models", "to", "ascertain", "a", "baseline", "performance", "for", "this", "specific", "task", ".", "To", "further", "improve", "the", "performance", ",", "we", "employ", "self-training", "and", "domain", "adaptation", "approaches", "to", "take", "advantage", "of", "large", "amounts", "of", "unlabeled", "data", "and", "existing", "stance", "detection", "datasets", ".", "The", "dataset", ",", "code", ",", "and", "other", "resources", "are", "available", "on", "GitHub", "."], "entities": [{"type": "Operation", "start": 123, "end": 128, "text": "self-training and domain adaptation approaches", "sent_idx": 4}, {"type": "Effect", "start": 119, "end": 120, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 135, "end": 142, "text": "unlabeled data and existing stance detection datasets", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--127"}
{"text": "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.", "tokens": ["Hindi", "grapheme-to-phoneme", "(", "G2P", ")", "conversion", "is", "mostly", "trivial", ",", "with", "one", "exception", ":", "whether", "a", "schwa", "represented", "in", "the", "orthography", "is", "pronounced", "or", "unpronounced", "(", "deleted", ")", ".", "Previous", "work", "has", "attempted", "to", "predict", "schwa", "deletion", "in", "a", "rule-based", "fashion", "using", "prosodic", "or", "phonetic", "analysis", ".", "We", "present", "the", "first", "statistical", "schwa", "deletion", "classifier", "for", "Hindi", ",", "which", "relies", "solely", "on", "the", "orthography", "as", "the", "input", "and", "outperforms", "previous", "approaches", ".", "We", "trained", "our", "model", "on", "a", "newly-compiled", "pronunciation", "lexicon", "extracted", "from", "various", "online", "dictionaries", ".", "Our", "best", "Hindi", "model", "achieves", "state", "of", "the", "art", "performance", ",", "and", "also", "achieves", "good", "performance", "on", "a", "closely", "related", "language", ",", "Punjabi", ",", "without", "modification", "."], "entities": [{"type": "Operation", "start": 51, "end": 55, "text": "statistical schwa deletion classifier", "sent_idx": 2}, {"type": "Effect", "start": 96, "end": 97, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--696"}
{"text": "This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on reference resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.", "tokens": ["This", "paper", "proposes", "an", "approach", "to", "reference", "resolution", "in", "situated", "dialogues", "by", "exploiting", "extra-linguistic", "information", ".", "Recently", ",", "investigations", "of", "referential", "behaviours", "involved", "in", "situations", "in", "the", "real", "world", "have", "received", "increasing", "attention", "by", "researchers", "(", "Di", "Eugenio", "et", "al.", ",", "2000", ";", "Byron", ",", "2005", ";", "van", "Deemter", ",", "2007", ";", "Spanger", "et", "al.", ",", "2009", ")", ".", "In", "order", "to", "create", "an", "accurate", "reference", "resolution", "model", ",", "we", "need", "to", "handle", "extra-linguistic", "information", "as", "well", "as", "textual", "information", "examined", "by", "existing", "approaches", "(", "Soon", "et", "al.", ",", "2001", ";", "Ng", "and", "Cardie", ",", "2002", ",", "etc", ".", ")", ".", "In", "this", "paper", ",", "we", "incorporate", "extra-linguistic", "information", "into", "an", "existing", "corpus-based", "reference", "resolution", "model", ",", "and", "investigate", "its", "effects", "on", "reference", "resolution", "problems", "within", "a", "corpus", "of", "Japanese", "dialogues", ".", "The", "results", "demonstrate", "that", "our", "proposed", "model", "achieves", "an", "accuracy", "of", "79.0", "%", "for", "this", "task", "."], "entities": [{"type": "Operation", "start": 106, "end": 116, "text": "incorporate extra-linguistic information into an existing corpus-based reference resolution model", "sent_idx": 3}, {"type": "Effect", "start": 141, "end": 142, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1128"}
{"text": "Information-extraction (IE) systems seek to distill semantic relations from natural-language text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? \n \nThis paper presents WOE, an open IE system which improves dramatically on TextRunner's precision and recall. The key to WOE's performance is a novel form of self-supervised learning for open extractors -- using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE's extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.", "tokens": ["Information-extraction", "(", "IE", ")", "systems", "seek", "to", "distill", "semantic", "relations", "from", "natural-language", "text", ",", "but", "most", "systems", "use", "supervised", "learning", "of", "relation-specific", "examples", "and", "are", "thus", "limited", "by", "the", "availability", "of", "training", "data", ".", "Open", "IE", "systems", "such", "as", "TextRunner", ",", "on", "the", "other", "hand", ",", "aim", "to", "handle", "the", "unbounded", "number", "of", "relations", "found", "on", "the", "Web", ".", "But", "how", "well", "can", "these", "open", "systems", "perform", "?", "\n \n", "This", "paper", "presents", "WOE", ",", "an", "open", "IE", "system", "which", "improves", "dramatically", "on", "TextRunner", "'s", "precision", "and", "recall", ".", "The", "key", "to", "WOE", "'s", "performance", "is", "a", "novel", "form", "of", "self-supervised", "learning", "for", "open", "extractors", "--", "using", "heuristic", "matches", "between", "Wikipedia", "infobox", "attribute", "values", "and", "corresponding", "sentences", "to", "construct", "training", "data", ".", "Like", "TextRunner", ",", "WOE", "'s", "extractor", "eschews", "lexicalized", "features", "and", "handles", "an", "unbounded", "set", "of", "semantic", "relations", ".", "WOE", "can", "operate", "in", "two", "modes", ":", "when", "restricted", "to", "POS", "tag", "features", ",", "it", "runs", "as", "quickly", "as", "TextRunner", ",", "but", "when", "set", "to", "use", "dependency-parse", "features", "its", "precision", "and", "recall", "rise", "even", "higher", "."], "entities": [{"type": "Operation", "start": 72, "end": 73, "text": "WOE", "sent_idx": 2}, {"type": "Effect", "start": 84, "end": 85, "text": "precision", "sent_idx": 2}, {"type": "Effect", "start": 86, "end": 87, "text": "recall", "sent_idx": 2}, {"type": "Operation", "start": 165, "end": 167, "text": "dependency-parse features", "sent_idx": 5}, {"type": "Effect", "start": 168, "end": 169, "text": "precision", "sent_idx": 5}, {"type": "Effect", "start": 170, "end": 171, "text": "recall", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 4}, {"type": "Pos_Affect", "head": 3, "tail": 5}], "id": "P10-1013"}
{"text": "Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task. Besides, we propose the Flow score , an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation ( r=0.9 ) with human ratings among 11 chatbots. Code and pre-trained models will be public.", "tokens": ["Nowadays", ",", "open-domain", "dialogue", "models", "can", "generate", "acceptable", "responses", "according", "to", "the", "historical", "context", "based", "on", "the", "large-scale", "pre-trained", "language", "models", ".", "However", ",", "they", "generally", "concatenate", "the", "dialogue", "history", "directly", "as", "the", "model", "input", "to", "predict", "the", "response", ",", "which", "we", "named", "as", "the", "flat", "pattern", "and", "ignores", "the", "dynamic", "information", "flow", "across", "dialogue", "utterances", ".", "In", "this", "work", ",", "we", "propose", "the", "DialoFlow", "model", ",", "in", "which", "we", "introduce", "a", "dynamic", "flow", "mechanism", "to", "model", "the", "context", "flow", ",", "and", "design", "three", "training", "objectives", "to", "capture", "the", "information", "dynamics", "across", "dialogue", "utterances", "by", "addressing", "the", "semantic", "influence", "brought", "about", "by", "each", "utterance", "in", "large-scale", "pre-training", ".", "Experiments", "on", "the", "multi-reference", "Reddit", "Dataset", "and", "DailyDialog", "Dataset", "demonstrate", "that", "our", "DialoFlow", "significantly", "outperforms", "the", "DialoGPT", "on", "the", "dialogue", "generation", "task", ".", "Besides", ",", "we", "propose", "the", "Flow", "score", ",", "an", "effective", "automatic", "metric", "for", "evaluating", "interactive", "human-bot", "conversation", "quality", "based", "on", "the", "pre-trained", "DialoFlow", ",", "which", "presents", "high", "chatbot-level", "correlation", "(", "r=0.9", ")", "with", "human", "ratings", "among", "11", "chatbots", ".", "Code", "and", "pre-trained", "models", "will", "be", "public", "."], "entities": [{"type": "Operation", "start": 136, "end": 138, "text": "Flow score", "sent_idx": 4}, {"type": "Effect", "start": 158, "end": 160, "text": "chatbot-level correlation", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--11"}
{"text": "It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \u201csyntactic distances\u201d, where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.", "tokens": ["It", "is", "commonly", "believed", "that", "knowledge", "of", "syntactic", "structure", "should", "improve", "language", "modeling", ".", "However", ",", "effectively", "and", "computationally", "efficiently", "incorporating", "syntactic", "structure", "into", "neural", "language", "models", "has", "been", "a", "challenging", "topic", ".", "In", "this", "paper", ",", "we", "make", "use", "of", "a", "multi-task", "objective", ",", "i.e.", ",", "the", "models", "simultaneously", "predict", "words", "as", "well", "as", "ground", "truth", "parse", "trees", "in", "a", "form", "called", "\u201c", "syntactic", "distances", "\u201d", ",", "where", "information", "between", "these", "two", "separate", "objectives", "shares", "the", "same", "intermediate", "representation", ".", "Experimental", "results", "on", "the", "Penn", "Treebank", "and", "Chinese", "Treebank", "datasets", "show", "that", "when", "ground", "truth", "parse", "trees", "are", "provided", "as", "additional", "training", "signals", ",", "the", "model", "is", "able", "to", "achieve", "lower", "perplexity", "and", "induce", "trees", "with", "better", "quality", "."], "entities": [{"type": "Operation", "start": 94, "end": 104, "text": "ground truth parse trees are provided as additional training signals", "sent_idx": 3}, {"type": "Effect", "start": 112, "end": 113, "text": "perplexity", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--591"}
{"text": "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large.", "tokens": ["The", "advent", "of", "large", "pre-trained", "language", "models", "has", "given", "rise", "to", "rapid", "progress", "in", "the", "field", "of", "Natural", "Language", "Processing", "(", "NLP", ")", ".", "While", "the", "performance", "of", "these", "models", "on", "standard", "benchmarks", "has", "scaled", "with", "size", ",", "compression", "techniques", "such", "as", "knowledge", "distillation", "have", "been", "key", "in", "making", "them", "practical", ".", "We", "present", "MATE-KD", ",", "a", "novel", "text-based", "adversarial", "training", "algorithm", "which", "improves", "the", "performance", "of", "knowledge", "distillation", ".", "MATE-KD", "first", "trains", "a", "masked", "language", "model-based", "generator", "to", "perturb", "text", "by", "maximizing", "the", "divergence", "between", "teacher", "and", "student", "logits", ".", "Then", "using", "knowledge", "distillation", "a", "student", "is", "trained", "on", "both", "the", "original", "and", "the", "perturbed", "training", "samples", ".", "We", "evaluate", "our", "algorithm", ",", "using", "BERT-based", "models", ",", "on", "the", "GLUE", "benchmark", "and", "demonstrate", "that", "MATE-KD", "outperforms", "competitive", "adversarial", "learning", "and", "data", "augmentation", "baselines", ".", "On", "the", "GLUE", "test", "set", "our", "6", "layer", "RoBERTa", "based", "model", "outperforms", "BERT-large", "."], "entities": [{"type": "Operation", "start": 58, "end": 62, "text": "text-based adversarial training algorithm", "sent_idx": 2}, {"type": "Effect", "start": 65, "end": 66, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--86"}
{"text": "Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.", "tokens": ["Neural", "Machine", "Translation", "(", "NMT", ")", "models", "are", "sensitive", "to", "small", "perturbations", "in", "the", "input", ".", "Robustness", "to", "such", "perturbations", "is", "typically", "measured", "using", "translation", "quality", "metrics", "such", "as", "BLEU", "on", "the", "noisy", "input", ".", "This", "paper", "proposes", "additional", "metrics", "which", "measure", "the", "relative", "degradation", "and", "changes", "in", "translation", "when", "small", "perturbations", "are", "added", "to", "the", "input", ".", "We", "focus", "on", "a", "class", "of", "models", "employing", "subword", "regularization", "to", "address", "robustness", "and", "perform", "extensive", "evaluations", "of", "these", "models", "using", "the", "robustness", "measures", "proposed", ".", "Results", "show", "that", "our", "proposed", "metrics", "reveal", "a", "clear", "trend", "of", "improved", "robustness", "to", "perturbations", "when", "subword", "regularization", "methods", "are", "used", "."], "entities": [{"type": "Operation", "start": 100, "end": 103, "text": "subword regularization methods", "sent_idx": 4}, {"type": "Effect", "start": 96, "end": 97, "text": "robustness", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--755"}
{"text": "Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.", "tokens": ["Unsupervised", "machine", "translation", ",", "which", "utilizes", "unpaired", "monolingual", "corpora", "as", "training", "data", ",", "has", "achieved", "comparable", "performance", "against", "supervised", "machine", "translation", ".", "However", ",", "it", "still", "suffers", "from", "data-scarce", "domains", ".", "To", "address", "this", "issue", ",", "this", "paper", "presents", "a", "novel", "meta-learning", "algorithm", "for", "unsupervised", "neural", "machine", "translation", "(", "UNMT", ")", "that", "trains", "the", "model", "to", "adapt", "to", "another", "domain", "by", "utilizing", "only", "a", "small", "amount", "of", "training", "data", ".", "We", "assume", "that", "domain-general", "knowledge", "is", "a", "significant", "factor", "in", "handling", "data-scarce", "domains", ".", "Hence", ",", "we", "extend", "the", "meta-learning", "algorithm", ",", "which", "utilizes", "knowledge", "learned", "from", "high-resource", "domains", ",", "to", "boost", "the", "performance", "of", "low-resource", "UNMT", ".", "Our", "model", "surpasses", "a", "transfer", "learning-based", "approach", "by", "up", "to", "2", "-", "3", "BLEU", "scores", ".", "Extensive", "experimental", "results", "show", "that", "our", "proposed", "algorithm", "is", "pertinent", "for", "fast", "adaptation", "and", "consistently", "outperforms", "other", "baselines", "."], "entities": [{"type": "Operation", "start": 41, "end": 51, "text": "meta-learning algorithm for unsupervised neural machine translation (UNMT)", "sent_idx": 2}, {"type": "Effect", "start": 121, "end": 123, "text": "BLEU scores", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--225"}
{"text": "Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer \u201cwho expressed what kind of sentiment towards what?\u201d. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously. We verify our methods on the benchmark MPQA corpus. The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.", "tokens": ["Opinion", "role", "labeling", "(", "ORL", ")", "is", "a", "fine-grained", "opinion", "analysis", "task", "and", "aims", "to", "answer", "\u201c", "who", "expressed", "what", "kind", "of", "sentiment", "towards", "what", "?", "\u201d", ".", "Due", "to", "the", "scarcity", "of", "labeled", "data", ",", "ORL", "remains", "challenging", "for", "data-driven", "methods", ".", "In", "this", "work", ",", "we", "try", "to", "enhance", "neural", "ORL", "models", "with", "syntactic", "knowledge", "by", "comparing", "and", "integrating", "different", "representations", ".", "We", "also", "propose", "dependency", "graph", "convolutional", "networks", "(", "DEPGCN", ")", "to", "encode", "parser", "information", "at", "different", "processing", "levels", ".", "In", "order", "to", "compensate", "for", "parser", "inaccuracy", "and", "reduce", "error", "propagation", ",", "we", "introduce", "multi-task", "learning", "(", "MTL", ")", "to", "train", "the", "parser", "and", "the", "ORL", "model", "simultaneously", ".", "We", "verify", "our", "methods", "on", "the", "benchmark", "MPQA", "corpus", ".", "The", "experimental", "results", "show", "that", "syntactic", "information", "is", "highly", "valuable", "for", "ORL", ",", "and", "our", "final", "MTL", "model", "effectively", "boosts", "the", "F1", "score", "by", "9.29", "over", "the", "syntax-agnostic", "baseline", ".", "In", "addition", ",", "we", "find", "that", "the", "contributions", "from", "syntactic", "knowledge", "do", "not", "fully", "overlap", "with", "contextualized", "word", "representations", "(", "BERT", ")", ".", "Our", "best", "model", "achieves", "4.34", "higher", "F1", "score", "than", "the", "current", "state-ofthe-art", "."], "entities": [{"type": "Operation", "start": 127, "end": 129, "text": "syntactic information", "sent_idx": 5}, {"type": "Effect", "start": 143, "end": 145, "text": "F1 score", "sent_idx": 5}, {"type": "Operation", "start": 67, "end": 74, "text": "dependency graph convolutional networks (DEPGCN)", "sent_idx": 2}, {"type": "Effect", "start": 181, "end": 183, "text": "F1 score", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--297"}
{"text": "This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system.", "tokens": ["This", "paper", "proposes", "to", "use", "monolingual", "collocations", "to", "improve", "Statistical", "Machine", "Translation", "(", "SMT", ")", ".", "We", "make", "use", "of", "the", "collocation", "probabilities", ",", "which", "are", "estimated", "from", "monolingual", "corpora", ",", "in", "two", "aspects", ",", "namely", "improving", "word", "alignment", "for", "various", "kinds", "of", "SMT", "systems", "and", "improving", "phrase", "table", "for", "phrase-based", "SMT", ".", "The", "experimental", "results", "show", "that", "our", "method", "improves", "the", "performance", "of", "both", "word", "alignment", "and", "translation", "quality", "significantly", ".", "As", "compared", "to", "baseline", "systems", ",", "we", "achieve", "absolute", "improvements", "of", "2.40", "BLEU", "score", "on", "a", "phrase-based", "SMT", "system", "and", "1.76", "BLEU", "score", "on", "a", "parsing-based", "SMT", "system", "."], "entities": [{"type": "Operation", "start": 36, "end": 45, "text": "improving word alignment for various kinds of SMT systems", "sent_idx": 1}, {"type": "Effect", "start": 84, "end": 86, "text": "BLEU score", "sent_idx": 3}, {"type": "Operation", "start": 46, "end": 52, "text": "improving phrase table for phrase-based SMT", "sent_idx": 1}, {"type": "Effect", "start": 93, "end": 95, "text": "BLEU score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "P10-1085"}
{"text": "Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.", "tokens": ["Concept", "normalization", ",", "the", "task", "of", "linking", "textual", "mentions", "of", "concepts", "to", "concepts", "in", "an", "ontology", ",", "is", "challenging", "because", "ontologies", "are", "large", ".", "In", "most", "cases", ",", "annotated", "datasets", "cover", "only", "a", "small", "sample", "of", "the", "concepts", ",", "yet", "concept", "normalizers", "are", "expected", "to", "predict", "all", "concepts", "in", "the", "ontology", ".", "In", "this", "paper", ",", "we", "propose", "an", "architecture", "consisting", "of", "a", "candidate", "generator", "and", "a", "list-wise", "ranker", "based", "on", "BERT", ".", "The", "ranker", "considers", "pairings", "of", "concept", "mentions", "and", "candidate", "concepts", ",", "allowing", "it", "to", "make", "predictions", "for", "any", "concept", ",", "not", "just", "those", "seen", "during", "training", ".", "We", "further", "enhance", "this", "list-wise", "approach", "with", "a", "semantic", "type", "regularizer", "that", "allows", "the", "model", "to", "incorporate", "semantic", "type", "information", "from", "the", "ontology", "during", "training", ".", "Our", "proposed", "concept", "normalization", "framework", "achieves", "state-of-the-art", "performance", "on", "multiple", "datasets", "."], "entities": [{"type": "Operation", "start": 128, "end": 131, "text": "concept normalization framework", "sent_idx": 5}, {"type": "Effect", "start": 133, "end": 134, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--748"}
{"text": "Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.", "tokens": ["Most", "of", "the", "existing", "models", "for", "document-level", "machine", "translation", "adopt", "dual-encoder", "structures", ".", "The", "representation", "of", "the", "source", "sentences", "and", "the", "document-level", "contexts", "are", "modeled", "with", "two", "separate", "encoders", ".", "Although", "these", "models", "can", "make", "use", "of", "the", "document-level", "contexts", ",", "they", "do", "not", "fully", "model", "the", "interaction", "between", "the", "contexts", "and", "the", "source", "sentences", ",", "and", "can", "not", "directly", "adapt", "to", "the", "recent", "pre-training", "models", "(", "e.g.", ",", "BERT", ")", "which", "encodes", "multiple", "sentences", "with", "a", "single", "encoder", ".", "In", "this", "work", ",", "we", "propose", "a", "simple", "and", "effective", "unified", "encoder", "that", "can", "outperform", "the", "baseline", "models", "of", "dual-encoder", "models", "in", "terms", "of", "BLEU", "and", "METEOR", "scores", ".", "Moreover", ",", "the", "pre-training", "models", "can", "further", "boost", "the", "performance", "of", "our", "proposed", "model", "."], "entities": [{"type": "Operation", "start": 90, "end": 92, "text": "unified encoder", "sent_idx": 3}, {"type": "Effect", "start": 104, "end": 105, "text": "BLEU", "sent_idx": 3}, {"type": "Effect", "start": 106, "end": 108, "text": "METEOR scores", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--321"}
{"text": "Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model. CARI is able to learn to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models\u2019 performance on several tweet sentiment analysis tasks. Our contributions are as follows: 1.We propose a new method, CARI, to integrate rules for pre-trained language models. CARI is context-aware and can trained end-to-end with the downstream NLP applications. 2.We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset. 3.We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks. We show that CARI outperformed existing rule-based FST approaches for sentiment classification.", "tokens": ["Models", "pre-trained", "on", "large-scale", "regular", "text", "corpora", "often", "do", "not", "work", "well", "for", "user-generated", "data", "where", "the", "language", "styles", "differ", "significantly", "from", "the", "mainstream", "text", ".", "Here", "we", "present", "Context-Aware", "Rule", "Injection", "(", "CARI", ")", ",", "an", "innovative", "method", "for", "formality", "style", "transfer", "(", "FST", ")", "by", "injecting", "multiple", "rules", "into", "an", "end-to-end", "BERT-based", "encoder", "and", "decoder", "model", ".", "CARI", "is", "able", "to", "learn", "to", "select", "optimal", "rules", "based", "on", "context", ".", "The", "intrinsic", "evaluation", "showed", "that", "CARI", "achieved", "the", "new", "highest", "performance", "on", "the", "FST", "benchmark", "dataset", ".", "Our", "extrinsic", "evaluation", "showed", "that", "CARI", "can", "greatly", "improve", "the", "regular", "pre-trained", "models", "\u2019", "performance", "on", "several", "tweet", "sentiment", "analysis", "tasks", ".", "Our", "contributions", "are", "as", "follows", ":", "1.We", "propose", "a", "new", "method", ",", "CARI", ",", "to", "integrate", "rules", "for", "pre-trained", "language", "models", ".", "CARI", "is", "context-aware", "and", "can", "trained", "end-to-end", "with", "the", "downstream", "NLP", "applications", ".", "2.We", "have", "achieved", "new", "state-of-the-art", "results", "for", "FST", "on", "the", "benchmark", "GYAFC", "dataset", ".", "3.We", "are", "the", "first", "to", "evaluate", "FST", "methods", "with", "extrinsic", "evaluation", "and", "specifically", "on", "sentiment", "classification", "tasks", ".", "We", "show", "that", "CARI", "outperformed", "existing", "rule-based", "FST", "approaches", "for", "sentiment", "classification", "."], "entities": [{"type": "Operation", "start": 94, "end": 95, "text": "CARI", "sent_idx": 4}, {"type": "Effect", "start": 99, "end": 104, "text": "regular pre-trained models\u2019 performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--124"}
{"text": "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution \u2013 there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT\u201914 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT\u201916 English-German tasks.", "tokens": ["The", "discrepancy", "between", "maximum", "likelihood", "estimation", "(", "MLE", ")", "and", "task", "measures", "such", "as", "BLEU", "score", "has", "been", "studied", "before", "for", "autoregressive", "neural", "machine", "translation", "(", "NMT", ")", "and", "resulted", "in", "alternative", "training", "algorithms", "(", "Ranzato", "et", "al.", ",", "2016", ";", "Norouzi", "et", "al.", ",", "2016", ";", "Shen", "et", "al.", ",", "2016", ";", "Wu", "et", "al.", ",", "2018", ")", ".", "However", ",", "MLE", "training", "remains", "the", "de", "facto", "approach", "for", "autoregressive", "NMT", "because", "of", "its", "computational", "efficiency", "and", "stability", ".", "Despite", "this", "mismatch", "between", "the", "training", "objective", "and", "task", "measure", ",", "we", "notice", "that", "the", "samples", "drawn", "from", "an", "MLE-based", "trained", "NMT", "support", "the", "desired", "distribution", "\u2013", "there", "are", "samples", "with", "much", "higher", "BLEU", "score", "comparing", "to", "the", "beam", "decoding", "output", ".", "To", "benefit", "from", "this", "observation", ",", "we", "train", "an", "energy-based", "model", "to", "mimic", "the", "behavior", "of", "the", "task", "measure", "(", "i.e.", ",", "the", "energy-based", "model", "assigns", "lower", "energy", "to", "samples", "with", "higher", "BLEU", "score", ")", ",", "which", "is", "resulted", "in", "a", "re-ranking", "algorithm", "based", "on", "the", "samples", "drawn", "from", "NMT", ":", "energy-based", "re-ranking", "(", "EBR", ")", ".", "We", "use", "both", "marginal", "energy", "models", "(", "over", "target", "sentence", ")", "and", "joint", "energy", "models", "(", "over", "both", "source", "and", "target", "sentences", ")", ".", "Our", "EBR", "with", "the", "joint", "energy", "model", "consistently", "improves", "the", "performance", "of", "the", "Transformer-based", "NMT", ":", "+", "3.7", "BLEU", "points", "on", "IWSLT\u201914", "German-English", ",", "+", "3.37", "BELU", "points", "on", "Sinhala-English", ",", "+", "1.4", "BLEU", "points", "on", "WMT\u201916", "English-German", "tasks", "."], "entities": [{"type": "Operation", "start": 204, "end": 205, "text": "EBR", "sent_idx": 5}, {"type": "Effect", "start": 213, "end": 214, "text": "performance", "sent_idx": 5}, {"type": "Effect", "start": 221, "end": 223, "text": "BLEU points", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--349"}
{"text": "While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.", "tokens": ["While", "deep", "learning", "models", "are", "making", "fast", "progress", "on", "the", "task", "of", "Natural", "Language", "Inference", ",", "recent", "studies", "have", "also", "shown", "that", "these", "models", "achieve", "high", "accuracy", "by", "exploiting", "several", "dataset", "biases", ",", "and", "without", "deep", "understanding", "of", "the", "language", "semantics", ".", "Using", "contradiction-word", "bias", "and", "word-overlapping", "bias", "as", "our", "two", "bias", "examples", ",", "this", "paper", "explores", "both", "data-level", "and", "model-level", "debiasing", "methods", "to", "robustify", "models", "against", "lexical", "dataset", "biases", ".", "First", ",", "we", "debias", "the", "dataset", "through", "data", "augmentation", "and", "enhancement", ",", "but", "show", "that", "the", "model", "bias", "can", "not", "be", "fully", "removed", "via", "this", "method", ".", "Next", ",", "we", "also", "compare", "two", "ways", "of", "directly", "debiasing", "the", "model", "without", "knowing", "what", "the", "dataset", "biases", "are", "in", "advance", ".", "The", "first", "approach", "aims", "to", "remove", "the", "label", "bias", "at", "the", "embedding", "level", ".", "The", "second", "approach", "employs", "a", "bag-of-words", "sub-model", "to", "capture", "the", "features", "that", "are", "likely", "to", "exploit", "the", "bias", "and", "prevents", "the", "original", "model", "from", "learning", "these", "biased", "features", "by", "forcing", "orthogonality", "between", "these", "two", "sub-models", ".", "We", "performed", "evaluations", "on", "new", "balanced", "datasets", "extracted", "from", "the", "original", "MNLI", "dataset", "as", "well", "as", "the", "NLI", "stress", "tests", ",", "and", "show", "that", "the", "orthogonality", "approach", "is", "better", "at", "debiasing", "the", "model", "while", "maintaining", "competitive", "overall", "accuracy", "."], "entities": [{"type": "Operation", "start": 195, "end": 197, "text": "orthogonality approach", "sent_idx": 6}, {"type": "Effect", "start": 207, "end": 208, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--773"}
{"text": "As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities. Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness. Concretely, we first propose a product-review coherent reasoning module to measure the intra- and inter-modal coherence between the target product and the review. In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task. Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model. The source code and datasets can be obtained from https://github.com/jhliu17/MCR.", "tokens": ["As", "more", "and", "more", "product", "reviews", "are", "posted", "in", "both", "text", "and", "images", ",", "Multimodal", "Review", "Analysis", "(", "MRA", ")", "becomes", "an", "attractive", "research", "topic", ".", "Among", "the", "existing", "review", "analysis", "tasks", ",", "helpfulness", "prediction", "on", "review", "text", "has", "become", "predominant", "due", "to", "its", "importance", "for", "e-commerce", "platforms", "and", "online", "shops", ",", "i.e.", "helping", "customers", "quickly", "acquire", "useful", "product", "information", ".", "This", "paper", "proposes", "a", "new", "task", "Multimodal", "Review", "Helpfulness", "Prediction", "(", "MRHP", ")", "aiming", "to", "analyze", "the", "review", "helpfulness", "from", "text", "and", "visual", "modalities", ".", "Meanwhile", ",", "a", "novel", "Multi-perspective", "Coherent", "Reasoning", "method", "(", "MCR", ")", "is", "proposed", "to", "solve", "the", "MRHP", "task", ",", "which", "conducts", "joint", "reasoning", "over", "texts", "and", "images", "from", "both", "the", "product", "and", "the", "review", ",", "and", "aggregates", "the", "signals", "to", "predict", "the", "review", "helpfulness", ".", "Concretely", ",", "we", "first", "propose", "a", "product-review", "coherent", "reasoning", "module", "to", "measure", "the", "intra-", "and", "inter-modal", "coherence", "between", "the", "target", "product", "and", "the", "review", ".", "In", "addition", ",", "we", "also", "devise", "an", "intra-review", "coherent", "reasoning", "module", "to", "identify", "the", "coherence", "between", "the", "text", "content", "and", "images", "of", "the", "review", ",", "which", "is", "a", "piece", "of", "strong", "evidence", "for", "review", "helpfulness", "prediction", ".", "To", "evaluate", "the", "effectiveness", "of", "MCR", ",", "we", "present", "two", "newly", "collected", "multimodal", "review", "datasets", "as", "benchmark", "evaluation", "resources", "for", "the", "MRHP", "task", ".", "Experimental", "results", "show", "that", "our", "MCR", "method", "can", "lead", "to", "a", "performance", "increase", "of", "up", "to", "8.5", "%", "as", "compared", "to", "the", "best", "performing", "text-only", "model", ".", "The", "source", "code", "and", "datasets", "can", "be", "obtained", "from", "https://github.com/jhliu17/MCR", "."], "entities": [{"type": "Operation", "start": 222, "end": 224, "text": "MCR method", "sent_idx": 7}, {"type": "Effect", "start": 228, "end": 229, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--461"}
{"text": "As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach.", "tokens": ["As", "an", "essential", "task", "in", "task-oriented", "dialog", "systems", ",", "slot", "filling", "requires", "extensive", "training", "data", "in", "a", "certain", "domain", ".", "However", ",", "such", "data", "are", "not", "always", "available", ".", "Hence", ",", "cross-domain", "slot", "filling", "has", "naturally", "arisen", "to", "cope", "with", "this", "data", "scarcity", "problem", ".", "In", "this", "paper", ",", "we", "propose", "a", "Coarse-to-fine", "approach", "(", "Coach", ")", "for", "cross-domain", "slot", "filling", ".", "Our", "model", "first", "learns", "the", "general", "pattern", "of", "slot", "entities", "by", "detecting", "whether", "the", "tokens", "are", "slot", "entities", "or", "not", ".", "It", "then", "predicts", "the", "specific", "types", "for", "the", "slot", "entities", ".", "In", "addition", ",", "we", "propose", "a", "template", "regularization", "approach", "to", "improve", "the", "adaptation", "robustness", "by", "regularizing", "the", "representation", "of", "utterances", "based", "on", "utterance", "templates", ".", "Experimental", "results", "show", "that", "our", "model", "significantly", "outperforms", "state-of-the-art", "approaches", "in", "slot", "filling", ".", "Furthermore", ",", "our", "model", "can", "also", "be", "applied", "to", "the", "cross-domain", "named", "entity", "recognition", "task", ",", "and", "it", "achieves", "better", "adaptation", "performance", "than", "other", "existing", "baselines", ".", "The", "code", "is", "available", "at", "https://github.com/zliucr/coach", "."], "entities": [{"type": "Operation", "start": 100, "end": 103, "text": "template regularization approach", "sent_idx": 6}, {"type": "Effect", "start": 107, "end": 108, "text": "robustness", "sent_idx": 6}, {"type": "Operation", "start": 109, "end": 118, "text": "regularizing the representation of utterances based on utterance templates", "sent_idx": 6}, {"type": "Operation", "start": 52, "end": 57, "text": "Coarse-to-fine approach (Coach)", "sent_idx": 3}, {"type": "Effect", "start": 154, "end": 155, "text": "performance", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 3, "tail": 4}], "id": "abstract-2020--acl-main--3"}
{"text": "Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods. Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce.", "tokens": ["Semi-Supervised", "Text", "Classification", "(", "SSTC", ")", "mainly", "works", "under", "the", "spirit", "of", "self-training", ".", "They", "initialize", "the", "deep", "classifier", "by", "training", "over", "labeled", "texts", ";", "and", "then", "alternatively", "predict", "unlabeled", "texts", "as", "their", "pseudo-labels", "and", "train", "the", "deep", "classifier", "over", "the", "mixture", "of", "labeled", "and", "pseudo-labeled", "texts", ".", "Naturally", ",", "their", "performance", "is", "largely", "affected", "by", "the", "accuracy", "of", "pseudo-labels", "for", "unlabeled", "texts", ".", "Unfortunately", ",", "they", "often", "suffer", "from", "low", "accuracy", "because", "of", "the", "margin", "bias", "problem", "caused", "by", "the", "large", "difference", "between", "representation", "distributions", "of", "labels", "in", "SSTC", ".", "To", "alleviate", "this", "problem", ",", "we", "apply", "the", "angular", "margin", "loss", ",", "and", "perform", "Gaussian", "linear", "transformation", "to", "achieve", "balanced", "label", "angle", "variances", ",", "i.e.", ",", "the", "variance", "of", "label", "angles", "of", "texts", "within", "the", "same", "label", ".", "More", "accuracy", "of", "predicted", "pseudo-labels", "can", "be", "achieved", "by", "constraining", "all", "label", "angle", "variances", "balanced", ",", "where", "they", "are", "estimated", "over", "both", "labeled", "and", "pseudo-labeled", "texts", "during", "self-training", "loops", ".", "With", "this", "insight", ",", "we", "propose", "a", "novel", "SSTC", "method", ",", "namely", "Semi-Supervised", "Text", "Classification", "with", "Balanced", "Deep", "representation", "Distributions", "(", "S2TC-BDD", ")", ".", "To", "evaluate", "S2TC-BDD", ",", "we", "compare", "it", "against", "the", "state-of-the-art", "SSTC", "methods", ".", "Empirical", "results", "demonstrate", "the", "effectiveness", "of", "S2TC-BDD", ",", "especially", "when", "the", "labeled", "texts", "are", "scarce", "."], "entities": [{"type": "Operation", "start": 138, "end": 144, "text": "constraining all label angle variances balanced", "sent_idx": 5}, {"type": "Effect", "start": 130, "end": 131, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--391"}
{"text": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.", "tokens": ["Open-domain", "question", "answering", "can", "be", "formulated", "as", "a", "phrase", "retrieval", "problem", ",", "in", "which", "we", "can", "expect", "huge", "scalability", "and", "speed", "benefit", "but", "often", "suffer", "from", "low", "accuracy", "due", "to", "the", "limitation", "of", "existing", "phrase", "representation", "models", ".", "In", "this", "paper", ",", "we", "aim", "to", "improve", "the", "quality", "of", "each", "phrase", "embedding", "by", "augmenting", "it", "with", "a", "contextualized", "sparse", "representation", "(", "Sparc", ")", ".", "Unlike", "previous", "sparse", "vectors", "that", "are", "term-frequency-based", "(", "e.g.", ",", "tf-idf", ")", "or", "directly", "learned", "(", "only", "few", "thousand", "dimensions", ")", ",", "we", "leverage", "rectified", "self-attention", "to", "indirectly", "learn", "sparse", "vectors", "in", "n-gram", "vocabulary", "space", ".", "By", "augmenting", "the", "previous", "phrase", "retrieval", "model", "(", "Seo", "et", "al.", ",", "2019", ")", "with", "Sparc", ",", "we", "show", "4%+", "improvement", "in", "CuratedTREC", "and", "SQuAD-Open", ".", "Our", "CuratedTREC", "score", "is", "even", "better", "than", "the", "best", "known", "retrieve", "&", "read", "model", "with", "at", "least", "45x", "faster", "inference", "speed", "."], "entities": [{"type": "Operation", "start": 53, "end": 63, "text": "augmenting it with a contextualized sparse representation (Sparc)", "sent_idx": 1}, {"type": "Effect", "start": 127, "end": 129, "text": "CuratedTREC score", "sent_idx": 4}, {"type": "Effect", "start": 145, "end": 147, "text": "inference speed", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--85"}
{"text": "This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.", "tokens": ["This", "paper", "describes", "an", "unsupervised", "dynamic", "graphical", "model", "for", "morphological", "segmentation", "and", "bilingual", "morpheme", "alignment", "for", "statistical", "machine", "translation", ".", "The", "model", "extends", "Hidden", "Semi-Markov", "chain", "models", "by", "using", "factored", "output", "nodes", "and", "special", "structures", "for", "its", "conditional", "probability", "distributions", ".", "It", "relies", "on", "morpho-syntactic", "and", "lexical", "source-side", "information", "(", "part-of-speech", ",", "morphological", "segmentation", ")", "while", "learning", "a", "morpheme", "segmentation", "over", "the", "target", "language", ".", "Our", "model", "outperforms", "a", "competitive", "word", "alignment", "system", "in", "alignment", "quality", ".", "Used", "in", "a", "monolingual", "morphological", "segmentation", "setting", "it", "substantially", "improves", "accuracy", "over", "previous", "state-of-the-art", "models", "on", "three", "Arabic", "and", "Hebrew", "datasets", "."], "entities": [{"type": "Operation", "start": 4, "end": 8, "text": "unsupervised dynamic graphical model", "sent_idx": 0}, {"type": "Effect", "start": 87, "end": 88, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1090"}
{"text": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question\u2019s intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN.", "tokens": ["Video", "Question", "Answering", "is", "a", "task", "which", "requires", "an", "AI", "agent", "to", "answer", "questions", "grounded", "in", "video", ".", "This", "task", "entails", "three", "key", "challenges", ":", "(", "1", ")", "understand", "the", "intention", "of", "various", "questions", ",", "(", "2", ")", "capturing", "various", "elements", "of", "the", "input", "video", "(", "e.g.", ",", "object", ",", "action", ",", "causality", ")", ",", "and", "(", "3", ")", "cross-modal", "grounding", "between", "language", "and", "vision", "information", ".", "We", "propose", "Motion-Appearance", "Synergistic", "Networks", "(", "MASN", ")", ",", "which", "embed", "two", "cross-modal", "features", "grounded", "on", "motion", "and", "appearance", "information", "and", "selectively", "utilize", "them", "depending", "on", "the", "question", "\u2019s", "intentions", ".", "MASN", "consists", "of", "a", "motion", "module", ",", "an", "appearance", "module", ",", "and", "a", "motion-appearance", "fusion", "module", ".", "The", "motion", "module", "computes", "the", "action-oriented", "cross-modal", "joint", "representations", ",", "while", "the", "appearance", "module", "focuses", "on", "the", "appearance", "aspect", "of", "the", "input", "video", ".", "Finally", ",", "the", "motion-appearance", "fusion", "module", "takes", "each", "output", "of", "the", "motion", "module", "and", "the", "appearance", "module", "as", "input", ",", "and", "performs", "question-guided", "fusion", ".", "As", "a", "result", ",", "MASN", "achieves", "new", "state-of-the-art", "performance", "on", "the", "TGIF-QA", "and", "MSVD-QA", "datasets", ".", "We", "also", "conduct", "qualitative", "analysis", "by", "visualizing", "the", "inference", "results", "of", "MASN", "."], "entities": [{"type": "Operation", "start": 168, "end": 169, "text": "MASN", "sent_idx": 6}, {"type": "Effect", "start": 172, "end": 173, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--481"}
{"text": "Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an All-Words unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme.", "tokens": ["Word", "Sense", "Disambiguation", "remains", "one", "of", "the", "most", "complex", "problems", "facing", "computational", "linguists", "to", "date", ".", "In", "this", "paper", "we", "present", "a", "system", "that", "combines", "evidence", "from", "a", "monolingual", "WSD", "system", "together", "with", "that", "from", "a", "multilingual", "WSD", "system", "to", "yield", "state", "of", "the", "art", "performance", "on", "standard", "All-Words", "data", "sets", ".", "The", "monolingual", "system", "is", "based", "on", "a", "modification", "of", "the", "graph", "based", "state", "of", "the", "art", "algorithm", "In-Degree", ".", "The", "multilingual", "system", "is", "an", "improvement", "over", "an", "All-Words", "unsupervised", "approach", ",", "SALAAM", ".", "SALAAM", "exploits", "multilingual", "evidence", "as", "a", "means", "of", "disambiguation", ".", "In", "this", "paper", ",", "we", "present", "modifications", "to", "both", "of", "the", "original", "approaches", "and", "then", "their", "combination", ".", "We", "finally", "report", "the", "highest", "results", "obtained", "to", "date", "on", "the", "SENSEVAL", "2", "standard", "data", "set", "using", "an", "unsupervised", "method", ",", "we", "achieve", "an", "overall", "F", "measure", "of", "64.58", "using", "a", "voting", "scheme", "."], "entities": [{"type": "Operation", "start": 142, "end": 146, "text": "using a voting scheme", "sent_idx": 6}, {"type": "Effect", "start": 138, "end": 140, "text": "F measure", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1156"}
{"text": "Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been widely studied. To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches. These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues.", "tokens": ["Named", "entity", "recognition", "is", "a", "key", "component", "of", "many", "text", "processing", "pipelines", "and", "it", "is", "thus", "essential", "for", "this", "component", "to", "be", "robust", "to", "different", "types", "of", "input", ".", "However", ",", "domain", "transfer", "of", "NER", "models", "with", "data", "from", "multiple", "genres", "has", "not", "been", "widely", "studied", ".", "To", "this", "end", ",", "we", "conduct", "NER", "experiments", "in", "three", "predictive", "setups", "on", "data", "from", ":", "a", ")", "multiple", "domains", ";", "b", ")", "multiple", "domains", "where", "the", "genre", "label", "is", "unknown", "at", "inference", "time", ";", "c", ")", "domains", "not", "encountered", "in", "training", ".", "We", "introduce", "a", "new", "architecture", "tailored", "to", "this", "task", "by", "using", "shared", "and", "private", "domain", "parameters", "and", "multi-task", "learning", ".", "This", "consistently", "outperforms", "all", "other", "baseline", "and", "competitive", "methods", "on", "all", "three", "experimental", "setups", ",", "with", "differences", "ranging", "between", "+", "1.95", "to", "+", "3.11", "average", "F1", "across", "multiple", "genres", "when", "compared", "to", "standard", "approaches", ".", "These", "results", "illustrate", "the", "challenges", "that", "need", "to", "be", "taken", "into", "account", "when", "building", "real-world", "NLP", "applications", "that", "are", "robust", "to", "various", "types", "of", "text", "and", "the", "methods", "that", "can", "help", ",", "at", "least", "partially", ",", "alleviate", "these", "issues", "."], "entities": [{"type": "Operation", "start": 100, "end": 109, "text": "using shared and private domain parameters and multi-task learning", "sent_idx": 3}, {"type": "Effect", "start": 134, "end": 136, "text": "average F1", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--750"}
{"text": "The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographic-identity terms (e.g., \u201cgay\u201d or \u201cblack\u201d) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram). We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.", "tokens": ["The", "element", "of", "repetition", "in", "cyberbullying", "behavior", "has", "directed", "recent", "computational", "studies", "toward", "detecting", "cyberbullying", "based", "on", "a", "social", "media", "session", ".", "In", "contrast", "to", "a", "single", "text", ",", "a", "session", "may", "consist", "of", "an", "initial", "post", "and", "an", "associated", "sequence", "of", "comments", ".", "Yet", ",", "emerging", "efforts", "to", "enhance", "the", "performance", "of", "session-based", "cyberbullying", "detection", "have", "largely", "overlooked", "unintended", "social", "biases", "in", "existing", "cyberbullying", "datasets", ".", "For", "example", ",", "a", "session", "containing", "certain", "demographic-identity", "terms", "(", "e.g.", ",", "\u201c", "gay", "\u201d", "or", "\u201c", "black", "\u201d", ")", "is", "more", "likely", "to", "be", "classified", "as", "an", "instance", "of", "cyberbullying", ".", "In", "this", "paper", ",", "we", "first", "show", "evidence", "of", "such", "bias", "in", "models", "trained", "on", "sessions", "collected", "from", "different", "social", "media", "platforms", "(", "e.g.", ",", "Instagram", ")", ".", "We", "then", "propose", "a", "context-aware", "and", "model-agnostic", "debiasing", "strategy", "that", "leverages", "a", "reinforcement", "learning", "technique", ",", "without", "requiring", "any", "extra", "resources", "or", "annotations", "apart", "from", "a", "pre-defined", "set", "of", "sensitive", "triggers", "commonly", "used", "for", "identifying", "cyberbullying", "instances", ".", "Empirical", "evaluations", "show", "that", "the", "proposed", "strategy", "can", "simultaneously", "alleviate", "the", "impacts", "of", "the", "unintended", "biases", "and", "improve", "the", "detection", "performance", "."], "entities": [{"type": "Operation", "start": 131, "end": 136, "text": "context-aware and model-agnostic debiasing strategy", "sent_idx": 5}, {"type": "Effect", "start": 185, "end": 186, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--168"}
{"text": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.", "tokens": ["In", "this", "work", ",", "we", "explore", "the", "implicit", "event", "argument", "detection", "task", ",", "which", "studies", "event", "arguments", "beyond", "sentence", "boundaries", ".", "The", "addition", "of", "cross-sentence", "argument", "candidates", "imposes", "great", "challenges", "for", "modeling", ".", "To", "reduce", "the", "number", "of", "candidates", ",", "we", "adopt", "a", "two-step", "approach", ",", "decomposing", "the", "problem", "into", "two", "sub-problems", ":", "argument", "head-word", "detection", "and", "head-to-span", "expansion", ".", "Evaluated", "on", "the", "recent", "RAMS", "dataset", "(", "Ebner", "et", "al.", ",", "2020", ")", ",", "our", "model", "achieves", "overall", "better", "performance", "than", "a", "strong", "sequence", "labeling", "baseline", ".", "We", "further", "provide", "detailed", "error", "analysis", ",", "presenting", "where", "the", "model", "mainly", "makes", "errors", "and", "indicating", "directions", "for", "future", "improvements", ".", "It", "remains", "a", "challenge", "to", "detect", "implicit", "arguments", ",", "calling", "for", "more", "future", "work", "of", "document-level", "modeling", "for", "this", "task", "."], "entities": [{"type": "Operation", "start": 46, "end": 52, "text": "decomposing the problem into two sub-problems", "sent_idx": 2}, {"type": "Effect", "start": 79, "end": 80, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--667"}
{"text": "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.", "tokens": ["Adaptive", "policies", "are", "better", "than", "fixed", "policies", "for", "simultaneous", "translation", ",", "since", "they", "can", "flexibly", "balance", "the", "tradeoff", "between", "translation", "quality", "and", "latency", "based", "on", "the", "current", "context", "information", ".", "But", "previous", "methods", "on", "obtaining", "adaptive", "policies", "either", "rely", "on", "complicated", "training", "process", ",", "or", "underperform", "simple", "fixed", "policies", ".", "We", "design", "an", "algorithm", "to", "achieve", "adaptive", "policies", "via", "a", "simple", "heuristic", "composition", "of", "a", "set", "of", "fixed", "policies", ".", "Experiments", "on", "Chinese", "-", ">", "English", "and", "German", "-", ">", "English", "show", "that", "our", "adaptive", "policies", "can", "outperform", "fixed", "ones", "by", "up", "to", "4", "BLEU", "points", "for", "the", "same", "latency", ",", "and", "more", "surprisingly", ",", "it", "even", "surpasses", "the", "BLEU", "score", "of", "full-sentence", "translation", "in", "the", "greedy", "mode", "(", "and", "very", "close", "to", "beam", "mode", ")", ",", "but", "with", "much", "lower", "latency", "."], "entities": [{"type": "Operation", "start": 84, "end": 86, "text": "adaptive policies", "sent_idx": 3}, {"type": "Effect", "start": 94, "end": 96, "text": "BLEU points", "sent_idx": 3}, {"type": "Effect", "start": 109, "end": 111, "text": "BLEU score", "sent_idx": 3}, {"type": "Effect", "start": 131, "end": 132, "text": "latency", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Neg_Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--254"}
{"text": "We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task.", "tokens": ["We", "observe", "that", "(", "1", ")", "how", "a", "given", "named", "entity", "(", "NE", ")", "is", "translated", "(", "i.e.", ",", "either", "semantically", "or", "phonetically", ")", "depends", "greatly", "on", "its", "associated", "entity", "type", ",", "and", "(", "2", ")", "entities", "within", "an", "aligned", "pair", "should", "share", "the", "same", "type", ".", "Also", ",", "(", "3", ")", "those", "initially", "detected", "NEs", "are", "anchors", ",", "whose", "information", "should", "be", "used", "to", "give", "certainty", "scores", "when", "selecting", "candidates", ".", "From", "this", "basis", ",", "an", "integrated", "model", "is", "thus", "proposed", "in", "this", "paper", "to", "jointly", "identify", "and", "align", "bilingual", "named", "entities", "between", "Chinese", "and", "English", ".", "It", "adopts", "a", "new", "mapping", "type", "ratio", "feature", "(", "which", "is", "the", "proportion", "of", "NE", "internal", "tokens", "that", "are", "semantically", "translated", ")", ",", "enforces", "an", "entity", "type", "consistency", "constraint", ",", "and", "utilizes", "additional", "monolingual", "candidate", "certainty", "factors", "(", "based", "on", "those", "NE", "anchors", ")", ".", "The", "experiments", "show", "that", "this", "novel", "approach", "has", "substantially", "raised", "the", "type-sensitive", "F-score", "of", "identified", "NE-pairs", "from", "68.4", "%", "to", "81.7", "%", "(", "42.1", "%", "F-score", "imperfection", "reduction", ")", "in", "our", "Chinese-English", "NE", "alignment", "task", "."], "entities": [{"type": "Operation", "start": 102, "end": 106, "text": "mapping type ratio feature", "sent_idx": 3}, {"type": "Effect", "start": 155, "end": 156, "text": "F-score", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1065"}
{"text": "Determining the semantic intent of web queries not only involves identifying their semantic class, which is a primary focus of previous works, but also understanding their semantic structure. In this work, we formally define the semantic structure of noun phrase queries as comprised of intent heads and intent modifiers. We present methods that automatically identify these constituents as well as their semantic roles based on Markov and semi-Markov conditional random fields. We show that the use of semantic features and syntactic features significantly contribute to improving the understanding performance.", "tokens": ["Determining", "the", "semantic", "intent", "of", "web", "queries", "not", "only", "involves", "identifying", "their", "semantic", "class", ",", "which", "is", "a", "primary", "focus", "of", "previous", "works", ",", "but", "also", "understanding", "their", "semantic", "structure", ".", "In", "this", "work", ",", "we", "formally", "define", "the", "semantic", "structure", "of", "noun", "phrase", "queries", "as", "comprised", "of", "intent", "heads", "and", "intent", "modifiers", ".", "We", "present", "methods", "that", "automatically", "identify", "these", "constituents", "as", "well", "as", "their", "semantic", "roles", "based", "on", "Markov", "and", "semi-Markov", "conditional", "random", "fields", ".", "We", "show", "that", "the", "use", "of", "semantic", "features", "and", "syntactic", "features", "significantly", "contribute", "to", "improving", "the", "understanding", "performance", "."], "entities": [{"type": "Operation", "start": 81, "end": 88, "text": "use of semantic features and syntactic features", "sent_idx": 3}, {"type": "Effect", "start": 94, "end": 95, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1136"}
{"text": "Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead \u201creallocate\u201d them\u2014the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over \u201ctransformer-base\u201d by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.", "tokens": ["Multi-head", "attentive", "neural", "architectures", "have", "achieved", "state-of-the-art", "results", "on", "a", "variety", "of", "natural", "language", "processing", "tasks", ".", "Evidence", "has", "shown", "that", "they", "are", "overparameterized", ";", "attention", "heads", "can", "be", "pruned", "without", "significant", "performance", "loss", ".", "In", "this", "work", ",", "we", "instead", "\u201c", "reallocate", "\u201d", "them", "\u2014", "the", "model", "learns", "to", "activate", "different", "heads", "on", "different", "inputs", ".", "Drawing", "connections", "between", "multi-head", "attention", "and", "mixture", "of", "experts", ",", "we", "propose", "the", "mixture", "of", "attentive", "experts", "model", "(", "MAE", ")", ".", "MAE", "is", "trained", "using", "a", "block", "coordinate", "descent", "algorithm", "that", "alternates", "between", "updating", "(", "1", ")", "the", "responsibilities", "of", "the", "experts", "and", "(", "2", ")", "their", "parameters", ".", "Experiments", "on", "machine", "translation", "and", "language", "modeling", "show", "that", "MAE", "outperforms", "strong", "baselines", "on", "both", "tasks", ".", "Particularly", ",", "on", "the", "WMT14", "English", "to", "German", "translation", "dataset", ",", "MAE", "improves", "over", "\u201c", "transformer-base", "\u201d", "by", "0.8", "BLEU", ",", "with", "a", "comparable", "number", "of", "parameters", ".", "Our", "analysis", "shows", "that", "our", "model", "learns", "to", "specialize", "different", "experts", "to", "different", "inputs", "."], "entities": [{"type": "Operation", "start": 135, "end": 136, "text": "MAE", "sent_idx": 6}, {"type": "Effect", "start": 143, "end": 144, "text": "BLEU", "sent_idx": 6}, {"type": "Effect", "start": 148, "end": 151, "text": "number of parameters", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--587"}
{"text": "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) \u201cexit\u201d from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.", "tokens": ["As", "NLP", "models", "become", "larger", ",", "executing", "a", "trained", "model", "requires", "significant", "computational", "resources", "incurring", "monetary", "and", "environmental", "costs", ".", "To", "better", "respect", "a", "given", "inference", "budget", ",", "we", "propose", "a", "modification", "to", "contextual", "representation", "fine-tuning", "which", ",", "during", "inference", ",", "allows", "for", "an", "early", "(", "and", "fast", ")", "\u201c", "exit", "\u201d", "from", "neural", "network", "calculations", "for", "simple", "instances", ",", "and", "late", "(", "and", "accurate", ")", "exit", "for", "hard", "instances", ".", "To", "achieve", "this", ",", "we", "add", "classifiers", "to", "different", "layers", "of", "BERT", "and", "use", "their", "calibrated", "confidence", "scores", "to", "make", "early", "exit", "decisions", ".", "We", "test", "our", "proposed", "modification", "on", "five", "different", "datasets", "in", "two", "tasks", ":", "three", "text", "classification", "datasets", "and", "two", "natural", "language", "inference", "benchmarks", ".", "Our", "method", "presents", "a", "favorable", "speed/accuracy", "tradeoff", "in", "almost", "all", "cases", ",", "producing", "models", "which", "are", "up", "to", "five", "times", "faster", "than", "the", "state", "of", "the", "art", ",", "while", "preserving", "their", "accuracy", ".", "Our", "method", "also", "requires", "almost", "no", "additional", "training", "resources", "(", "in", "either", "time", "or", "parameters", ")", "compared", "to", "the", "baseline", "BERT", "model", ".", "Finally", ",", "our", "method", "alleviates", "the", "need", "for", "costly", "retraining", "of", "multiple", "models", "at", "different", "levels", "of", "efficiency", ";", "we", "allow", "users", "to", "control", "the", "inference", "speed/accuracy", "tradeoff", "using", "a", "single", "trained", "model", ",", "by", "setting", "a", "single", "variable", "at", "inference", "time", ".", "We", "publicly", "release", "our", "code", "."], "entities": [{"type": "Operation", "start": 76, "end": 83, "text": "add classifiers to different layers of BERT", "sent_idx": 2}, {"type": "Effect", "start": 201, "end": 202, "text": "speed/accuracy", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--593"}
{"text": "Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model\u2019s performance by transferring teacher model\u2019s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples\u2019 partitions. Based on above protocol, we conduct extensive experiments and find that the teacher\u2019s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT\u201914 English-German and WMT\u201919 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.", "tokens": ["Neural", "Machine", "Translation", "(", "NMT", ")", "models", "achieve", "state-of-the-art", "performance", "on", "many", "translation", "benchmarks", ".", "As", "an", "active", "research", "field", "in", "NMT", ",", "knowledge", "distillation", "is", "widely", "applied", "to", "enhance", "the", "model", "\u2019s", "performance", "by", "transferring", "teacher", "model", "\u2019s", "knowledge", "on", "each", "training", "sample", ".", "However", ",", "previous", "work", "rarely", "discusses", "the", "different", "impacts", "and", "connections", "among", "these", "samples", ",", "which", "serve", "as", "the", "medium", "for", "transferring", "teacher", "knowledge", ".", "In", "this", "paper", ",", "we", "design", "a", "novel", "protocol", "that", "can", "effectively", "analyze", "the", "different", "impacts", "of", "samples", "by", "comparing", "various", "samples", "\u2019", "partitions", ".", "Based", "on", "above", "protocol", ",", "we", "conduct", "extensive", "experiments", "and", "find", "that", "the", "teacher", "\u2019s", "knowledge", "is", "not", "the", "more", ",", "the", "better", ".", "Knowledge", "over", "specific", "samples", "may", "even", "hurt", "the", "whole", "performance", "of", "knowledge", "distillation", ".", "Finally", ",", "to", "address", "these", "issues", ",", "we", "propose", "two", "simple", "yet", "effective", "strategies", ",", "i.e.", ",", "batch-level", "and", "global-level", "selections", ",", "to", "pick", "suitable", "samples", "for", "distillation", ".", "We", "evaluate", "our", "approaches", "on", "two", "large-scale", "machine", "translation", "tasks", ",", "WMT\u201914", "English-German", "and", "WMT\u201919", "Chinese-English", ".", "Experimental", "results", "show", "that", "our", "approaches", "yield", "up", "to", "+", "1.28", "and", "+", "0.89", "BLEU", "points", "improvements", "over", "the", "Transformer", "baseline", ",", "respectively", "."], "entities": [{"type": "Operation", "start": 150, "end": 154, "text": "batch-level and global-level selections", "sent_idx": 6}, {"type": "Effect", "start": 193, "end": 195, "text": "BLEU points", "sent_idx": 8}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--504"}
{"text": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers \u2013 groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.", "tokens": ["Active", "learning", "promises", "to", "alleviate", "the", "massive", "data", "needs", "of", "supervised", "machine", "learning", ":", "it", "has", "successfully", "improved", "sample", "efficiency", "by", "an", "order", "of", "magnitude", "on", "traditional", "tasks", "like", "topic", "classification", "and", "object", "recognition", ".", "However", ",", "we", "uncover", "a", "striking", "contrast", "to", "this", "promise", ":", "across", "5", "models", "and", "4", "datasets", "on", "the", "task", "of", "visual", "question", "answering", ",", "a", "wide", "variety", "of", "active", "learning", "approaches", "fail", "to", "outperform", "random", "selection", ".", "To", "understand", "this", "discrepancy", ",", "we", "profile", "8", "active", "learning", "methods", "on", "a", "per-example", "basis", ",", "and", "identify", "the", "problem", "as", "collective", "outliers", "\u2013", "groups", "of", "examples", "that", "active", "learning", "methods", "prefer", "to", "acquire", "but", "models", "fail", "to", "learn", "(", "e.g.", ",", "questions", "that", "ask", "about", "text", "in", "images", "or", "require", "external", "knowledge", ")", ".", "Through", "systematic", "ablation", "experiments", "and", "qualitative", "visualizations", ",", "we", "verify", "that", "collective", "outliers", "are", "a", "general", "phenomenon", "responsible", "for", "degrading", "pool-based", "active", "learning", ".", "Notably", ",", "we", "show", "that", "active", "learning", "sample", "efficiency", "increases", "significantly", "as", "the", "number", "of", "collective", "outliers", "in", "the", "active", "learning", "pool", "decreases", ".", "We", "conclude", "with", "a", "discussion", "and", "prescriptive", "recommendations", "for", "mitigating", "the", "effects", "of", "these", "outliers", "in", "future", "work", "."], "entities": [{"type": "Operation", "start": 165, "end": 175, "text": "number of collective outliers in the active learning pool decreases", "sent_idx": 4}, {"type": "Effect", "start": 157, "end": 161, "text": "active learning sample efficiency", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--564"}
{"text": "In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node\u2019s children. Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called \u201crecursive semi-Markov model\u201d. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.", "tokens": ["In", "this", "paper", ",", "we", "study", "the", "task", "of", "graph-based", "constituent", "parsing", "in", "the", "setting", "that", "binarization", "is", "not", "conducted", "as", "a", "pre-processing", "step", ",", "where", "a", "constituent", "tree", "may", "consist", "of", "nodes", "with", "more", "than", "two", "children", ".", "Previous", "graph-based", "methods", "on", "this", "setting", "typically", "generate", "hidden", "nodes", "with", "the", "dummy", "label", "inside", "the", "n-ary", "nodes", ",", "in", "order", "to", "transform", "the", "tree", "into", "a", "binary", "tree", "for", "prediction", ".", "The", "limitation", "is", "that", "the", "hidden", "nodes", "break", "the", "sibling", "relations", "of", "the", "n-ary", "node", "\u2019s", "children", ".", "Consequently", ",", "the", "dependencies", "of", "such", "sibling", "constituents", "might", "not", "be", "accurately", "modeled", "and", "is", "being", "ignored", ".", "To", "solve", "this", "limitation", ",", "we", "propose", "a", "novel", "graph-based", "framework", ",", "which", "is", "called", "\u201c", "recursive", "semi-Markov", "model", "\u201d", ".", "The", "main", "idea", "is", "to", "utilize", "1-order", "semi-Markov", "model", "to", "predict", "the", "immediate", "children", "sequence", "of", "a", "constituent", "candidate", ",", "which", "then", "recursively", "serves", "as", "a", "child", "candidate", "of", "its", "parent", ".", "In", "this", "manner", ",", "the", "dependencies", "of", "sibling", "constituents", "can", "be", "described", "by", "1-order", "transition", "features", ",", "which", "solves", "the", "above", "limitation", ".", "Through", "experiments", ",", "the", "proposed", "framework", "obtains", "the", "F1", "of", "95.92", "%", "and", "92.50", "%", "on", "the", "datasets", "of", "PTB", "and", "CTB", "5.1", "respectively", ".", "Specially", ",", "the", "recursive", "semi-Markov", "model", "shows", "advantages", "in", "modeling", "nodes", "with", "more", "than", "two", "children", ",", "whose", "average", "F1", "can", "be", "improved", "by", "0.3", "-", "1.1", "points", "in", "PTB", "and", "2.3", "-", "6.8", "points", "in", "CTB", "5.1", "."], "entities": [{"type": "Operation", "start": 211, "end": 214, "text": "recursive semi-Markov model", "sent_idx": 8}, {"type": "Effect", "start": 227, "end": 228, "text": "F1", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--205"}
{"text": "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.", "tokens": ["News", "recommendation", "is", "an", "important", "technique", "for", "personalized", "news", "service", ".", "Compared", "with", "product", "and", "movie", "recommendations", "which", "have", "been", "comprehensively", "studied", ",", "the", "research", "on", "news", "recommendation", "is", "much", "more", "limited", ",", "mainly", "due", "to", "the", "lack", "of", "a", "high-quality", "benchmark", "dataset", ".", "In", "this", "paper", ",", "we", "present", "a", "large-scale", "dataset", "named", "MIND", "for", "news", "recommendation", ".", "Constructed", "from", "the", "user", "click", "logs", "of", "Microsoft", "News", ",", "MIND", "contains", "1", "million", "users", "and", "more", "than", "160k", "English", "news", "articles", ",", "each", "of", "which", "has", "rich", "textual", "content", "such", "as", "title", ",", "abstract", "and", "body", ".", "We", "demonstrate", "MIND", "a", "good", "testbed", "for", "news", "recommendation", "through", "a", "comparative", "study", "of", "several", "state-of-the-art", "news", "recommendation", "methods", "which", "are", "originally", "developed", "on", "different", "proprietary", "datasets", ".", "Our", "results", "show", "the", "performance", "of", "news", "recommendation", "highly", "relies", "on", "the", "quality", "of", "news", "content", "understanding", "and", "user", "interest", "modeling", ".", "Many", "natural", "language", "processing", "techniques", "such", "as", "effective", "text", "representation", "methods", "and", "pre-trained", "language", "models", "can", "effectively", "improve", "the", "performance", "of", "news", "recommendation", ".", "The", "MIND", "dataset", "will", "be", "available", "at", "https://msnews.github.io", "."], "entities": [{"type": "Operation", "start": 155, "end": 158, "text": "text representation methods", "sent_idx": 6}, {"type": "Effect", "start": 166, "end": 167, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 159, "end": 162, "text": "pre-trained language models", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--331"}
{"text": "Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L\u221e norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.", "tokens": ["Pooling", "is", "an", "important", "technique", "for", "learning", "text", "representations", "in", "many", "neural", "NLP", "models", ".", "In", "conventional", "pooling", "methods", "such", "as", "average", ",", "max", "and", "attentive", "pooling", ",", "text", "representations", "are", "weighted", "summations", "of", "the", "L1", "or", "L\u221e", "norm", "of", "input", "features", ".", "However", ",", "their", "pooling", "norms", "are", "always", "fixed", "and", "may", "not", "be", "optimal", "for", "learning", "accurate", "text", "representations", "in", "different", "tasks", ".", "In", "addition", ",", "in", "many", "popular", "pooling", "methods", "such", "as", "max", "and", "attentive", "pooling", "some", "features", "may", "be", "over-emphasized", ",", "while", "other", "useful", "ones", "are", "not", "fully", "exploited", ".", "In", "this", "paper", ",", "we", "propose", "an", "Attentive", "Pooling", "with", "Learnable", "Norms", "(", "APLN", ")", "approach", "for", "text", "representation", ".", "Different", "from", "existing", "pooling", "methods", "that", "use", "a", "fixed", "pooling", "norm", ",", "we", "propose", "to", "learn", "the", "norm", "in", "an", "end-to-end", "manner", "to", "automatically", "find", "the", "optimal", "ones", "for", "text", "representation", "in", "different", "tasks", ".", "In", "addition", ",", "we", "propose", "two", "methods", "to", "ensure", "the", "numerical", "stability", "of", "the", "model", "training", ".", "The", "first", "one", "is", "scale", "limiting", ",", "which", "re-scales", "the", "input", "to", "ensure", "non-negativity", "and", "alleviate", "the", "risk", "of", "exponential", "explosion", ".", "The", "second", "one", "is", "re-formulation", ",", "which", "decomposes", "the", "exponent", "operation", "to", "avoid", "computing", "the", "real-valued", "powers", "of", "the", "input", "and", "further", "accelerate", "the", "pooling", "operation", ".", "Experimental", "results", "on", "four", "benchmark", "datasets", "show", "that", "our", "approach", "can", "effectively", "improve", "the", "performance", "of", "attentive", "pooling", "."], "entities": [{"type": "Operation", "start": 101, "end": 110, "text": "Attentive Pooling with Learnable Norms (APLN) approach", "sent_idx": 4}, {"type": "Effect", "start": 229, "end": 233, "text": "performance of attentive pooling", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--267"}
{"text": "Named entity recognition (NER) is a well-studied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.", "tokens": ["Named", "entity", "recognition", "(", "NER", ")", "is", "a", "well-studied", "task", "in", "natural", "language", "processing", ".", "Traditional", "NER", "research", "only", "deals", "with", "flat", "entities", "and", "ignores", "nested", "entities", ".", "The", "span-based", "methods", "treat", "entity", "recognition", "as", "a", "span", "classification", "task", ".", "Although", "these", "methods", "have", "the", "innate", "ability", "to", "handle", "nested", "NER", ",", "they", "suffer", "from", "high", "computational", "cost", ",", "ignorance", "of", "boundary", "information", ",", "under-utilization", "of", "the", "spans", "that", "partially", "match", "with", "entities", ",", "and", "difficulties", "in", "long", "entity", "recognition", ".", "To", "tackle", "these", "issues", ",", "we", "propose", "a", "two-stage", "entity", "identifier", ".", "First", "we", "generate", "span", "proposals", "by", "filtering", "and", "boundary", "regression", "on", "the", "seed", "spans", "to", "locate", "the", "entities", ",", "and", "then", "label", "the", "boundary-adjusted", "span", "proposals", "with", "the", "corresponding", "categories", ".", "Our", "method", "effectively", "utilizes", "the", "boundary", "information", "of", "entities", "and", "partially", "matched", "spans", "during", "training", ".", "Through", "boundary", "regression", ",", "entities", "of", "any", "length", "can", "be", "covered", "theoretically", ",", "which", "improves", "the", "ability", "to", "recognize", "long", "entities", ".", "In", "addition", ",", "many", "low-quality", "seed", "spans", "are", "filtered", "out", "in", "the", "first", "stage", ",", "which", "reduces", "the", "time", "complexity", "of", "inference", ".", "Experiments", "on", "nested", "NER", "datasets", "demonstrate", "that", "our", "proposed", "method", "outperforms", "previous", "state-of-the-art", "models", "."], "entities": [{"type": "Operation", "start": 165, "end": 176, "text": "many low-quality seed spans are filtered out in the first stage", "sent_idx": 8}, {"type": "Effect", "start": 180, "end": 184, "text": "time complexity of inference", "sent_idx": 8}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--216"}
{"text": "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.", "tokens": ["Large-scale", "pre-trained", "language", "models", "such", "as", "BERT", "have", "brought", "significant", "improvements", "to", "NLP", "applications", ".", "However", ",", "they", "are", "also", "notorious", "for", "being", "slow", "in", "inference", ",", "which", "makes", "them", "difficult", "to", "deploy", "in", "real-time", "applications", ".", "We", "propose", "a", "simple", "but", "effective", "method", ",", "DeeBERT", ",", "to", "accelerate", "BERT", "inference", ".", "Our", "approach", "allows", "samples", "to", "exit", "earlier", "without", "passing", "through", "the", "entire", "model", ".", "Experiments", "show", "that", "DeeBERT", "is", "able", "to", "save", "up", "to", "~40", "%", "inference", "time", "with", "minimal", "degradation", "in", "model", "quality", ".", "Further", "analyses", "show", "different", "behaviors", "in", "the", "BERT", "transformer", "layers", "and", "also", "reveal", "their", "redundancy", ".", "Our", "work", "provides", "new", "ideas", "to", "efficiently", "apply", "deep", "transformer-based", "models", "to", "downstream", "tasks", ".", "Code", "is", "available", "at", "https://github.com/castorini/DeeBERT", "."], "entities": [{"type": "Operation", "start": 69, "end": 70, "text": "DeeBERT", "sent_idx": 4}, {"type": "Effect", "start": 78, "end": 80, "text": "inference time", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--204"}
{"text": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.", "tokens": ["Recent", "work", "in", "neural", "machine", "translation", "has", "demonstrated", "both", "the", "necessity", "and", "feasibility", "of", "using", "inter-sentential", "context", ",", "context", "from", "sentences", "other", "than", "those", "currently", "being", "translated", ".", "However", ",", "while", "many", "current", "methods", "present", "model", "architectures", "that", "theoretically", "can", "use", "this", "extra", "context", ",", "it", "is", "often", "not", "clear", "how", "much", "they", "do", "actually", "utilize", "it", "at", "translation", "time", ".", "In", "this", "paper", ",", "we", "introduce", "a", "new", "metric", ",", "conditional", "cross-mutual", "information", ",", "to", "quantify", "usage", "of", "context", "by", "these", "models", ".", "Using", "this", "metric", ",", "we", "measure", "how", "much", "document-level", "machine", "translation", "systems", "use", "particular", "varieties", "of", "context", ".", "We", "find", "that", "target", "context", "is", "referenced", "more", "than", "source", "context", ",", "and", "that", "including", "more", "context", "has", "a", "diminishing", "affect", "on", "results", ".", "We", "then", "introduce", "a", "new", ",", "simple", "training", "method", ",", "context-aware", "word", "dropout", ",", "to", "increase", "the", "usage", "of", "context", "by", "context-aware", "models", ".", "Experiments", "show", "that", "our", "method", "not", "only", "increases", "context", "usage", ",", "but", "also", "improves", "the", "translation", "quality", "according", "to", "metrics", "such", "as", "BLEU", "and", "COMET", ",", "as", "well", "as", "performance", "on", "anaphoric", "pronoun", "resolution", "and", "lexical", "cohesion", "contrastive", "datasets", "."], "entities": [{"type": "Operation", "start": 71, "end": 74, "text": "conditional cross-mutual information", "sent_idx": 2}, {"type": "Effect", "start": 158, "end": 160, "text": "context usage", "sent_idx": 6}, {"type": "Effect", "start": 165, "end": 167, "text": "translation quality", "sent_idx": 6}, {"type": "Effect", "start": 172, "end": 173, "text": "BLEU", "sent_idx": 6}, {"type": "Effect", "start": 174, "end": 175, "text": "COMET", "sent_idx": 6}, {"type": "Operation", "start": 136, "end": 139, "text": "context-aware word dropout", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 0, "tail": 4}, {"type": "Pos_Affect", "head": 5, "tail": 1}, {"type": "Pos_Affect", "head": 5, "tail": 2}, {"type": "Pos_Affect", "head": 5, "tail": 3}, {"type": "Pos_Affect", "head": 5, "tail": 4}], "id": "abstract-2021--acl-long--505"}
{"text": "Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions. In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark. The results show that our proposed framework outperformed vanilla LAMOL on most permutations. Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales.", "tokens": ["Lifelong", "learning", "(", "LL", ")", "aims", "to", "train", "a", "neural", "network", "on", "a", "stream", "of", "tasks", "while", "retaining", "knowledge", "from", "previous", "tasks", ".", "However", ",", "many", "prior", "attempts", "in", "NLP", "still", "suffer", "from", "the", "catastrophic", "forgetting", "issue", ",", "where", "the", "model", "completely", "forgets", "what", "it", "just", "learned", "in", "the", "previous", "tasks", ".", "In", "this", "paper", ",", "we", "introduce", "Rational", "LAMOL", ",", "a", "novel", "end-to-end", "LL", "framework", "for", "language", "models", ".", "In", "order", "to", "alleviate", "catastrophic", "forgetting", ",", "Rational", "LAMOL", "enhances", "LAMOL", ",", "a", "recent", "LL", "model", ",", "by", "applying", "critical", "freezing", "guided", "by", "human", "rationales", ".", "When", "the", "human", "rationales", "are", "not", "available", ",", "we", "propose", "exploiting", "unsupervised", "generated", "rationales", "as", "substitutions", ".", "In", "the", "experiment", ",", "we", "tested", "Rational", "LAMOL", "on", "permutations", "of", "three", "datasets", "from", "the", "ERASER", "benchmark", ".", "The", "results", "show", "that", "our", "proposed", "framework", "outperformed", "vanilla", "LAMOL", "on", "most", "permutations", ".", "Furthermore", ",", "unsupervised", "rationale", "generation", "was", "able", "to", "consistently", "improve", "the", "overall", "LL", "performance", "from", "the", "baseline", "without", "relying", "on", "human-annotated", "rationales", "."], "entities": [{"type": "Operation", "start": 147, "end": 150, "text": "unsupervised rationale generation", "sent_idx": 7}, {"type": "Effect", "start": 158, "end": 159, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--229"}
{"text": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.", "tokens": ["We", "present", "an", "approach", "to", "multilingual", "grammar", "induction", "that", "exploits", "a", "phylogeny-structured", "model", "of", "parameter", "drift", ".", "Our", "method", "does", "not", "require", "any", "translated", "texts", "or", "token-level", "alignments", ".", "Instead", ",", "the", "phylogenetic", "prior", "couples", "languages", "at", "a", "parameter", "level", ".", "Joint", "induction", "in", "the", "multilingual", "model", "substantially", "outperforms", "independent", "learning", ",", "with", "larger", "gains", "both", "from", "more", "articulated", "phylogenies", "and", "as", "well", "as", "from", "increasing", "numbers", "of", "languages", ".", "Across", "eight", "languages", ",", "the", "multilingual", "approach", "gives", "error", "reductions", "over", "the", "standard", "monolingual", "DMV", "averaging", "21.1", "%", "and", "reaching", "as", "high", "as", "39", "%", "."], "entities": [{"type": "Operation", "start": 74, "end": 77, "text": "the multilingual approach", "sent_idx": 4}, {"type": "Effect", "start": 78, "end": 79, "text": "error", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1131"}
{"text": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model\u2019s ability to extract entity mention features; (2) the performance of attention is largely influenced by various noise distribution patterns, which is closely related to real-world datasets; (3) KG-enhanced attention indeed improves RE performance, while not through enhanced attention but by incorporating entity prior; and (4) attention mechanism may exacerbate the issue of insufficient training data. Based on these findings, we show that a straightforward variant of RE model can achieve significant improvements (6% AUC on average) on two real-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at https://github.com/zig-kwin-hu/how-KG-ATT-help.", "tokens": ["Knowledge", "Graph", "(", "KG", ")", "and", "attention", "mechanism", "have", "been", "demonstrated", "effective", "in", "introducing", "and", "selecting", "useful", "information", "for", "weakly", "supervised", "methods", ".", "However", ",", "only", "qualitative", "analysis", "and", "ablation", "study", "are", "provided", "as", "evidence", ".", "In", "this", "paper", ",", "we", "contribute", "a", "dataset", "and", "propose", "a", "paradigm", "to", "quantitatively", "evaluate", "the", "effect", "of", "attention", "and", "KG", "on", "bag-level", "relation", "extraction", "(", "RE", ")", ".", "We", "find", "that", "(", "1", ")", "higher", "attention", "accuracy", "may", "lead", "to", "worse", "performance", "as", "it", "may", "harm", "the", "model", "\u2019s", "ability", "to", "extract", "entity", "mention", "features", ";", "(", "2", ")", "the", "performance", "of", "attention", "is", "largely", "influenced", "by", "various", "noise", "distribution", "patterns", ",", "which", "is", "closely", "related", "to", "real-world", "datasets", ";", "(", "3", ")", "KG-enhanced", "attention", "indeed", "improves", "RE", "performance", ",", "while", "not", "through", "enhanced", "attention", "but", "by", "incorporating", "entity", "prior", ";", "and", "(", "4", ")", "attention", "mechanism", "may", "exacerbate", "the", "issue", "of", "insufficient", "training", "data", ".", "Based", "on", "these", "findings", ",", "we", "show", "that", "a", "straightforward", "variant", "of", "RE", "model", "can", "achieve", "significant", "improvements", "(", "6", "%", "AUC", "on", "average", ")", "on", "two", "real-world", "datasets", "as", "compared", "with", "three", "state-of-the-art", "baselines", ".", "Our", "codes", "and", "datasets", "are", "available", "at", "https://github.com/zig-kwin-hu/how-KG-ATT-help", "."], "entities": [{"type": "Operation", "start": 162, "end": 167, "text": "straightforward variant of RE model", "sent_idx": 4}, {"type": "Effect", "start": 174, "end": 175, "text": "AUC", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--359"}
{"text": "Arabic handwriting recognition (HR) is a challenging problem due to Arabic's connected letter forms, consonantal diacritics and rich morphology. In this paper we isolate the task of identification of erroneous words in HR from the task of producing corrections for these words. We consider a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify these errors. Our best approach achieves a roughly ~15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words.", "tokens": ["Arabic", "handwriting", "recognition", "(", "HR", ")", "is", "a", "challenging", "problem", "due", "to", "Arabic", "'s", "connected", "letter", "forms", ",", "consonantal", "diacritics", "and", "rich", "morphology", ".", "In", "this", "paper", "we", "isolate", "the", "task", "of", "identification", "of", "erroneous", "words", "in", "HR", "from", "the", "task", "of", "producing", "corrections", "for", "these", "words", ".", "We", "consider", "a", "variety", "of", "linguistic", "(", "morphological", "and", "syntactic", ")", "and", "non-linguistic", "features", "to", "automatically", "identify", "these", "errors", ".", "Our", "best", "approach", "achieves", "a", "roughly", "~15", "%", "absolute", "increase", "in", "F-score", "over", "a", "simple", "but", "reasonable", "baseline", ".", "A", "detailed", "error", "analysis", "shows", "that", "linguistic", "features", ",", "such", "as", "lemma", "(", "i.e.", ",", "citation", "form", ")", "models", ",", "help", "improve", "HR-error", "detection", "precisely", "where", "we", "expect", "them", "to", ":", "semantically", "incoherent", "error", "words", "."], "entities": [{"type": "Operation", "start": 98, "end": 106, "text": "lemma (i.e., citation form) models", "sent_idx": 4}, {"type": "Effect", "start": 79, "end": 80, "text": "F-score", "sent_idx": 3}, {"type": "Operation", "start": 93, "end": 95, "text": "linguistic features", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "P11-1088"}
{"text": "Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.", "tokens": ["Training", "on", "only", "perfect", "Standard", "English", "corpora", "predisposes", "pre-trained", "neural", "networks", "to", "discriminate", "against", "minorities", "from", "non-standard", "linguistic", "backgrounds", "(", "e.g.", ",", "African", "American", "Vernacular", "English", ",", "Colloquial", "Singapore", "English", ",", "etc", ".", ")", ".", "We", "perturb", "the", "inflectional", "morphology", "of", "words", "to", "craft", "plausible", "and", "semantically", "similar", "adversarial", "examples", "that", "expose", "these", "biases", "in", "popular", "NLP", "models", ",", "e.g.", ",", "BERT", "and", "Transformer", ",", "and", "show", "that", "adversarially", "fine-tuning", "them", "for", "a", "single", "epoch", "significantly", "improves", "robustness", "without", "sacrificing", "performance", "on", "clean", "data", "."], "entities": [{"type": "Operation", "start": 68, "end": 75, "text": "adversarially fine-tuning them for a single epoch", "sent_idx": 1}, {"type": "Effect", "start": 77, "end": 78, "text": "robustness", "sent_idx": 1}, {"type": "Effect", "start": 80, "end": 81, "text": "performance", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--263"}
{"text": "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.", "tokens": ["When", "training", "multilingual", "machine", "translation", "(", "MT", ")", "models", "that", "can", "translate", "to/from", "multiple", "languages", ",", "we", "are", "faced", "with", "imbalanced", "training", "sets", ":", "some", "languages", "have", "much", "more", "training", "data", "than", "others", ".", "Standard", "practice", "is", "to", "up-sample", "less", "resourced", "languages", "to", "increase", "representation", ",", "and", "the", "degree", "of", "up-sampling", "has", "a", "large", "effect", "on", "the", "overall", "performance", ".", "In", "this", "paper", ",", "we", "propose", "a", "method", "that", "instead", "automatically", "learns", "how", "to", "weight", "training", "data", "through", "a", "data", "scorer", "that", "is", "optimized", "to", "maximize", "performance", "on", "all", "test", "languages", ".", "Experiments", "on", "two", "sets", "of", "languages", "under", "both", "one-to-many", "and", "many-to-one", "MT", "settings", "show", "our", "method", "not", "only", "consistently", "outperforms", "heuristic", "baselines", "in", "terms", "of", "average", "performance", ",", "but", "also", "offers", "flexible", "control", "over", "the", "performance", "of", "which", "languages", "are", "optimized", "."], "entities": [{"type": "Operation", "start": 70, "end": 81, "text": "automatically learns how to weight training data through a data scorer", "sent_idx": 2}, {"type": "Effect", "start": 117, "end": 119, "text": "average performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--754"}
{"text": "Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.", "tokens": ["Multilingual", "neural", "machine", "translation", "has", "shown", "the", "capability", "of", "directly", "translating", "between", "language", "pairs", "unseen", "in", "training", ",", "i.e.", "zero-shot", "translation", ".", "Despite", "being", "conceptually", "attractive", ",", "it", "often", "suffers", "from", "low", "output", "quality", ".", "The", "difficulty", "of", "generalizing", "to", "new", "translation", "directions", "suggests", "the", "model", "representations", "are", "highly", "specific", "to", "those", "language", "pairs", "seen", "in", "training", ".", "We", "demonstrate", "that", "a", "main", "factor", "causing", "the", "language-specific", "representations", "is", "the", "positional", "correspondence", "to", "input", "tokens", ".", "We", "show", "that", "this", "can", "be", "easily", "alleviated", "by", "removing", "residual", "connections", "in", "an", "encoder", "layer", ".", "With", "this", "modification", ",", "we", "gain", "up", "to", "18.5", "BLEU", "points", "on", "zero-shot", "translation", "while", "retaining", "quality", "on", "supervised", "directions", ".", "The", "improvements", "are", "particularly", "prominent", "between", "related", "languages", ",", "where", "our", "proposed", "model", "outperforms", "pivot-based", "translation", ".", "Moreover", ",", "our", "approach", "allows", "easy", "integration", "of", "new", "languages", ",", "which", "substantially", "expands", "translation", "coverage", ".", "By", "thorough", "inspections", "of", "the", "hidden", "layer", "outputs", ",", "we", "show", "that", "our", "approach", "indeed", "leads", "to", "more", "language-independent", "representations", "."], "entities": [{"type": "Operation", "start": 85, "end": 92, "text": "removing residual connections in an encoder layer", "sent_idx": 4}, {"type": "Effect", "start": 102, "end": 104, "text": "BLEU points", "sent_idx": 5}, {"type": "Effect", "start": 145, "end": 147, "text": "translation coverage", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--101"}
{"text": "Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS. Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.", "tokens": ["Contextual", "features", "always", "play", "an", "important", "role", "in", "Chinese", "word", "segmentation", "(", "CWS", ")", ".", "Wordhood", "information", ",", "being", "one", "of", "the", "contextual", "features", ",", "is", "proved", "to", "be", "useful", "in", "many", "conventional", "character-based", "segmenters", ".", "However", ",", "this", "feature", "receives", "less", "attention", "in", "recent", "neural", "models", "and", "it", "is", "also", "challenging", "to", "design", "a", "framework", "that", "can", "properly", "integrate", "wordhood", "information", "from", "different", "wordhood", "measures", "to", "existing", "neural", "frameworks", ".", "In", "this", "paper", ",", "we", "therefore", "propose", "a", "neural", "framework", ",", "WMSeg", ",", "which", "uses", "memory", "networks", "to", "incorporate", "wordhood", "information", "with", "several", "popular", "encoder-decoder", "combinations", "for", "CWS", ".", "Experimental", "results", "on", "five", "benchmark", "datasets", "indicate", "the", "memory", "mechanism", "successfully", "models", "wordhood", "information", "for", "neural", "segmenters", "and", "helps", "WMSeg", "achieve", "state-of-the-art", "performance", "on", "all", "those", "datasets", ".", "Further", "experiments", "and", "analyses", "also", "demonstrate", "the", "robustness", "of", "our", "proposed", "framework", "with", "respect", "to", "different", "wordhood", "measures", "and", "the", "efficiency", "of", "wordhood", "information", "in", "cross-domain", "experiments", "."], "entities": [{"type": "Operation", "start": 108, "end": 110, "text": "memory mechanism", "sent_idx": 4}, {"type": "Effect", "start": 122, "end": 123, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 82, "end": 83, "text": "WMSeg", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 136, "text": "robustness", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--734"}
{"text": "Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. \n \nWe argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.", "tokens": ["Current", "approaches", "for", "semantic", "parsing", "take", "a", "supervised", "approach", "requiring", "a", "considerable", "amount", "of", "training", "data", "which", "is", "expensive", "and", "difficult", "to", "obtain", ".", "This", "supervision", "bottleneck", "is", "one", "of", "the", "major", "difficulties", "in", "scaling", "up", "semantic", "parsing", ".", "\n \n", "We", "argue", "that", "a", "semantic", "parser", "can", "be", "trained", "effectively", "without", "annotated", "data", ",", "and", "introduce", "an", "unsupervised", "learning", "algorithm", ".", "The", "algorithm", "takes", "a", "self", "training", "approach", "driven", "by", "confidence", "estimation", ".", "Evaluated", "over", "Geoquery", ",", "a", "standard", "dataset", "for", "this", "task", ",", "our", "system", "achieved", "66", "%", "accuracy", ",", "compared", "to", "80", "%", "of", "its", "fully", "supervised", "counterpart", ",", "demonstrating", "the", "promise", "of", "unsupervised", "approaches", "for", "this", "task", "."], "entities": [{"type": "Operation", "start": 84, "end": 86, "text": "our system", "sent_idx": 4}, {"type": "Effect", "start": 89, "end": 90, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1149"}
{"text": "Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.", "tokens": ["Emotion-cause", "pair", "extraction", "aims", "to", "extract", "all", "potential", "pairs", "of", "emotions", "and", "corresponding", "causes", "from", "unannotated", "emotion", "text", ".", "Most", "existing", "methods", "are", "pipelined", "framework", ",", "which", "identifies", "emotions", "and", "extracts", "causes", "separately", ",", "leading", "to", "a", "drawback", "of", "error", "propagation", ".", "Towards", "this", "issue", ",", "we", "propose", "a", "transition-based", "model", "to", "transform", "the", "task", "into", "a", "procedure", "of", "parsing-like", "directed", "graph", "construction", ".", "The", "proposed", "model", "incrementally", "generates", "the", "directed", "graph", "with", "labeled", "edges", "based", "on", "a", "sequence", "of", "actions", ",", "from", "which", "we", "can", "recognize", "emotions", "with", "the", "corresponding", "causes", "simultaneously", ",", "thereby", "optimizing", "separate", "subtasks", "jointly", "and", "maximizing", "mutual", "benefits", "of", "tasks", "interdependently", ".", "Experimental", "results", "show", "that", "our", "approach", "achieves", "the", "best", "performance", ",", "outperforming", "the", "state-of-the-art", "methods", "by", "6.71", "%", "(", "p<0.01", ")", "in", "F1", "measure", "."], "entities": [{"type": "Operation", "start": 49, "end": 51, "text": "transition-based model", "sent_idx": 2}, {"type": "Effect", "start": 129, "end": 131, "text": "F1 measure", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--342"}
{"text": "Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.", "tokens": ["Unlike", "widely", "used", "Named", "Entity", "Recognition", "(", "NER", ")", "data", "sets", "in", "generic", "domains", ",", "biomedical", "NER", "data", "sets", "often", "contain", "mentions", "consisting", "of", "discontinuous", "spans", ".", "Conventional", "sequence", "tagging", "techniques", "encode", "Markov", "assumptions", "that", "are", "efficient", "but", "preclude", "recovery", "of", "these", "mentions", ".", "We", "propose", "a", "simple", ",", "effective", "transition-based", "model", "with", "generic", "neural", "encoding", "for", "discontinuous", "NER", ".", "Through", "extensive", "experiments", "on", "three", "biomedical", "data", "sets", ",", "we", "show", "that", "our", "model", "can", "effectively", "recognize", "discontinuous", "mentions", "without", "sacrificing", "the", "accuracy", "on", "continuous", "mentions", "."], "entities": [{"type": "Operation", "start": 50, "end": 52, "text": "transition-based model", "sent_idx": 2}, {"type": "Effect", "start": 82, "end": 83, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--520"}
{"text": "We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. The model\u2019s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments. With up to 93% cost savings, our approach significantly outperforms existing annotation procedures. Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.", "tokens": ["We", "present", "an", "efficient", "annotation", "framework", "for", "argument", "quality", ",", "a", "feature", "difficult", "to", "be", "measured", "reliably", "as", "per", "previous", "work", ".", "A", "stochastic", "transitivity", "model", "is", "combined", "with", "an", "effective", "sampling", "strategy", "to", "infer", "high-quality", "labels", "with", "low", "effort", "from", "crowdsourced", "pairwise", "judgments", ".", "The", "model", "\u2019s", "capabilities", "are", "showcased", "by", "compiling", "Webis-ArgQuality-20", ",", "an", "argument", "quality", "corpus", "that", "comprises", "scores", "for", "rhetorical", ",", "logical", ",", "dialectical", ",", "and", "overall", "quality", "inferred", "from", "a", "total", "of", "41,859", "pairwise", "judgments", "among", "1,271", "arguments", ".", "With", "up", "to", "93", "%", "cost", "savings", ",", "our", "approach", "significantly", "outperforms", "existing", "annotation", "procedures", ".", "Furthermore", ",", "novel", "insight", "into", "argument", "quality", "is", "provided", "through", "statistical", "analysis", ",", "and", "a", "new", "aggregation", "method", "to", "infer", "overall", "quality", "from", "individual", "quality", "dimensions", "is", "proposed", "."], "entities": [{"type": "Operation", "start": 4, "end": 9, "text": "annotation framework for argument quality", "sent_idx": 0}, {"type": "Effect", "start": 89, "end": 90, "text": "cost", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--511"}
{"text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \u201cvirtual tokens\u201d. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "tokens": ["Fine-tuning", "is", "the", "de", "facto", "way", "of", "leveraging", "large", "pretrained", "language", "models", "for", "downstream", "tasks", ".", "However", ",", "fine-tuning", "modifies", "all", "the", "language", "model", "parameters", "and", "therefore", "necessitates", "storing", "a", "full", "copy", "for", "each", "task", ".", "In", "this", "paper", ",", "we", "propose", "prefix-tuning", ",", "a", "lightweight", "alternative", "to", "fine-tuning", "for", "natural", "language", "generation", "tasks", ",", "which", "keeps", "language", "model", "parameters", "frozen", "and", "instead", "optimizes", "a", "sequence", "of", "continuous", "task-specific", "vectors", ",", "which", "we", "call", "the", "prefix", ".", "Prefix-tuning", "draws", "inspiration", "from", "prompting", "for", "language", "models", ",", "allowing", "subsequent", "tokens", "to", "attend", "to", "this", "prefix", "as", "if", "it", "were", "\u201c", "virtual", "tokens", "\u201d", ".", "We", "apply", "prefix-tuning", "to", "GPT-2", "for", "table-to-text", "generation", "and", "to", "BART", "for", "summarization", ".", "We", "show", "that", "by", "learning", "only", "0.1", "%", "of", "the", "parameters", ",", "prefix-tuning", "obtains", "comparable", "performance", "in", "the", "full", "data", "setting", ",", "outperforms", "fine-tuning", "in", "low-data", "settings", ",", "and", "extrapolates", "better", "to", "examples", "with", "topics", "that", "are", "unseen", "during", "training", "."], "entities": [{"type": "Operation", "start": 129, "end": 130, "text": "prefix-tuning", "sent_idx": 5}, {"type": "Effect", "start": 132, "end": 133, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--353"}
{"text": "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic \u201cweak\u201d data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank.", "tokens": ["The", "effectiveness", "of", "Neural", "Information", "Retrieval", "(", "Neu-IR", ")", "often", "depends", "on", "a", "large", "scale", "of", "in-domain", "relevance", "training", "signals", ",", "which", "are", "not", "always", "available", "in", "real-world", "ranking", "scenarios", ".", "To", "democratize", "the", "benefits", "of", "Neu-IR", ",", "this", "paper", "presents", "MetaAdaptRank", ",", "a", "domain", "adaptive", "learning", "method", "that", "generalizes", "Neu-IR", "models", "from", "label-rich", "source", "domains", "to", "few-shot", "target", "domains", ".", "Drawing", "on", "source-domain", "massive", "relevance", "supervision", ",", "MetaAdaptRank", "contrastively", "synthesizes", "a", "large", "number", "of", "weak", "supervision", "signals", "for", "target", "domains", "and", "meta-learns", "to", "reweight", "these", "synthetic", "\u201c", "weak", "\u201d", "data", "based", "on", "their", "benefits", "to", "the", "target-domain", "ranking", "accuracy", "of", "Neu-IR", "models", ".", "Experiments", "on", "three", "TREC", "benchmarks", "in", "the", "web", ",", "news", ",", "and", "biomedical", "domains", "show", "that", "MetaAdaptRank", "significantly", "improves", "the", "few-shot", "ranking", "accuracy", "of", "Neu-IR", "models", ".", "Further", "analyses", "indicate", "that", "MetaAdaptRank", "thrives", "from", "both", "its", "contrastive", "weak", "data", "synthesis", "and", "meta-reweighted", "data", "selection", ".", "The", "code", "and", "data", "of", "this", "paper", "can", "be", "obtained", "from", "https://github.com/thunlp/MetaAdaptRank", "."], "entities": [{"type": "Operation", "start": 120, "end": 121, "text": "MetaAdaptRank", "sent_idx": 3}, {"type": "Effect", "start": 125, "end": 127, "text": "ranking accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--390"}
{"text": "In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.", "tokens": ["In", "structured", "prediction", "problems", ",", "cross-lingual", "transfer", "learning", "is", "an", "efficient", "way", "to", "train", "quality", "models", "for", "low-resource", "languages", ",", "and", "further", "improvement", "can", "be", "obtained", "by", "learning", "from", "multiple", "source", "languages", ".", "However", ",", "not", "all", "source", "models", "are", "created", "equal", "and", "some", "may", "hurt", "performance", "on", "the", "target", "language", ".", "Previous", "work", "has", "explored", "the", "similarity", "between", "source", "and", "target", "sentences", "as", "an", "approximate", "measure", "of", "strength", "for", "different", "source", "models", ".", "In", "this", "paper", ",", "we", "propose", "a", "multi-view", "framework", ",", "by", "leveraging", "a", "small", "number", "of", "labeled", "target", "sentences", ",", "to", "effectively", "combine", "multiple", "source", "models", "into", "an", "aggregated", "source", "view", "at", "different", "granularity", "levels", "(", "language", ",", "sentence", ",", "or", "sub-structure", ")", ",", "and", "transfer", "it", "to", "a", "target", "view", "based", "on", "a", "task-specific", "model", ".", "By", "encouraging", "the", "two", "views", "to", "interact", "with", "each", "other", ",", "our", "framework", "can", "dynamically", "adjust", "the", "confidence", "level", "of", "each", "source", "model", "and", "improve", "the", "performance", "of", "both", "views", "during", "training", ".", "Experiments", "for", "three", "structured", "prediction", "tasks", "on", "sixteen", "data", "sets", "show", "that", "our", "framework", "achieves", "significant", "improvement", "over", "all", "existing", "approaches", ",", "including", "these", "with", "access", "to", "additional", "source", "language", "data", "."], "entities": [{"type": "Operation", "start": 81, "end": 83, "text": "multi-view framework", "sent_idx": 3}, {"type": "Effect", "start": 157, "end": 158, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--207"}
{"text": "This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision.", "tokens": ["This", "paper", "focuses", "on", "mining", "the", "hyponymy", "(", "or", "is-a", ")", "relation", "from", "large-scale", ",", "open-domain", "web", "documents", ".", "A", "nonlinear", "probabilistic", "model", "is", "exploited", "to", "model", "the", "correlation", "between", "sentences", "in", "the", "aggregation", "of", "pattern", "matching", "results", ".", "Based", "on", "the", "model", ",", "we", "design", "a", "set", "of", "evidence", "combination", "and", "propagation", "algorithms", ".", "These", "significantly", "improve", "the", "result", "quality", "of", "existing", "approaches", ".", "Experimental", "results", "conducted", "on", "500", "million", "web", "pages", "and", "hypernym", "labels", "for", "300", "terms", "show", "over", "20", "%", "performance", "improvement", "in", "terms", "of", "P@5", ",", "MAP", "and", "R-Precision", "."], "entities": [{"type": "Operation", "start": 20, "end": 23, "text": "nonlinear probabilistic model", "sent_idx": 1}, {"type": "Effect", "start": 83, "end": 84, "text": "performance", "sent_idx": 4}, {"type": "Effect", "start": 88, "end": 89, "text": "P@5", "sent_idx": 4}, {"type": "Effect", "start": 90, "end": 91, "text": "MAP", "sent_idx": 4}, {"type": "Effect", "start": 92, "end": 93, "text": "R-Precision", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 0, "tail": 4}], "id": "P11-1116"}
{"text": "In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.", "tokens": ["In", "several", "question", "answering", "benchmarks", ",", "pretrained", "models", "have", "reached", "human", "parity", "through", "fine-tuning", "on", "an", "order", "of", "100,000", "annotated", "questions", "and", "answers", ".", "We", "explore", "the", "more", "realistic", "few-shot", "setting", ",", "where", "only", "a", "few", "hundred", "training", "examples", "are", "available", ",", "and", "observe", "that", "standard", "models", "perform", "poorly", ",", "highlighting", "the", "discrepancy", "between", "current", "pretraining", "objectives", "and", "question", "answering", ".", "We", "propose", "a", "new", "pretraining", "scheme", "tailored", "for", "question", "answering", ":", "recurring", "span", "selection", ".", "Given", "a", "passage", "with", "multiple", "sets", "of", "recurring", "spans", ",", "we", "mask", "in", "each", "set", "all", "recurring", "spans", "but", "one", ",", "and", "ask", "the", "model", "to", "select", "the", "correct", "span", "in", "the", "passage", "for", "each", "masked", "span", ".", "Masked", "spans", "are", "replaced", "with", "a", "special", "token", ",", "viewed", "as", "a", "question", "representation", ",", "that", "is", "later", "used", "during", "fine-tuning", "to", "select", "the", "answer", "span", ".", "The", "resulting", "model", "obtains", "surprisingly", "good", "results", "on", "multiple", "benchmarks", "(", "e.g.", ",", "72.7", "F1", "on", "SQuAD", "with", "only", "128", "training", "examples", ")", ",", "while", "maintaining", "competitive", "performance", "in", "the", "high-resource", "setting", "."], "entities": [{"type": "Operation", "start": 72, "end": 75, "text": "recurring span selection", "sent_idx": 2}, {"type": "Effect", "start": 155, "end": 156, "text": "F1", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--239"}
{"text": "Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.", "tokens": ["Recently", ",", "the", "character-word", "lattice", "structure", "has", "been", "proved", "to", "be", "effective", "for", "Chinese", "named", "entity", "recognition", "(", "NER", ")", "by", "incorporating", "the", "word", "information", ".", "However", ",", "since", "the", "lattice", "structure", "is", "complex", "and", "dynamic", ",", "the", "lattice-based", "models", "are", "hard", "to", "fully", "utilize", "the", "parallel", "computation", "of", "GPUs", "and", "usually", "have", "a", "low", "inference", "speed", ".", "In", "this", "paper", ",", "we", "propose", "FLAT", ":", "Flat-LAttice", "Transformer", "for", "Chinese", "NER", ",", "which", "converts", "the", "lattice", "structure", "into", "a", "flat", "structure", "consisting", "of", "spans", ".", "Each", "span", "corresponds", "to", "a", "character", "or", "latent", "word", "and", "its", "position", "in", "the", "original", "lattice", ".", "With", "the", "power", "of", "Transformer", "and", "well-designed", "position", "encoding", ",", "FLAT", "can", "fully", "leverage", "the", "lattice", "information", "and", "has", "an", "excellent", "parallel", "ability", ".", "Experiments", "on", "four", "datasets", "show", "FLAT", "outperforms", "other", "lexicon-based", "models", "in", "performance", "and", "efficiency", "."], "entities": [{"type": "Operation", "start": 131, "end": 132, "text": "FLAT", "sent_idx": 5}, {"type": "Effect", "start": 137, "end": 138, "text": "performance", "sent_idx": 5}, {"type": "Effect", "start": 139, "end": 140, "text": "efficiency", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--611"}
{"text": "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.", "tokens": ["Integrating", "extracted", "knowledge", "from", "the", "Web", "to", "knowledge", "graphs", "(", "KGs", ")", "can", "facilitate", "tasks", "like", "question", "answering", ".", "We", "study", "relation", "integration", "that", "aims", "to", "align", "free-text", "relations", "in", "subject-relation-object", "extractions", "to", "relations", "in", "a", "target", "KG", ".", "To", "address", "the", "challenge", "that", "free-text", "relations", "are", "ambiguous", ",", "previous", "methods", "exploit", "neighbor", "entities", "and", "relations", "for", "additional", "context", ".", "However", ",", "the", "predictions", "are", "made", "independently", ",", "which", "can", "be", "mutually", "inconsistent", ".", "We", "propose", "a", "two-stage", "Collective", "Relation", "Integration", "(", "CoRI", ")", "model", ",", "where", "the", "first", "stage", "independently", "makes", "candidate", "predictions", ",", "and", "the", "second", "stage", "employs", "a", "collective", "model", "that", "accesses", "all", "candidate", "predictions", "to", "make", "globally", "coherent", "predictions", ".", "We", "further", "improve", "the", "collective", "model", "with", "augmented", "data", "from", "the", "portion", "of", "the", "target", "KG", "that", "is", "otherwise", "unused", ".", "Experiment", "results", "on", "two", "datasets", "show", "that", "CoRI", "can", "significantly", "outperform", "the", "baselines", ",", "improving", "AUC", "from", ".677", "to", ".748", "and", "from", ".716", "to", ".780", ",", "respectively", "."], "entities": [{"type": "Operation", "start": 142, "end": 143, "text": "CoRI", "sent_idx": 6}, {"type": "Effect", "start": 150, "end": 151, "text": "AUC", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--363"}
{"text": "In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users' information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user's query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.", "tokens": ["In", "this", "paper", "we", "address", "the", "problem", "of", "question", "recommendation", "from", "large", "archives", "of", "community", "question", "answering", "data", "by", "exploiting", "the", "users", "'", "information", "needs", ".", "Our", "experimental", "results", "indicate", "that", "questions", "based", "on", "the", "same", "or", "similar", "information", "need", "can", "provide", "excellent", "question", "recommendation", ".", "We", "show", "that", "translation", "model", "can", "be", "effectively", "utilized", "to", "predict", "the", "information", "need", "given", "only", "the", "user", "'s", "query", "question", ".", "Experiments", "show", "that", "the", "proposed", "information", "need", "prediction", "approach", "can", "improve", "the", "performance", "of", "question", "recommendation", "."], "entities": [{"type": "Operation", "start": 73, "end": 77, "text": "information need prediction approach", "sent_idx": 3}, {"type": "Effect", "start": 80, "end": 81, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1143"}
{"text": "A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.", "tokens": ["A", "central", "concern", "in", "Computational", "Social", "Sciences", "(", "CSS", ")", "is", "fairness", ":", "where", "the", "role", "of", "NLP", "is", "to", "scale", "up", "text", "analysis", "to", "large", "corpora", ",", "the", "quality", "of", "automatic", "analyses", "should", "be", "as", "independent", "as", "possible", "of", "textual", "properties", ".", "We", "analyze", "the", "performance", "of", "a", "state-of-the-art", "neural", "model", "on", "the", "task", "of", "political", "claims", "detection", "(", "i.e.", ",", "the", "identification", "of", "forward-looking", "statements", "made", "by", "political", "actors", ")", "and", "identify", "a", "strong", "frequency", "bias", ":", "claims", "made", "by", "frequent", "actors", "are", "recognized", "better", ".", "We", "propose", "two", "simple", "debiasing", "methods", "which", "mask", "proper", "names", "and", "pronouns", "during", "training", "of", "the", "model", ",", "thus", "removing", "personal", "information", "bias", ".", "We", "find", "that", "(", "a", ")", "these", "methods", "significantly", "decrease", "frequency", "bias", "while", "keeping", "the", "overall", "performance", "stable", ";", "and", "(", "b", ")", "the", "resulting", "models", "improve", "when", "evaluated", "in", "an", "out-of-domain", "setting", "."], "entities": [{"type": "Operation", "start": 95, "end": 105, "text": "mask proper names and pronouns during training of the model", "sent_idx": 2}, {"type": "Effect", "start": 122, "end": 124, "text": "frequency bias", "sent_idx": 3}, {"type": "Effect", "start": 128, "end": 129, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--404"}
{"text": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8\u00d7-15\u00d7 speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.", "tokens": ["Recent", "work", "on", "non-autoregressive", "neural", "machine", "translation", "(", "NAT", ")", "aims", "at", "improving", "the", "efficiency", "by", "parallel", "decoding", "without", "sacrificing", "the", "quality", ".", "However", ",", "existing", "NAT", "methods", "are", "either", "inferior", "to", "Transformer", "or", "require", "multiple", "decoding", "passes", ",", "leading", "to", "reduced", "speedup", ".", "We", "propose", "the", "Glancing", "Language", "Model", "(", "GLM", ")", "for", "single-pass", "parallel", "generation", "models", ".", "With", "GLM", ",", "we", "develop", "Glancing", "Transformer", "(", "GLAT", ")", "for", "machine", "translation", ".", "With", "only", "single-pass", "parallel", "decoding", ",", "GLAT", "is", "able", "to", "generate", "high-quality", "translation", "with", "8", "\u00d7", "-15", "\u00d7", "speedup", ".", "Note", "that", "GLAT", "does", "not", "modify", "the", "network", "architecture", ",", "which", "is", "a", "training", "method", "to", "learn", "word", "interdependency", ".", "Experiments", "on", "multiple", "WMT", "language", "directions", "show", "that", "GLAT", "outperforms", "all", "previous", "single", "pass", "non-autoregressive", "methods", ",", "and", "is", "nearly", "comparable", "to", "Transformer", ",", "reducing", "the", "gap", "to", "0.25", "-", "0.9", "BLEU", "points", "."], "entities": [{"type": "Operation", "start": 79, "end": 80, "text": "GLAT", "sent_idx": 4}, {"type": "Effect", "start": 91, "end": 92, "text": "speedup", "sent_idx": 4}, {"type": "Operation", "start": 121, "end": 122, "text": "GLAT", "sent_idx": 6}, {"type": "Effect", "start": 144, "end": 146, "text": "BLEU points", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--155"}
{"text": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG", "tokens": ["Neural-based", "end-to-end", "approaches", "to", "natural", "language", "generation", "(", "NLG", ")", "from", "structured", "data", "or", "knowledge", "are", "data-hungry", ",", "making", "their", "adoption", "for", "real-world", "applications", "difficult", "with", "limited", "data", ".", "In", "this", "work", ",", "we", "propose", "the", "new", "task", "of", "few-shot", "natural", "language", "generation", ".", "Motivated", "by", "how", "humans", "tend", "to", "summarize", "tabular", "data", ",", "we", "propose", "a", "simple", "yet", "effective", "approach", "and", "show", "that", "it", "not", "only", "demonstrates", "strong", "performance", "but", "also", "provides", "good", "generalization", "across", "domains", ".", "The", "design", "of", "the", "model", "architecture", "is", "based", "on", "two", "aspects", ":", "content", "selection", "from", "input", "data", "and", "language", "modeling", "to", "compose", "coherent", "sentences", ",", "which", "can", "be", "acquired", "from", "prior", "knowledge", ".", "With", "just", "200", "training", "examples", ",", "across", "multiple", "domains", ",", "we", "show", "that", "our", "approach", "achieves", "very", "reasonable", "performances", "and", "outperforms", "the", "strongest", "baseline", "by", "an", "average", "of", "over", "8.0", "BLEU", "points", "improvement", ".", "Our", "code", "and", "data", "can", "be", "found", "at", "https://github.com/czyssrs/Few-Shot-NLG"], "entities": [{"type": "Operation", "start": 90, "end": 95, "text": "content selection from input data", "sent_idx": 3}, {"type": "Effect", "start": 129, "end": 130, "text": "performances", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 143, "text": "BLEU points", "sent_idx": 4}, {"type": "Operation", "start": 96, "end": 98, "text": "language modeling", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 2}], "id": "abstract-2020--acl-main--18"}
{"text": "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings.", "tokens": ["Users", "of", "medical", "question", "answering", "systems", "often", "submit", "long", "and", "detailed", "questions", ",", "making", "it", "hard", "to", "achieve", "high", "recall", "in", "answer", "retrieval", ".", "To", "alleviate", "this", "problem", ",", "we", "propose", "a", "novel", "Multi-Task", "Learning", "(", "MTL", ")", "method", "with", "data", "augmentation", "for", "medical", "question", "understanding", ".", "We", "first", "establish", "an", "equivalence", "between", "the", "tasks", "of", "question", "summarization", "and", "Recognizing", "Question", "Entailment", "(", "RQE", ")", "using", "their", "definitions", "in", "the", "medical", "domain", ".", "Based", "on", "this", "equivalence", ",", "we", "propose", "a", "data", "augmentation", "algorithm", "to", "use", "just", "one", "dataset", "to", "optimize", "for", "both", "tasks", ",", "with", "a", "weighted", "MTL", "loss", ".", "We", "introduce", "gradually", "soft", "parameter-sharing", ":", "a", "constraint", "for", "decoder", "parameters", "to", "be", "close", ",", "that", "is", "gradually", "loosened", "as", "we", "move", "to", "the", "highest", "layer", ".", "We", "show", "through", "ablation", "studies", "that", "our", "proposed", "novelties", "improve", "performance", ".", "Our", "method", "outperforms", "existing", "MTL", "methods", "across", "4", "datasets", "of", "medical", "question", "pairs", ",", "in", "ROUGE", "scores", ",", "RQE", "accuracy", "and", "human", "evaluation", ".", "Finally", ",", "we", "show", "that", "our", "method", "fares", "better", "than", "single-task", "learning", "under", "4", "low-resource", "settings", "."], "entities": [{"type": "Operation", "start": 81, "end": 84, "text": "data augmentation algorithm", "sent_idx": 3}, {"type": "Effect", "start": 155, "end": 157, "text": "ROUGE scores", "sent_idx": 6}, {"type": "Effect", "start": 158, "end": 160, "text": "RQE accuracy", "sent_idx": 6}, {"type": "Effect", "start": 138, "end": 139, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--119"}
{"text": "We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.", "tokens": ["We", "investigate", "active", "learning", "methods", "for", "Japanese", "dependency", "parsing", ".", "We", "propose", "active", "learning", "methods", "of", "using", "partial", "dependency", "relations", "in", "a", "given", "sentence", "for", "parsing", "and", "evaluate", "their", "effectiveness", "empirically", ".", "Furthermore", ",", "we", "utilize", "syntactic", "constraints", "of", "Japanese", "to", "obtain", "more", "labeled", "examples", "from", "precious", "labeled", "ones", "that", "annotators", "give", ".", "Experimental", "results", "show", "that", "our", "proposed", "methods", "improve", "considerably", "the", "learning", "curve", "of", "Japanese", "dependency", "parsing", ".", "In", "order", "to", "achieve", "an", "accuracy", "of", "over", "88.3", "%", ",", "one", "of", "our", "methods", "requires", "only", "34.4", "%", "of", "labeled", "examples", "as", "compared", "to", "passive", "learning", "."], "entities": [{"type": "Operation", "start": 12, "end": 15, "text": "active learning methods", "sent_idx": 1}, {"type": "Effect", "start": 75, "end": 76, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1037"}
{"text": "In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems.", "tokens": ["In", "this", "paper", ",", "we", "present", "a", "simple", "and", "effective", "method", "to", "address", "the", "issue", "of", "how", "to", "generate", "diversified", "translation", "systems", "from", "a", "single", "Statistical", "Machine", "Translation", "(", "SMT", ")", "engine", "for", "system", "combination", ".", "Our", "method", "is", "based", "on", "the", "framework", "of", "boosting", ".", "First", ",", "a", "sequence", "of", "weak", "translation", "systems", "is", "generated", "from", "a", "baseline", "system", "in", "an", "iterative", "manner", ".", "Then", ",", "a", "strong", "translation", "system", "is", "built", "from", "the", "ensemble", "of", "these", "weak", "translation", "systems", ".", "To", "adapt", "boosting", "to", "SMT", "system", "combination", ",", "several", "key", "components", "of", "the", "original", "boosting", "algorithms", "are", "redesigned", "in", "this", "work", ".", "We", "evaluate", "our", "method", "on", "Chinese-to-English", "Machine", "Translation", "(", "MT", ")", "tasks", "in", "three", "baseline", "systems", ",", "including", "a", "phrase-based", "system", ",", "a", "hierarchical", "phrase-based", "system", "and", "a", "syntax-based", "system", ".", "The", "experimental", "results", "on", "three", "NIST", "evaluation", "test", "sets", "show", "that", "our", "method", "leads", "to", "significant", "improvements", "in", "translation", "accuracy", "over", "the", "baseline", "systems", "."], "entities": [{"type": "Operation", "start": 96, "end": 98, "text": "boosting algorithms", "sent_idx": 4}, {"type": "Effect", "start": 153, "end": 155, "text": "translation accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1076"}
{"text": "In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40% of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots.", "tokens": ["In", "this", "paper", "we", "explore", "the", "improvement", "of", "intent", "recognition", "in", "conversational", "systems", "by", "the", "use", "of", "meta-knowledge", "embedded", "in", "intent", "identifiers", ".", "Developers", "often", "include", "such", "knowledge", ",", "structure", "as", "taxonomies", ",", "in", "the", "documentation", "of", "chatbots", ".", "By", "using", "neuro-symbolic", "algorithms", "to", "incorporate", "those", "taxonomies", "into", "embeddings", "of", "the", "output", "space", ",", "we", "were", "able", "to", "improve", "accuracy", "in", "intent", "recognition", ".", "In", "datasets", "with", "intents", "and", "example", "utterances", "from", "200", "professional", "chatbots", ",", "we", "saw", "decreases", "in", "the", "equal", "error", "rate", "(", "EER", ")", "in", "more", "than", "40", "%", "of", "the", "chatbots", "in", "comparison", "to", "the", "baseline", "of", "the", "same", "algorithm", "without", "the", "meta-knowledge", ".", "The", "meta-knowledge", "proved", "also", "to", "be", "effective", "in", "detecting", "out-of-scope", "utterances", ",", "improving", "the", "false", "acceptance", "rate", "(", "FAR", ")", "in", "two", "thirds", "of", "the", "chatbots", ",", "with", "decreases", "of", "0.05", "or", "more", "in", "FAR", "in", "almost", "40", "%", "of", "the", "chatbots", ".", "When", "considering", "only", "the", "well-developed", "workspaces", "with", "a", "high", "level", "use", "of", "taxonomies", ",", "FAR", "decreased", "more", "than", "0.05", "in", "77", "%", "of", "them", ",", "and", "more", "than", "0.1", "in", "39", "%", "of", "the", "chatbots", "."], "entities": [{"type": "Operation", "start": 109, "end": 110, "text": "meta-knowledge", "sent_idx": 4}, {"type": "Effect", "start": 122, "end": 128, "text": "false acceptance rate (FAR)", "sent_idx": 4}, {"type": "Effect", "start": 81, "end": 87, "text": "equal error rate (EER)", "sent_idx": 3}, {"type": "Effect", "start": 142, "end": 143, "text": "FAR", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Neg_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--545"}
{"text": "Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task.", "tokens": ["Bilingual", "lexicons", "map", "words", "in", "one", "language", "to", "their", "translations", "in", "another", ",", "and", "are", "typically", "induced", "by", "learning", "linear", "projections", "to", "align", "monolingual", "word", "embedding", "spaces", ".", "In", "this", "paper", ",", "we", "show", "it", "is", "possible", "to", "produce", "much", "higher", "quality", "lexicons", "with", "methods", "that", "combine", "(", "1", ")", "unsupervised", "bitext", "mining", "and", "(", "2", ")", "unsupervised", "word", "alignment", ".", "Directly", "applying", "a", "pipeline", "that", "uses", "recent", "algorithms", "for", "both", "subproblems", "significantly", "improves", "induced", "lexicon", "quality", "and", "further", "gains", "are", "possible", "by", "learning", "to", "filter", "the", "resulting", "lexical", "entries", ",", "with", "both", "unsupervised", "and", "semi-supervised", "schemes", ".", "Our", "final", "model", "outperforms", "the", "state", "of", "the", "art", "on", "the", "BUCC", "2020", "shared", "task", "by", "14", "F1", "points", "averaged", "over", "12", "language", "pairs", ",", "while", "also", "providing", "a", "more", "interpretable", "approach", "that", "allows", "for", "rich", "reasoning", "of", "word", "meaning", "in", "context", ".", "Further", "analysis", "of", "our", "output", "and", "the", "standard", "reference", "lexicons", "suggests", "they", "are", "of", "comparable", "quality", ",", "and", "new", "benchmarks", "may", "be", "needed", "to", "measure", "further", "progress", "on", "this", "task", "."], "entities": [{"type": "Operation", "start": 62, "end": 69, "text": "applying a pipeline that uses recent algorithms", "sent_idx": 2}, {"type": "Effect", "start": 74, "end": 77, "text": "induced lexicon quality", "sent_idx": 2}, {"type": "Operation", "start": 98, "end": 101, "text": "Our final model", "sent_idx": 3}, {"type": "Effect", "start": 115, "end": 117, "text": "F1 points", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--67"}
{"text": "Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences.", "tokens": ["Despite", "recent", "successes", "of", "large", "pre-trained", "language", "models", "in", "solving", "reasoning", "tasks", ",", "their", "inference", "capabilities", "remain", "opaque", ".", "We", "posit", "that", "such", "models", "can", "be", "made", "more", "interpretable", "by", "explicitly", "generating", "interim", "inference", "rules", ",", "and", "using", "them", "to", "guide", "the", "generation", "of", "task-specific", "textual", "outputs", ".", "In", "this", "paper", "we", "present", "Coins", ",", "a", "recursive", "inference", "framework", "that", "i", ")", "iteratively", "reads", "context", "sentences", ",", "ii", ")", "dynamically", "generates", "contextualized", "inference", "rules", ",", "encodes", "them", ",", "and", "iii", ")", "uses", "them", "to", "guide", "task-specific", "output", "generation", ".", "We", "apply", "to", "a", "Narrative", "Story", "Completion", "task", "that", "asks", "a", "model", "to", "complete", "a", "story", "with", "missing", "sentences", ",", "to", "produce", "a", "coherent", "story", "with", "plausible", "logical", "connections", ",", "causal", "relationships", ",", "and", "temporal", "dependencies", ".", "By", "modularizing", "inference", "and", "sentence", "generation", "steps", "in", "a", "recurrent", "model", ",", "we", "aim", "to", "make", "reasoning", "steps", "and", "their", "effects", "on", "next", "sentence", "generation", "transparent", ".", "Our", "automatic", "and", "manual", "evaluations", "show", "that", "the", "model", "generates", "better", "story", "sentences", "than", "SOTA", "baselines", ",", "especially", "in", "terms", "of", "coherence", ".", "We", "further", "demonstrate", "improved", "performance", "over", "strong", "pre-trained", "LMs", "in", "generating", "commonsense", "inference", "rules", ".", "The", "recursive", "nature", "of", "holds", "the", "potential", "for", "controlled", "generation", "of", "longer", "sequences", "."], "entities": [{"type": "Operation", "start": 53, "end": 54, "text": "Coins", "sent_idx": 2}, {"type": "Effect", "start": 180, "end": 181, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--395"}
{"text": "Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP. Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters. Here, we propose a model that can disambiguate between mappings and convert between the two scripts. The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. We further construct benchmark datasets for topic classification and script conversion. Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy. These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification. An error analysis reveals that our method\u2019s particular strengths are in dealing with code mixing and named entities.", "tokens": ["Simplified", "Chinese", "to", "Traditional", "Chinese", "character", "conversion", "is", "a", "common", "preprocessing", "step", "in", "Chinese", "NLP", ".", "Despite", "this", ",", "current", "approaches", "have", "insufficient", "performance", "because", "they", "do", "not", "take", "into", "account", "that", "a", "simplified", "Chinese", "character", "can", "correspond", "to", "multiple", "traditional", "characters", ".", "Here", ",", "we", "propose", "a", "model", "that", "can", "disambiguate", "between", "mappings", "and", "convert", "between", "the", "two", "scripts", ".", "The", "model", "is", "based", "on", "subword", "segmentation", ",", "two", "language", "models", ",", "as", "well", "as", "a", "method", "for", "mapping", "between", "subword", "sequences", ".", "We", "further", "construct", "benchmark", "datasets", "for", "topic", "classification", "and", "script", "conversion", ".", "Our", "proposed", "method", "outperforms", "previous", "Chinese", "Character", "conversion", "approaches", "by", "6", "points", "in", "accuracy", ".", "These", "results", "are", "further", "confirmed", "in", "a", "downstream", "application", ",", "where", "2kenize", "is", "used", "to", "convert", "pretraining", "dataset", "for", "topic", "classification", ".", "An", "error", "analysis", "reveals", "that", "our", "method", "\u2019s", "particular", "strengths", "are", "in", "dealing", "with", "code", "mixing", "and", "named", "entities", "."], "entities": [{"type": "Operation", "start": 96, "end": 99, "text": "Our proposed method", "sent_idx": 5}, {"type": "Effect", "start": 109, "end": 110, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--648"}
{"text": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.", "tokens": ["In", "recent", "years", ",", "research", "in", "natural", "language", "processing", "has", "increasingly", "focused", "on", "normalizing", "SMS", "messages", ".", "Different", "well-defined", "approaches", "have", "been", "proposed", ",", "but", "the", "problem", "remains", "far", "from", "being", "solved", ":", "best", "systems", "achieve", "a", "11", "%", "Word", "Error", "Rate", ".", "This", "paper", "presents", "a", "method", "that", "shares", "similarities", "with", "both", "spell", "checking", "and", "machine", "translation", "approaches", ".", "The", "normalization", "part", "of", "the", "system", "is", "entirely", "based", "on", "models", "trained", "from", "a", "corpus", ".", "Evaluated", "in", "French", "by", "10-fold-cross", "validation", ",", "the", "system", "achieves", "a", "9.3", "%", "Word", "Error", "Rate", "and", "a", "0.83", "BLEU", "score", "."], "entities": [{"type": "Operation", "start": 49, "end": 59, "text": "shares similarities with both spell checking and machine translation approaches", "sent_idx": 2}, {"type": "Effect", "start": 89, "end": 92, "text": "Word Error Rate", "sent_idx": 4}, {"type": "Effect", "start": 95, "end": 97, "text": "BLEU score", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "P10-1079"}
{"text": "We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.", "tokens": ["We", "describe", "an", "efficient", "hierarchical", "method", "to", "compute", "attention", "in", "the", "Transformer", "architecture", ".", "The", "proposed", "attention", "mechanism", "exploits", "a", "matrix", "structure", "similar", "to", "the", "Hierarchical", "Matrix", "(", "H-Matrix", ")", "developed", "by", "the", "numerical", "analysis", "community", ",", "and", "has", "linear", "run", "time", "and", "memory", "complexity", ".", "We", "perform", "extensive", "experiments", "to", "show", "that", "the", "inductive", "bias", "embodied", "by", "our", "hierarchical", "attention", "is", "effective", "in", "capturing", "the", "hierarchical", "structure", "in", "the", "sequences", "typical", "for", "natural", "language", "and", "vision", "tasks", ".", "Our", "method", "is", "superior", "to", "alternative", "sub-quadratic", "proposals", "by", "over", "+", "6", "points", "on", "average", "on", "the", "Long", "Range", "Arena", "benchmark", ".", "It", "also", "sets", "a", "new", "SOTA", "test", "perplexity", "on", "One-Billion", "Word", "dataset", "with", "5x", "fewer", "model", "parameters", "than", "that", "of", "the", "previous-best", "Transformer-based", "models", "."], "entities": [{"type": "Operation", "start": 4, "end": 6, "text": "hierarchical method", "sent_idx": 0}, {"type": "Effect", "start": 108, "end": 109, "text": "perplexity", "sent_idx": 4}, {"type": "Effect", "start": 116, "end": 118, "text": "model parameters", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--294"}
{"text": "Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets.", "tokens": ["Syntactic", "information", ",", "especially", "dependency", "trees", ",", "has", "been", "widely", "used", "by", "existing", "studies", "to", "improve", "relation", "extraction", "with", "better", "semantic", "guidance", "for", "analyzing", "the", "context", "information", "associated", "with", "the", "given", "entities", ".", "However", ",", "most", "existing", "studies", "suffer", "from", "the", "noise", "in", "the", "dependency", "trees", ",", "especially", "when", "they", "are", "automatically", "generated", ",", "so", "that", "intensively", "leveraging", "dependency", "information", "may", "introduce", "confusions", "to", "relation", "classification", "and", "necessary", "pruning", "is", "of", "great", "importance", "in", "this", "task", ".", "In", "this", "paper", ",", "we", "propose", "a", "dependency-driven", "approach", "for", "relation", "extraction", "with", "attentive", "graph", "convolutional", "networks", "(", "A-GCN", ")", ".", "In", "this", "approach", ",", "an", "attention", "mechanism", "upon", "graph", "convolutional", "networks", "is", "applied", "to", "different", "contextual", "words", "in", "the", "dependency", "tree", "obtained", "from", "an", "off-the-shelf", "dependency", "parser", ",", "to", "distinguish", "the", "importance", "of", "different", "word", "dependencies", ".", "Consider", "that", "dependency", "types", "among", "words", "also", "contain", "important", "contextual", "guidance", ",", "which", "is", "potentially", "helpful", "for", "relation", "extraction", ",", "we", "also", "include", "the", "type", "information", "in", "A-GCN", "modeling", ".", "Experimental", "results", "on", "two", "English", "benchmark", "datasets", "demonstrate", "the", "effectiveness", "of", "our", "A-GCN", ",", "which", "outperforms", "previous", "studies", "and", "achieves", "state-of-the-art", "performance", "on", "both", "datasets", "."], "entities": [{"type": "Operation", "start": 177, "end": 178, "text": "A-GCN", "sent_idx": 5}, {"type": "Effect", "start": 186, "end": 187, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--344"}
{"text": "Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac.", "tokens": ["Transformer-based", "language", "models", "(", "TLMs", ")", ",", "such", "as", "BERT", ",", "ALBERT", "and", "GPT-3", ",", "have", "shown", "strong", "performance", "in", "a", "wide", "range", "of", "NLP", "tasks", "and", "currently", "dominate", "the", "field", "of", "NLP", ".", "However", ",", "many", "researchers", "wonder", "whether", "these", "models", "can", "maintain", "their", "dominance", "forever", ".", "Of", "course", ",", "we", "do", "not", "have", "answers", "now", ",", "but", ",", "as", "an", "attempt", "to", "find", "better", "neural", "architectures", "and", "training", "schemes", ",", "we", "pretrain", "a", "simple", "CNN", "using", "a", "GAN-style", "learning", "scheme", "and", "Wikipedia", "data", ",", "and", "then", "integrate", "it", "with", "standard", "TLMs", ".", "We", "show", "that", "on", "the", "GLUE", "tasks", ",", "the", "combination", "of", "our", "pretrained", "CNN", "with", "ALBERT", "outperforms", "the", "original", "ALBERT", "and", "achieves", "a", "similar", "performance", "to", "that", "of", "SOTA", ".", "Furthermore", ",", "on", "open-domain", "QA", "(", "Quasar-T", "and", "SearchQA", ")", ",", "the", "combination", "of", "the", "CNN", "with", "ALBERT", "or", "RoBERTa", "achieved", "stronger", "performance", "than", "SOTA", "and", "the", "original", "TLMs", ".", "We", "hope", "that", "this", "work", "provides", "a", "hint", "for", "developing", "a", "novel", "strong", "network", "architecture", "along", "with", "its", "training", "scheme", ".", "Our", "source", "code", "and", "models", "are", "available", "at", "https://github.com/nict-wisdom/bertac", "."], "entities": [{"type": "Operation", "start": 136, "end": 144, "text": "combination of the CNN with ALBERT or RoBERTa", "sent_idx": 4}, {"type": "Effect", "start": 146, "end": 147, "text": "performance", "sent_idx": 4}, {"type": "Operation", "start": 103, "end": 110, "text": "combination of our pretrained CNN with ALBERT", "sent_idx": 3}, {"type": "Effect", "start": 118, "end": 119, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--164"}
{"text": "The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world\u2019s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin. Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.", "tokens": ["The", "current", "state-of-the-art", "generative", "models", "for", "open-domain", "question", "answering", "(", "ODQA", ")", "have", "focused", "on", "generating", "direct", "answers", "from", "unstructured", "textual", "information", ".", "However", ",", "a", "large", "amount", "of", "world", "\u2019s", "knowledge", "is", "stored", "in", "structured", "databases", ",", "and", "need", "to", "be", "accessed", "using", "query", "languages", "such", "as", "SQL", ".", "Furthermore", ",", "query", "languages", "can", "answer", "questions", "that", "require", "complex", "reasoning", ",", "as", "well", "as", "offering", "full", "explainability", ".", "In", "this", "paper", ",", "we", "propose", "a", "hybrid", "framework", "that", "takes", "both", "textual", "and", "tabular", "evidences", "as", "input", "and", "generates", "either", "direct", "answers", "or", "SQL", "queries", "depending", "on", "which", "form", "could", "better", "answer", "the", "question", ".", "The", "generated", "SQL", "queries", "can", "then", "be", "executed", "on", "the", "associated", "databases", "to", "obtain", "the", "final", "answers", ".", "To", "the", "best", "of", "our", "knowledge", ",", "this", "is", "the", "first", "paper", "that", "applies", "Text2SQL", "to", "ODQA", "tasks", ".", "Empirically", ",", "we", "demonstrate", "that", "on", "several", "ODQA", "datasets", ",", "the", "hybrid", "methods", "consistently", "outperforms", "the", "baseline", "models", "that", "only", "takes", "homogeneous", "input", "by", "a", "large", "margin", ".", "Specifically", "we", "achieve", "the", "state-of-the-art", "performance", "on", "OpenSQuAD", "dataset", "using", "a", "T5-base", "model", ".", "In", "a", "detailed", "analysis", ",", "we", "demonstrate", "that", "the", "being", "able", "to", "generate", "structural", "SQL", "queries", "can", "always", "bring", "gains", ",", "especially", "for", "those", "questions", "that", "requires", "complex", "reasoning", "."], "entities": [{"type": "Operation", "start": 181, "end": 183, "text": "T5-base model", "sent_idx": 7}, {"type": "Effect", "start": 175, "end": 176, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--315"}
{"text": "Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.", "tokens": ["Selecting", "input", "features", "of", "top", "relevance", "has", "become", "a", "popular", "method", "for", "building", "self-explaining", "models", ".", "In", "this", "work", ",", "we", "extend", "this", "selective", "rationalization", "approach", "to", "text", "matching", ",", "where", "the", "goal", "is", "to", "jointly", "select", "and", "align", "text", "pieces", ",", "such", "as", "tokens", "or", "sentences", ",", "as", "a", "justification", "for", "the", "downstream", "prediction", ".", "Our", "approach", "employs", "optimal", "transport", "(", "OT", ")", "to", "find", "a", "minimal", "cost", "alignment", "between", "the", "inputs", ".", "However", ",", "directly", "applying", "OT", "often", "produces", "dense", "and", "therefore", "uninterpretable", "alignments", ".", "To", "overcome", "this", "limitation", ",", "we", "introduce", "novel", "constrained", "variants", "of", "the", "OT", "problem", "that", "result", "in", "highly", "sparse", "alignments", "with", "controllable", "sparsity", ".", "Our", "model", "is", "end-to-end", "differentiable", "using", "the", "Sinkhorn", "algorithm", "for", "OT", "and", "can", "be", "trained", "without", "any", "alignment", "annotations", ".", "We", "evaluate", "our", "model", "on", "the", "StackExchange", ",", "MultiNews", ",", "e-SNLI", ",", "and", "MultiRC", "datasets", ".", "Our", "model", "achieves", "very", "sparse", "rationale", "selections", "with", "high", "fidelity", "while", "preserving", "prediction", "accuracy", "compared", "to", "strong", "attention", "baseline", "models", "."], "entities": [{"type": "Operation", "start": 21, "end": 29, "text": "extend this selective rationalization approach to text matching", "sent_idx": 1}, {"type": "Effect", "start": 160, "end": 161, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--496"}
{"text": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.", "tokens": ["Recent", "works", "in", "dialogue", "state", "tracking", "(", "DST", ")", "focus", "on", "an", "open", "vocabulary-based", "setting", "to", "resolve", "scalability", "and", "generalization", "issues", "of", "the", "predefined", "ontology-based", "approaches", ".", "However", ",", "they", "are", "inefficient", "in", "that", "they", "predict", "the", "dialogue", "state", "at", "every", "turn", "from", "scratch", ".", "Here", ",", "we", "consider", "dialogue", "state", "as", "an", "explicit", "fixed-sized", "memory", "and", "propose", "a", "selectively", "overwriting", "mechanism", "for", "more", "efficient", "DST", ".", "This", "mechanism", "consists", "of", "two", "steps", ":", "(", "1", ")", "predicting", "state", "operation", "on", "each", "of", "the", "memory", "slots", ",", "and", "(", "2", ")", "overwriting", "the", "memory", "with", "new", "values", ",", "of", "which", "only", "a", "few", "are", "generated", "according", "to", "the", "predicted", "state", "operations", ".", "Our", "method", "decomposes", "DST", "into", "two", "sub-tasks", "and", "guides", "the", "decoder", "to", "focus", "only", "on", "one", "of", "the", "tasks", ",", "thus", "reducing", "the", "burden", "of", "the", "decoder", ".", "This", "enhances", "the", "effectiveness", "of", "training", "and", "DST", "performance", ".", "Our", "SOM-DST", "(", "Selectively", "Overwriting", "Memory", "for", "Dialogue", "State", "Tracking", ")", "model", "achieves", "state-of-the-art", "joint", "goal", "accuracy", "with", "51.72", "%", "in", "MultiWOZ", "2.0", "and", "53.01", "%", "in", "MultiWOZ", "2.1", "in", "an", "open", "vocabulary-based", "DST", "setting", ".", "In", "addition", ",", "we", "analyze", "the", "accuracy", "gaps", "between", "the", "current", "and", "the", "ground", "truth-given", "situations", "and", "suggest", "that", "it", "is", "a", "promising", "direction", "to", "improve", "state", "operation", "prediction", "to", "boost", "the", "DST", "performance", "."], "entities": [{"type": "Operation", "start": 151, "end": 162, "text": "SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model", "sent_idx": 6}, {"type": "Effect", "start": 164, "end": 167, "text": "joint goal accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--53"}
{"text": "Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.", "tokens": ["Existing", "word", "similarity", "measures", "are", "not", "robust", "to", "data", "sparseness", "since", "they", "rely", "only", "on", "the", "point", "estimation", "of", "words", "'", "context", "profiles", "obtained", "from", "a", "limited", "amount", "of", "data", ".", "This", "paper", "proposes", "a", "Bayesian", "method", "for", "robust", "distributional", "word", "similarities", ".", "The", "method", "uses", "a", "distribution", "of", "context", "profiles", "obtained", "by", "Bayesian", "estimation", "and", "takes", "the", "expectation", "of", "a", "base", "similarity", "measure", "under", "that", "distribution", ".", "When", "the", "context", "profiles", "are", "multinomial", "distributions", ",", "the", "priors", "are", "Dirichlet", ",", "and", "the", "base", "measure", "is", "the", "Bhattacharyya", "coefficient", ",", "we", "can", "derive", "an", "analytical", "form", "that", "allows", "efficient", "calculation", ".", "For", "the", "task", "of", "word", "similarity", "estimation", "using", "a", "large", "amount", "of", "Web", "data", "in", "Japanese", ",", "we", "show", "that", "the", "proposed", "measure", "gives", "better", "accuracies", "than", "other", "well-known", "similarity", "measures", "."], "entities": [{"type": "Operation", "start": 35, "end": 37, "text": "Bayesian method", "sent_idx": 1}, {"type": "Effect", "start": 126, "end": 127, "text": "accuracies", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1026"}
{"text": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.", "tokens": ["We", "address", "the", "problem", "of", "calibrating", "prediction", "confidence", "for", "output", "entities", "of", "interest", "in", "natural", "language", "processing", "(", "NLP", ")", "applications", ".", "It", "is", "important", "that", "NLP", "applications", "such", "as", "named", "entity", "recognition", "and", "question", "answering", "produce", "calibrated", "confidence", "scores", "for", "their", "predictions", ",", "especially", "if", "the", "applications", "are", "to", "be", "deployed", "in", "a", "safety-critical", "domain", "such", "as", "healthcare", ".", "However", "the", "output", "space", "of", "such", "structured", "prediction", "models", "are", "often", "too", "large", "to", "directly", "adapt", "binary", "or", "multi-class", "calibration", "methods", ".", "In", "this", "study", ",", "we", "propose", "a", "general", "calibration", "scheme", "for", "output", "entities", "of", "interest", "in", "neural", "network", "based", "structured", "prediction", "models", ".", "Our", "proposed", "method", "can", "be", "used", "with", "any", "binary", "class", "calibration", "scheme", "and", "a", "neural", "network", "model", ".", "Additionally", ",", "we", "show", "that", "our", "calibration", "method", "can", "also", "be", "used", "as", "an", "uncertainty-aware", ",", "entity-specific", "decoding", "step", "to", "improve", "the", "performance", "of", "the", "underlying", "model", "at", "no", "additional", "training", "cost", "or", "data", "requirements", ".", "We", "show", "that", "our", "method", "outperforms", "current", "calibration", "techniques", "for", "Named", "Entity", "Recognition", ",", "Part-of-speech", "tagging", "and", "Question", "Answering", "systems", ".", "We", "also", "observe", "an", "improvement", "in", "model", "performance", "from", "our", "decoding", "step", "across", "several", "tasks", "and", "benchmark", "datasets", ".", "Our", "method", "improves", "the", "calibration", "and", "model", "performance", "on", "out-of-domain", "test", "scenarios", "as", "well", "."], "entities": [{"type": "Operation", "start": 129, "end": 131, "text": "calibration method", "sent_idx": 5}, {"type": "Effect", "start": 145, "end": 146, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--188"}
{"text": "Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.", "tokens": ["Despite", "the", "well-developed", "cut-edge", "representation", "learning", "for", "language", ",", "most", "language", "representation", "models", "usually", "focus", "on", "specific", "levels", "of", "linguistic", "units", ".", "This", "work", "introduces", "universal", "language", "representation", "learning", ",", "i.e.", ",", "embeddings", "of", "different", "levels", "of", "linguistic", "units", "or", "text", "with", "quite", "diverse", "lengths", "in", "a", "uniform", "vector", "space", ".", "We", "propose", "the", "training", "objective", "MiSAD", "that", "utilizes", "meaningful", "n-grams", "extracted", "from", "large", "unlabeled", "corpus", "by", "a", "simple", "but", "effective", "algorithm", "for", "pre-trained", "language", "models", ".", "Then", "we", "empirically", "verify", "that", "well", "designed", "pre-training", "scheme", "may", "effectively", "yield", "universal", "language", "representation", ",", "which", "will", "bring", "great", "convenience", "when", "handling", "multiple", "layers", "of", "linguistic", "objects", "in", "a", "unified", "way", ".", "Especially", ",", "our", "model", "achieves", "the", "highest", "accuracy", "on", "analogy", "tasks", "in", "different", "language", "levels", "and", "significantly", "improves", "the", "performance", "on", "downstream", "tasks", "in", "the", "GLUE", "benchmark", "and", "a", "question", "answering", "dataset", "."], "entities": [{"type": "Operation", "start": 54, "end": 57, "text": "training objective MiSAD", "sent_idx": 2}, {"type": "Effect", "start": 117, "end": 118, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 129, "end": 130, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--398"}
{"text": "We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-to-string system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets.", "tokens": ["We", "introduce", "synchronous", "tree", "adjoining", "grammars", "(", "TAG", ")", "into", "tree-to-string", "translation", ",", "which", "converts", "a", "source", "tree", "to", "a", "target", "string", ".", "Without", "reconstructing", "TAG", "derivations", "explicitly", ",", "our", "rule", "extraction", "algorithm", "directly", "learns", "tree-to-string", "rules", "from", "aligned", "Treebank-style", "trees", ".", "As", "tree-to-string", "translation", "casts", "decoding", "as", "a", "tree", "parsing", "problem", "rather", "than", "parsing", ",", "the", "decoder", "still", "runs", "fast", "when", "adjoining", "is", "included", ".", "Less", "than", "2", "times", "slower", ",", "the", "adjoining", "tree-to-string", "system", "improves", "translation", "quality", "by", "+", "0.7", "BLEU", "over", "the", "baseline", "system", "only", "allowing", "for", "tree", "substitution", "on", "NIST", "Chinese-English", "test", "sets", "."], "entities": [{"type": "Operation", "start": 73, "end": 76, "text": "adjoining tree-to-string system", "sent_idx": 3}, {"type": "Effect", "start": 77, "end": 79, "text": "translation quality", "sent_idx": 3}, {"type": "Effect", "start": 82, "end": 83, "text": "BLEU", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "P11-1128"}
{"text": "Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models.", "tokens": ["Building", "on", "earlier", "work", "that", "integrates", "different", "factors", "in", "language", "modeling", ",", "we", "view", "(", "i", ")", "backing", "off", "to", "a", "shorter", "history", "and", "(", "ii", ")", "class-based", "generalization", "as", "two", "complementary", "mechanisms", "of", "using", "a", "larger", "equivalence", "class", "for", "prediction", "when", "the", "default", "equivalence", "class", "is", "too", "small", "for", "reliable", "estimation", ".", "This", "view", "entails", "that", "the", "classes", "in", "a", "language", "model", "should", "be", "learned", "from", "rare", "events", "only", "and", "should", "be", "preferably", "applied", "to", "rare", "events", ".", "We", "construct", "such", "a", "model", "and", "show", "that", "both", "training", "on", "rare", "events", "and", "preferable", "application", "to", "rare", "events", "improve", "perplexity", "when", "compared", "to", "a", "simple", "direct", "interpolation", "of", "class-based", "with", "standard", "language", "models", "."], "entities": [{"type": "Operation", "start": 87, "end": 98, "text": "both training on rare events and preferable application to rare events", "sent_idx": 2}, {"type": "Effect", "start": 99, "end": 100, "text": "perplexity", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1152"}
{"text": "Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance.", "tokens": ["Searching", "documents", "that", "are", "similar", "to", "a", "query", "document", "is", "an", "important", "component", "in", "modern", "information", "retrieval", ".", "Some", "existing", "hashing", "methods", "can", "be", "used", "for", "efficient", "document", "similarity", "search", ".", "However", ",", "unsupervised", "hashing", "methods", "can", "not", "incorporate", "prior", "knowledge", "for", "better", "hashing", ".", "Although", "some", "supervised", "hashing", "methods", "can", "derive", "effective", "hash", "functions", "from", "prior", "knowledge", ",", "they", "are", "either", "computationally", "expensive", "or", "poorly", "discriminative", ".", "This", "paper", "proposes", "a", "novel", "(semi-)supervised", "hashing", "method", "named", "Semi-Supervised", "SimHash", "(", "S3H", ")", "for", "high-dimensional", "data", "similarity", "search", ".", "The", "basic", "idea", "of", "S3H", "is", "to", "learn", "the", "optimal", "feature", "weights", "from", "prior", "knowledge", "to", "relocate", "the", "data", "such", "that", "similar", "data", "have", "similar", "hash", "codes", ".", "We", "evaluate", "our", "method", "with", "several", "state-of-the-art", "methods", "on", "two", "large", "datasets", ".", "All", "the", "results", "show", "that", "our", "method", "gets", "the", "best", "performance", "."], "entities": [{"type": "Operation", "start": 73, "end": 76, "text": "(semi-)supervised hashing method", "sent_idx": 4}, {"type": "Effect", "start": 139, "end": 140, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1010"}
{"text": "In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar.", "tokens": ["In", "the", "deep", "learning", "(", "DL", ")", "era", ",", "parsing", "models", "are", "extremely", "simplified", "with", "little", "hurt", "on", "performance", ",", "thanks", "to", "the", "remarkable", "capability", "of", "multi-layer", "BiLSTMs", "in", "context", "representation", ".", "As", "the", "most", "popular", "graph-based", "dependency", "parser", "due", "to", "its", "high", "efficiency", "and", "performance", ",", "the", "biaffine", "parser", "directly", "scores", "single", "dependencies", "under", "the", "arc-factorization", "assumption", ",", "and", "adopts", "a", "very", "simple", "local", "token-wise", "cross-entropy", "training", "loss", ".", "This", "paper", "for", "the", "first", "time", "presents", "a", "second-order", "TreeCRF", "extension", "to", "the", "biaffine", "parser", ".", "For", "a", "long", "time", ",", "the", "complexity", "and", "inefficiency", "of", "the", "inside-outside", "algorithm", "hinder", "the", "popularity", "of", "TreeCRF", ".", "To", "address", "this", "issue", ",", "we", "propose", "an", "effective", "way", "to", "batchify", "the", "inside", "and", "Viterbi", "algorithms", "for", "direct", "large", "matrix", "operation", "on", "GPUs", ",", "and", "to", "avoid", "the", "complex", "outside", "algorithm", "via", "efficient", "back-propagation", ".", "Experiments", "and", "analysis", "on", "27", "datasets", "from", "13", "languages", "clearly", "show", "that", "techniques", "developed", "before", "the", "DL", "era", ",", "such", "as", "structural", "learning", "(", "global", "TreeCRF", "loss", ")", "and", "high-order", "modeling", "are", "still", "useful", ",", "and", "can", "further", "boost", "parsing", "performance", "over", "the", "state-of-the-art", "biaffine", "parser", ",", "especially", "for", "partially", "annotated", "training", "data", ".", "We", "release", "our", "code", "at", "https://github.com/yzhangcs/crfpar", "."], "entities": [{"type": "Operation", "start": 153, "end": 159, "text": "techniques developed before the DL era", "sent_idx": 5}, {"type": "Effect", "start": 181, "end": 182, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--302"}
{"text": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT\u201914 English-German and WMT\u201917 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).", "tokens": ["A", "neural", "machine", "translation", "(", "NMT", ")", "system", "is", "expensive", "to", "train", ",", "especially", "with", "high-resource", "settings", ".", "As", "the", "NMT", "architectures", "become", "deeper", "and", "wider", ",", "this", "issue", "gets", "worse", "and", "worse", ".", "In", "this", "paper", ",", "we", "aim", "to", "improve", "the", "efficiency", "of", "training", "an", "NMT", "by", "introducing", "a", "novel", "norm-based", "curriculum", "learning", "method", ".", "We", "use", "the", "norm", "(", "aka", "length", "or", "module", ")", "of", "a", "word", "embedding", "as", "a", "measure", "of", "1", ")", "the", "difficulty", "of", "the", "sentence", ",", "2", ")", "the", "competence", "of", "the", "model", ",", "and", "3", ")", "the", "weight", "of", "the", "sentence", ".", "The", "norm-based", "sentence", "difficulty", "takes", "the", "advantages", "of", "both", "linguistically", "motivated", "and", "model-based", "sentence", "difficulties", ".", "It", "is", "easy", "to", "determine", "and", "contains", "learning-dependent", "features", ".", "The", "norm-based", "model", "competence", "makes", "NMT", "learn", "the", "curriculum", "in", "a", "fully", "automated", "way", ",", "while", "the", "norm-based", "sentence", "weight", "further", "enhances", "the", "learning", "of", "the", "vector", "representation", "of", "the", "NMT", ".", "Experimental", "results", "for", "the", "WMT\u201914", "English-German", "and", "WMT\u201917", "Chinese-English", "translation", "tasks", "demonstrate", "that", "the", "proposed", "method", "outperforms", "strong", "baselines", "in", "terms", "of", "BLEU", "score", "(", "+", "1.17/+1.56", ")", "and", "training", "speedup", "(", "2.22x/3.33x", ")", "."], "entities": [{"type": "Operation", "start": 127, "end": 129, "text": "norm-based model", "sent_idx": 6}, {"type": "Effect", "start": 180, "end": 182, "text": "BLEU score", "sent_idx": 7}, {"type": "Effect", "start": 188, "end": 189, "text": "speedup", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--41"}
{"text": "It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.", "tokens": ["It", "is", "a", "common", "belief", "that", "training", "deep", "transformers", "from", "scratch", "requires", "large", "datasets", ".", "Consequently", ",", "for", "small", "datasets", ",", "people", "usually", "use", "shallow", "and", "simple", "additional", "layers", "on", "top", "of", "pre-trained", "models", "during", "fine-tuning", ".", "This", "work", "shows", "that", "this", "does", "not", "always", "need", "to", "be", "the", "case", ":", "with", "proper", "initialization", "and", "optimization", ",", "the", "benefits", "of", "very", "deep", "transformers", "can", "carry", "over", "to", "challenging", "tasks", "with", "small", "datasets", ",", "including", "Text-to-SQL", "semantic", "parsing", "and", "logical", "reading", "comprehension", ".", "In", "particular", ",", "we", "successfully", "train", "48", "layers", "of", "transformers", ",", "comprising", "24", "fine-tuned", "layers", "from", "pre-trained", "RoBERTa", "and", "24", "relation-aware", "layers", "trained", "from", "scratch", ".", "With", "fewer", "training", "steps", "and", "no", "task-specific", "pre-training", ",", "we", "obtain", "the", "state", "of", "the", "art", "performance", "on", "the", "challenging", "cross-domain", "Text-to-SQL", "parsing", "benchmark", "Spider", ".", "We", "achieve", "this", "by", "deriving", "a", "novel", "Data", "dependent", "Transformer", "Fixed-update", "initialization", "scheme", "(", "DT-Fixup", ")", ",", "inspired", "by", "the", "prior", "T-Fixup", "work", ".", "Further", "error", "analysis", "shows", "that", "increasing", "depth", "can", "help", "improve", "generalization", "on", "small", "datasets", "for", "hard", "cases", "that", "require", "reasoning", "and", "structural", "understanding", "."], "entities": [{"type": "Operation", "start": 163, "end": 165, "text": "increasing depth", "sent_idx": 6}, {"type": "Effect", "start": 168, "end": 169, "text": "generalization", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--163"}
{"text": "We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities.", "tokens": ["We", "study", "the", "problem", "of", "building", "entity", "tagging", "systems", "by", "using", "a", "few", "rules", "as", "weak", "supervision", ".", "Previous", "methods", "mostly", "focus", "on", "disambiguating", "entity", "types", "based", "on", "contexts", "and", "expert-provided", "rules", ",", "while", "assuming", "entity", "spans", "are", "given", ".", "In", "this", "work", ",", "we", "propose", "a", "novel", "method", "TALLOR", "that", "bootstraps", "high-quality", "logical", "rules", "to", "train", "a", "neural", "tagger", "in", "a", "fully", "automated", "manner", ".", "Specifically", ",", "we", "introduce", "compound", "rules", "that", "are", "composed", "from", "simple", "rules", "to", "increase", "the", "precision", "of", "boundary", "detection", "and", "generate", "more", "diverse", "pseudo", "labels", ".", "We", "further", "design", "a", "dynamic", "label", "selection", "strategy", "to", "ensure", "pseudo", "label", "quality", "and", "therefore", "avoid", "overfitting", "the", "neural", "tagger", ".", "Experiments", "on", "three", "datasets", "demonstrate", "that", "our", "method", "outperforms", "other", "weakly", "supervised", "methods", "and", "even", "rivals", "a", "state-of-the-art", "distantly", "supervised", "tagger", "with", "a", "lexicon", "of", "over", "2,000", "terms", "when", "starting", "from", "only", "20", "simple", "rules", ".", "Our", "method", "can", "serve", "as", "a", "tool", "for", "rapidly", "building", "taggers", "in", "emerging", "domains", "and", "tasks", ".", "Case", "studies", "show", "that", "learned", "rules", "can", "potentially", "explain", "the", "predicted", "entities", "."], "entities": [{"type": "Operation", "start": 70, "end": 72, "text": "compound rules", "sent_idx": 3}, {"type": "Effect", "start": 81, "end": 82, "text": "precision", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--352"}
{"text": "The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.", "tokens": ["The", "timings", "of", "spoken", "response", "offsets", "in", "human", "dialogue", "have", "been", "shown", "to", "vary", "based", "on", "contextual", "elements", "of", "the", "dialogue", ".", "We", "propose", "neural", "models", "that", "simulate", "the", "distributions", "of", "these", "response", "offsets", ",", "taking", "into", "account", "the", "response", "turn", "as", "well", "as", "the", "preceding", "turn", ".", "The", "models", "are", "designed", "to", "be", "integrated", "into", "the", "pipeline", "of", "an", "incremental", "spoken", "dialogue", "system", "(", "SDS", ")", ".", "We", "evaluate", "our", "models", "using", "offline", "experiments", "as", "well", "as", "human", "listening", "tests", ".", "We", "show", "that", "human", "listeners", "consider", "certain", "response", "timings", "to", "be", "more", "natural", "based", "on", "the", "dialogue", "context", ".", "The", "introduction", "of", "these", "models", "into", "SDS", "pipelines", "could", "increase", "the", "perceived", "naturalness", "of", "interactions", "."], "entities": [{"type": "Operation", "start": 102, "end": 109, "text": "introduction of these models into SDS pipelines", "sent_idx": 5}, {"type": "Effect", "start": 112, "end": 116, "text": "perceived naturalness of interactions", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--221"}
{"text": "The advent of contextual word embeddings \u2014 representations of words which incorporate semantic and syntactic information from their context\u2014has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.", "tokens": ["The", "advent", "of", "contextual", "word", "embeddings", "\u2014", "representations", "of", "words", "which", "incorporate", "semantic", "and", "syntactic", "information", "from", "their", "context", "\u2014", "has", "led", "to", "tremendous", "improvements", "on", "a", "wide", "variety", "of", "NLP", "tasks", ".", "However", ",", "recent", "contextual", "models", "have", "prohibitively", "high", "computational", "cost", "in", "many", "use-cases", "and", "are", "often", "hard", "to", "interpret", ".", "In", "this", "work", ",", "we", "demonstrate", "that", "our", "proposed", "distillation", "method", ",", "which", "is", "a", "simple", "extension", "of", "CBOW-based", "training", ",", "allows", "to", "significantly", "improve", "computational", "efficiency", "of", "NLP", "applications", ",", "while", "outperforming", "the", "quality", "of", "existing", "static", "embeddings", "trained", "from", "scratch", "as", "well", "as", "those", "distilled", "from", "previously", "proposed", "methods", ".", "As", "a", "side-effect", ",", "our", "approach", "also", "allows", "a", "fair", "comparison", "of", "both", "contextual", "and", "static", "embeddings", "via", "standard", "lexical", "evaluation", "tasks", "."], "entities": [{"type": "Operation", "start": 71, "end": 73, "text": "CBOW-based training", "sent_idx": 2}, {"type": "Effect", "start": 78, "end": 80, "text": "computational efficiency", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--408"}
{"text": "Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities. Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality. FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation. Extensive experiments show the effectiveness of such embeddings. Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.", "tokens": ["Entity", "embeddings", ",", "which", "represent", "different", "aspects", "of", "each", "entity", "with", "a", "single", "vector", "like", "word", "embeddings", ",", "are", "a", "key", "component", "of", "neural", "entity", "linking", "models", ".", "Existing", "entity", "embeddings", "are", "learned", "from", "canonical", "Wikipedia", "articles", "and", "local", "contexts", "surrounding", "target", "entities", ".", "Such", "entity", "embeddings", "are", "effective", ",", "but", "too", "distinctive", "for", "linking", "models", "to", "learn", "contextual", "commonality", ".", "We", "propose", "a", "simple", "yet", "effective", "method", ",", "FGS2EE", ",", "to", "inject", "fine-grained", "semantic", "information", "into", "entity", "embeddings", "to", "reduce", "the", "distinctiveness", "and", "facilitate", "the", "learning", "of", "contextual", "commonality", ".", "FGS2EE", "first", "uses", "the", "embeddings", "of", "semantic", "type", "words", "to", "generate", "semantic", "embeddings", ",", "and", "then", "combines", "them", "with", "existing", "entity", "embeddings", "through", "linear", "aggregation", ".", "Extensive", "experiments", "show", "the", "effectiveness", "of", "such", "embeddings", ".", "Based", "on", "our", "entity", "embeddings", ",", "we", "achieved", "new", "sate-of-the-art", "performance", "on", "entity", "linking", "."], "entities": [{"type": "Operation", "start": 91, "end": 92, "text": "FGS2EE", "sent_idx": 4}, {"type": "Effect", "start": 136, "end": 137, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--612"}
{"text": "Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.", "tokens": ["Transformers", "have", "gradually", "become", "a", "key", "component", "for", "many", "state-of-the-art", "natural", "language", "representation", "models", ".", "A", "recent", "Transformer", "based", "model-", "BERTachieved", "state-of-the-art", "results", "on", "various", "natural", "language", "processing", "tasks", ",", "including", "GLUE", ",", "SQuAD", "v1.1", ",", "and", "SQuAD", "v2.0", ".", "This", "model", "however", "is", "computationally", "prohibitive", "and", "has", "a", "huge", "number", "of", "parameters", ".", "In", "this", "work", "we", "revisit", "the", "architecture", "choices", "of", "BERT", "in", "efforts", "to", "obtain", "a", "lighter", "model", ".", "We", "focus", "on", "reducing", "the", "number", "of", "parameters", "yet", "our", "methods", "can", "be", "applied", "towards", "other", "objectives", "such", "FLOPs", "or", "latency", ".", "We", "show", "that", "much", "efficient", "light", "BERT", "models", "can", "be", "obtained", "by", "reducing", "algorithmically", "chosen", "correct", "architecture", "design", "dimensions", "rather", "than", "reducing", "the", "number", "of", "Transformer", "encoder", "layers", ".", "In", "particular", ",", "our", "schuBERT", "gives", "6.6", "%", "higher", "average", "accuracy", "on", "GLUE", "and", "SQuAD", "datasets", "as", "compared", "to", "BERT", "with", "three", "encoder", "layers", "while", "having", "the", "same", "number", "of", "parameters", "."], "entities": [{"type": "Operation", "start": 127, "end": 128, "text": "schuBERT", "sent_idx": 6}, {"type": "Effect", "start": 132, "end": 134, "text": "average accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--250"}
{"text": "We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT\u201919 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT\u201919 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available.", "tokens": ["We", "show", "that", "margin-based", "bitext", "mining", "in", "a", "multilingual", "sentence", "space", "can", "be", "successfully", "scaled", "to", "operate", "on", "monolingual", "corpora", "of", "billions", "of", "sentences", ".", "We", "use", "32", "snapshots", "of", "a", "curated", "common", "crawl", "corpus", "(", "Wenzel", "et", "al", ",", "2019", ")", "totaling", "71", "billion", "unique", "sentences", ".", "Using", "one", "unified", "approach", "for", "90", "languages", ",", "we", "were", "able", "to", "mine", "10.8", "billion", "parallel", "sentences", ",", "out", "of", "which", "only", "2.9", "billions", "are", "aligned", "with", "English", ".", "We", "illustrate", "the", "capability", "of", "our", "scalable", "mining", "system", "to", "create", "high", "quality", "training", "sets", "from", "one", "language", "to", "any", "other", "by", "training", "hundreds", "of", "different", "machine", "translation", "models", "and", "evaluating", "them", "on", "the", "many-to-many", "TED", "benchmark", ".", "Further", ",", "we", "evaluate", "on", "competitive", "translation", "benchmarks", "such", "as", "WMT", "and", "WAT", ".", "Using", "only", "mined", "bitext", ",", "we", "set", "a", "new", "state", "of", "the", "art", "for", "a", "single", "system", "on", "the", "WMT\u201919", "test", "set", "for", "English-German/Russian/Chinese", ".", "In", "particular", ",", "our", "English/German", "and", "English/Russian", "systems", "outperform", "the", "best", "single", "ones", "by", "over", "4", "BLEU", "points", "and", "are", "on", "par", "with", "best", "WMT\u201919", "systems", ",", "which", "train", "on", "the", "WMT", "training", "data", "and", "augment", "it", "with", "backtranslation", ".", "We", "also", "achieve", "excellent", "results", "for", "distant", "languages", "pairs", "like", "Russian/Japanese", ",", "outperforming", "the", "best", "submission", "at", "the", "2020", "WAT", "workshop", ".", "All", "of", "the", "mined", "bitext", "will", "be", "freely", "available", "."], "entities": [{"type": "Operation", "start": 158, "end": 162, "text": "English/German and English/Russian systems", "sent_idx": 6}, {"type": "Effect", "start": 170, "end": 172, "text": "BLEU points", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--507"}
{"text": "Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT\u2019s effectiveness. All source codes of this work are available at https://github.com/thunlp/KernelGAT.", "tokens": ["Fact", "Verification", "requires", "fine-grained", "natural", "language", "inference", "capability", "that", "finds", "subtle", "clues", "to", "identify", "the", "syntactical", "and", "semantically", "correct", "but", "not", "well-supported", "claims", ".", "This", "paper", "presents", "Kernel", "Graph", "Attention", "Network", "(", "KGAT", ")", ",", "which", "conducts", "more", "fine-grained", "fact", "verification", "with", "kernel-based", "attentions", ".", "Given", "a", "claim", "and", "a", "set", "of", "potential", "evidence", "sentences", "that", "form", "an", "evidence", "graph", ",", "KGAT", "introduces", "node", "kernels", ",", "which", "better", "measure", "the", "importance", "of", "the", "evidence", "node", ",", "and", "edge", "kernels", ",", "which", "conduct", "fine-grained", "evidence", "propagation", "in", "the", "graph", ",", "into", "Graph", "Attention", "Networks", "for", "more", "accurate", "fact", "verification", ".", "KGAT", "achieves", "a", "70.38", "%", "FEVER", "score", "and", "significantly", "outperforms", "existing", "fact", "verification", "models", "on", "FEVER", ",", "a", "large-scale", "benchmark", "for", "fact", "verification", ".", "Our", "analyses", "illustrate", "that", ",", "compared", "to", "dot-product", "attentions", ",", "the", "kernel-based", "attention", "concentrates", "more", "on", "relevant", "evidence", "sentences", "and", "meaningful", "clues", "in", "the", "evidence", "graph", ",", "which", "is", "the", "main", "source", "of", "KGAT", "\u2019s", "effectiveness", ".", "All", "source", "codes", "of", "this", "work", "are", "available", "at", "https://github.com/thunlp/KernelGAT", "."], "entities": [{"type": "Operation", "start": 99, "end": 100, "text": "KGAT", "sent_idx": 3}, {"type": "Effect", "start": 104, "end": 106, "text": "FEVER score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--655"}
{"text": "We present a novel model to represent and assess the discourse coherence of text. Our model assumes that coherent text implicitly favors certain types of discourse relation transitions. We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. The experimental results demonstrate that our model is able to significantly outperform the state-of-the-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task.", "tokens": ["We", "present", "a", "novel", "model", "to", "represent", "and", "assess", "the", "discourse", "coherence", "of", "text", ".", "Our", "model", "assumes", "that", "coherent", "text", "implicitly", "favors", "certain", "types", "of", "discourse", "relation", "transitions", ".", "We", "implement", "this", "model", "and", "apply", "it", "towards", "the", "text", "ordering", "ranking", "task", ",", "which", "aims", "to", "discern", "an", "original", "text", "from", "a", "permuted", "ordering", "of", "its", "sentences", ".", "The", "experimental", "results", "demonstrate", "that", "our", "model", "is", "able", "to", "significantly", "outperform", "the", "state-of-the-art", "coherence", "model", "by", "Barzilay", "and", "Lapata", "(", "2005", ")", ",", "reducing", "the", "error", "rate", "of", "the", "previous", "approach", "by", "an", "average", "of", "29", "%", "over", "three", "data", "sets", "against", "human", "upper", "bounds", ".", "We", "further", "show", "that", "our", "model", "is", "synergistic", "with", "the", "previous", "approach", ",", "demonstrating", "an", "error", "reduction", "of", "73", "%", "when", "the", "features", "from", "both", "models", "are", "combined", "for", "the", "task", "."], "entities": [{"type": "Operation", "start": 110, "end": 112, "text": "our model", "sent_idx": 4}, {"type": "Effect", "start": 121, "end": 122, "text": "error", "sent_idx": 4}, {"type": "Operation", "start": 64, "end": 66, "text": "our model", "sent_idx": 3}, {"type": "Effect", "start": 85, "end": 87, "text": "error rate", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 3}], "id": "P11-1100"}
{"text": "We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Sch\u00fctze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson\u2019s r over single-source systems.", "tokens": ["We", "address", "the", "task", "of", "unsupervised", "Semantic", "Textual", "Similarity", "(", "STS", ")", "by", "ensembling", "diverse", "pre-trained", "sentence", "encoders", "into", "sentence", "meta-embeddings", ".", "We", "apply", ",", "extend", "and", "evaluate", "different", "meta-embedding", "methods", "from", "the", "word", "embedding", "literature", "at", "the", "sentence", "level", ",", "including", "dimensionality", "reduction", "(", "Yin", "and", "Sch\u00fctze", ",", "2016", ")", ",", "generalized", "Canonical", "Correlation", "Analysis", "(", "Rastogi", "et", "al.", ",", "2015", ")", "and", "cross-view", "auto-encoders", "(", "Bollegala", "and", "Bao", ",", "2018", ")", ".", "Our", "sentence", "meta-embeddings", "set", "a", "new", "unsupervised", "State", "of", "The", "Art", "(", "SoTA", ")", "on", "the", "STS", "Benchmark", "and", "on", "the", "STS12-STS16", "datasets", ",", "with", "gains", "of", "between", "3.7", "%", "and", "6.4", "%", "Pearson", "\u2019s", "r", "over", "single-source", "systems", "."], "entities": [{"type": "Operation", "start": 75, "end": 77, "text": "sentence meta-embeddings", "sent_idx": 2}, {"type": "Effect", "start": 107, "end": 110, "text": "Pearson\u2019s r", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--628"}
{"text": "Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.", "tokens": ["Conversational", "Question", "Simplification", "(", "CQS", ")", "aims", "to", "simplify", "self-contained", "questions", "into", "conversational", "ones", "by", "incorporating", "some", "conversational", "characteristics", ",", "e.g.", ",", "anaphora", "and", "ellipsis", ".", "Existing", "maximum", "likelihood", "estimation", "based", "methods", "often", "get", "trapped", "in", "easily", "learned", "tokens", "as", "all", "tokens", "are", "treated", "equally", "during", "training", ".", "In", "this", "work", ",", "we", "introduce", "a", "Reinforcement", "Iterative", "Sequence", "Editing", "(", "RISE", ")", "framework", "that", "optimizes", "the", "minimum", "Levenshtein", "distance", "through", "explicit", "editing", "actions", ".", "RISE", "is", "able", "to", "pay", "attention", "to", "tokens", "that", "are", "related", "to", "conversational", "characteristics", ".", "To", "train", "RISE", ",", "we", "devise", "an", "Iterative", "Reinforce", "Training", "(", "IRT", ")", "algorithm", "with", "a", "Dynamic", "Programming", "based", "Sampling", "(", "DPS", ")", "process", "to", "improve", "exploration", ".", "Experimental", "results", "on", "two", "benchmark", "datasets", "show", "that", "RISE", "significantly", "outperforms", "state-of-the-art", "methods", "and", "generalizes", "well", "on", "unseen", "data", "."], "entities": [{"type": "Operation", "start": 55, "end": 63, "text": "Reinforcement Iterative Sequence Editing (RISE) framework", "sent_idx": 2}, {"type": "Effect", "start": 66, "end": 69, "text": "minimum Levenshtein distance", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--438"}
{"text": "Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.", "tokens": ["Leveraging", "persona", "information", "of", "users", "in", "Neural", "Response", "Generators", "(", "NRG", ")", "to", "perform", "personalized", "conversations", "has", "been", "considered", "as", "an", "attractive", "and", "important", "topic", "in", "the", "research", "of", "conversational", "agents", "over", "the", "past", "few", "years", ".", "Despite", "of", "the", "promising", "progress", "achieved", "by", "recent", "studies", "in", "this", "field", ",", "persona", "information", "tends", "to", "be", "incorporated", "into", "neural", "networks", "in", "the", "form", "of", "user", "embeddings", ",", "with", "the", "expectation", "that", "the", "persona", "can", "be", "involved", "via", "End-to-End", "learning", ".", "This", "paper", "proposes", "to", "adopt", "the", "personality-related", "characteristics", "of", "human", "conversations", "into", "variational", "response", "generators", ",", "by", "designing", "a", "specific", "conditional", "variational", "autoencoder", "based", "deep", "model", "with", "two", "new", "regularization", "terms", "employed", "to", "the", "loss", "function", ",", "so", "as", "to", "guide", "the", "optimization", "towards", "the", "direction", "of", "generating", "both", "persona-aware", "and", "relevant", "responses", ".", "Besides", ",", "to", "reasonably", "evaluate", "the", "performances", "of", "various", "persona", "modeling", "approaches", ",", "this", "paper", "further", "presents", "three", "direct", "persona-oriented", "metrics", "from", "different", "perspectives", ".", "The", "experimental", "results", "have", "shown", "that", "our", "proposed", "methodology", "can", "notably", "improve", "the", "performance", "of", "persona-aware", "response", "generation", ",", "and", "the", "metrics", "are", "reasonable", "to", "evaluate", "the", "results", "."], "entities": [{"type": "Operation", "start": 99, "end": 105, "text": "conditional variational autoencoder based deep model", "sent_idx": 2}, {"type": "Effect", "start": 171, "end": 172, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--7"}
{"text": "Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.", "tokens": ["Deep", "learning", "models", "have", "achieved", "great", "success", "on", "the", "task", "of", "Natural", "Language", "Inference", "(", "NLI", ")", ",", "though", "only", "a", "few", "attempts", "try", "to", "explain", "their", "behaviors", ".", "Existing", "explanation", "methods", "usually", "pick", "prominent", "features", "such", "as", "words", "or", "phrases", "from", "the", "input", "text", ".", "However", ",", "for", "NLI", ",", "alignments", "among", "words", "or", "phrases", "are", "more", "enlightening", "clues", "to", "explain", "the", "model", ".", "To", "this", "end", ",", "this", "paper", "presents", "AREC", ",", "a", "post-hoc", "approach", "to", "generate", "alignment", "rationale", "explanations", "for", "co-attention", "based", "models", "in", "NLI", ".", "The", "explanation", "is", "based", "on", "feature", "selection", ",", "which", "keeps", "few", "but", "sufficient", "alignments", "while", "maintaining", "the", "same", "prediction", "of", "the", "target", "model", ".", "Experimental", "results", "show", "that", "our", "method", "is", "more", "faithful", "and", "human-readable", "compared", "with", "many", "existing", "approaches", ".", "We", "further", "study", "and", "re-evaluate", "three", "typical", "models", "through", "our", "explanation", "beyond", "accuracy", ",", "and", "propose", "a", "simple", "method", "that", "greatly", "improves", "the", "model", "robustness", "."], "entities": [{"type": "Operation", "start": 72, "end": 73, "text": "AREC", "sent_idx": 3}, {"type": "Effect", "start": 142, "end": 143, "text": "accuracy", "sent_idx": 6}, {"type": "Effect", "start": 153, "end": 155, "text": "model robustness", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--417"}
{"text": "Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.", "tokens": ["Multimodal", "pre-training", "models", ",", "such", "as", "LXMERT", ",", "have", "achieved", "excellent", "results", "in", "downstream", "tasks", ".", "However", ",", "current", "pre-trained", "models", "require", "large", "amounts", "of", "training", "data", "and", "have", "huge", "model", "sizes", ",", "which", "make", "them", "impossible", "to", "apply", "in", "low-resource", "situations", ".", "How", "to", "obtain", "similar", "or", "even", "better", "performance", "than", "a", "larger", "model", "under", "the", "premise", "of", "less", "pre-training", "data", "and", "smaller", "model", "size", "has", "become", "an", "important", "problem", ".", "In", "this", "paper", ",", "we", "propose", "a", "new", "Multi-stage", "Pre-training", "(", "MSP", ")", "method", ",", "which", "uses", "information", "at", "different", "granularities", "from", "word", ",", "phrase", "to", "sentence", "in", "both", "texts", "and", "images", "to", "pre-train", "a", "model", "in", "stages", ".", "We", "also", "design", "several", "different", "pre-training", "tasks", "suitable", "for", "the", "information", "granularity", "in", "different", "stage", "in", "order", "to", "efficiently", "capture", "the", "diverse", "knowledge", "from", "a", "limited", "corpus", ".", "We", "take", "a", "Simplified", "LXMERT", "(", "LXMERT-S", ")", "which", "is", "with", "45.9", "%", "parameters", "of", "the", "original", "LXMERT", "model", "and", "only", "11.44", "%", "of", "the", "original", "pre-training", "data", "as", "the", "testbed", "of", "our", "MSP", "method", ".", "Experimental", "results", "show", "that", "our", "method", "achieves", "comparable", "performance", "to", "the", "original", "LXMERT", "model", "in", "all", "downstream", "tasks", ",", "and", "even", "outperforms", "the", "original", "model", "in", "Image-Text", "Retrieval", "task", "."], "entities": [{"type": "Operation", "start": 142, "end": 147, "text": "Simplified LXMERT (LXMERT-S)", "sent_idx": 5}, {"type": "Effect", "start": 152, "end": 153, "text": "parameters", "sent_idx": 5}, {"type": "Effect", "start": 183, "end": 184, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--199"}
{"text": "Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora.", "tokens": ["Open-domain", "dialog", "systems", "have", "a", "user-centric", "goal", ":", "to", "provide", "humans", "with", "an", "engaging", "conversation", "experience", ".", "User", "engagement", "is", "one", "of", "the", "most", "important", "metrics", "for", "evaluating", "open-domain", "dialog", "systems", ",", "and", "could", "also", "be", "used", "as", "real-time", "feedback", "to", "benefit", "dialog", "policy", "learning", ".", "Existing", "work", "on", "detecting", "user", "disengagement", "typically", "requires", "hand-labeling", "many", "dialog", "samples", ".", "We", "propose", "HERALD", ",", "an", "efficient", "annotation", "framework", "that", "reframes", "the", "training", "data", "annotation", "process", "as", "a", "denoising", "problem", ".", "Specifically", ",", "instead", "of", "manually", "labeling", "training", "samples", ",", "we", "first", "use", "a", "set", "of", "labeling", "heuristics", "to", "label", "training", "samples", "automatically", ".", "We", "then", "denoise", "the", "weakly", "labeled", "data", "using", "the", "Shapley", "algorithm", ".", "Finally", ",", "we", "use", "the", "denoised", "data", "to", "train", "a", "user", "engagement", "detector", ".", "Our", "experiments", "show", "that", "HERALD", "improves", "annotation", "efficiency", "significantly", "and", "achieves", "86", "%", "user", "disengagement", "detection", "accuracy", "in", "two", "dialog", "corpora", "."], "entities": [{"type": "Operation", "start": 132, "end": 133, "text": "HERALD", "sent_idx": 7}, {"type": "Effect", "start": 134, "end": 136, "text": "annotation efficiency", "sent_idx": 7}, {"type": "Effect", "start": 141, "end": 145, "text": "user disengagement detection accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--283"}
{"text": "Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.", "tokens": ["Text-to-image", "retrieval", "is", "an", "essential", "task", "in", "cross-modal", "information", "retrieval", ",", "i.e.", ",", "retrieving", "relevant", "images", "from", "a", "large", "and", "unlabelled", "dataset", "given", "textual", "queries", ".", "In", "this", "paper", ",", "we", "propose", "VisualSparta", ",", "a", "novel", "(", "Visual-text", "Sparse", "Transformer", "Matching", ")", "model", "that", "shows", "significant", "improvement", "in", "terms", "of", "both", "accuracy", "and", "efficiency", ".", "VisualSparta", "is", "capable", "of", "outperforming", "previous", "state-of-the-art", "scalable", "methods", "in", "MSCOCO", "and", "Flickr30", "K", ".", "We", "also", "show", "that", "it", "achieves", "substantial", "retrieving", "speed", "advantages", ",", "i.e.", ",", "for", "a", "1", "million", "image", "index", ",", "VisualSparta", "using", "CPU", "gets", "~391X", "speedup", "compared", "to", "CPU", "vector", "search", "and", "~5.4X", "speedup", "compared", "to", "vector", "search", "with", "GPU", "acceleration", ".", "Experiments", "show", "that", "this", "speed", "advantage", "even", "gets", "bigger", "for", "larger", "datasets", "because", "VisualSparta", "can", "be", "efficiently", "implemented", "as", "an", "inverted", "index", ".", "To", "the", "best", "of", "our", "knowledge", ",", "VisualSparta", "is", "the", "first", "transformer-based", "text-to-image", "retrieval", "model", "that", "can", "achieve", "real-time", "searching", "for", "large-scale", "datasets", ",", "with", "significant", "accuracy", "improvement", "compared", "to", "previous", "state-of-the-art", "methods", "."], "entities": [{"type": "Operation", "start": 142, "end": 143, "text": "VisualSparta", "sent_idx": 5}, {"type": "Effect", "start": 161, "end": 162, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--389"}
{"text": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.", "tokens": ["With", "the", "great", "success", "of", "pre-trained", "language", "models", ",", "the", "pretrain-finetune", "paradigm", "now", "becomes", "the", "undoubtedly", "dominant", "solution", "for", "natural", "language", "understanding", "(", "NLU", ")", "tasks", ".", "At", "the", "fine-tune", "stage", ",", "target", "task", "data", "is", "usually", "introduced", "in", "a", "completely", "random", "order", "and", "treated", "equally", ".", "However", ",", "examples", "in", "NLU", "tasks", "can", "vary", "greatly", "in", "difficulty", ",", "and", "similar", "to", "human", "learning", "procedure", ",", "language", "models", "can", "benefit", "from", "an", "easy-to-difficult", "curriculum", ".", "Based", "on", "this", "idea", ",", "we", "propose", "our", "Curriculum", "Learning", "approach", ".", "By", "reviewing", "the", "trainset", "in", "a", "crossed", "way", ",", "we", "are", "able", "to", "distinguish", "easy", "examples", "from", "difficult", "ones", ",", "and", "arrange", "a", "curriculum", "for", "language", "models", ".", "Without", "any", "manual", "model", "architecture", "design", "or", "use", "of", "external", "data", ",", "our", "Curriculum", "Learning", "approach", "obtains", "significant", "and", "universal", "performance", "improvements", "on", "a", "wide", "range", "of", "NLU", "tasks", "."], "entities": [{"type": "Operation", "start": 128, "end": 131, "text": "Curriculum Learning approach", "sent_idx": 5}, {"type": "Effect", "start": 135, "end": 136, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--542"}
{"text": "We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.", "tokens": ["We", "present", "a", "human-and-model-in-the-loop", "process", "for", "dynamically", "generating", "datasets", "and", "training", "better", "performing", "and", "more", "robust", "hate", "detection", "models", ".", "We", "provide", "a", "new", "dataset", "of", "40,000", "entries", ",", "generated", "and", "labelled", "by", "trained", "annotators", "over", "four", "rounds", "of", "dynamic", "data", "creation", ".", "It", "includes", "15,000", "challenging", "perturbations", "and", "each", "hateful", "entry", "has", "fine-grained", "labels", "for", "the", "type", "and", "target", "of", "hate", ".", "Hateful", "entries", "make", "up", "54", "%", "of", "the", "dataset", ",", "which", "is", "substantially", "higher", "than", "comparable", "datasets", ".", "We", "show", "that", "model", "performance", "is", "substantially", "improved", "using", "this", "approach", ".", "Models", "trained", "on", "later", "rounds", "of", "data", "collection", "perform", "better", "on", "test", "sets", "and", "are", "harder", "for", "annotators", "to", "trick", ".", "They", "also", "have", "better", "performance", "on", "HateCheck", ",", "a", "suite", "of", "functional", "tests", "for", "online", "hate", "detection", ".", "We", "provide", "the", "code", ",", "dataset", "and", "annotation", "guidelines", "for", "other", "researchers", "to", "use", "."], "entities": [{"type": "Operation", "start": 3, "end": 5, "text": "human-and-model-in-the-loop process", "sent_idx": 0}, {"type": "Effect", "start": 85, "end": 86, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--132"}
{"text": "Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks", "tokens": ["Compared", "to", "the", "general", "news", "domain", ",", "information", "extraction", "(", "IE", ")", "from", "biomedical", "text", "requires", "much", "broader", "domain", "knowledge", ".", "However", ",", "many", "previous", "IE", "methods", "do", "not", "utilize", "any", "external", "knowledge", "during", "inference", ".", "Due", "to", "the", "exponential", "growth", "of", "biomedical", "publications", ",", "models", "that", "do", "not", "go", "beyond", "their", "fixed", "set", "of", "parameters", "will", "likely", "fall", "behind", ".", "Inspired", "by", "how", "humans", "look", "up", "relevant", "information", "to", "comprehend", "a", "scientific", "text", ",", "we", "present", "a", "novel", "framework", "that", "utilizes", "external", "knowledge", "for", "joint", "entity", "and", "relation", "extraction", "named", "KECI", "(", "Knowledge-Enhanced", "Collective", "Inference", ")", ".", "Given", "an", "input", "text", ",", "KECI", "first", "constructs", "an", "initial", "span", "graph", "representing", "its", "initial", "understanding", "of", "the", "text", ".", "It", "then", "uses", "an", "entity", "linker", "to", "form", "a", "knowledge", "graph", "containing", "relevant", "background", "knowledge", "for", "the", "the", "entity", "mentions", "in", "the", "text", ".", "To", "make", "the", "final", "predictions", ",", "KECI", "fuses", "the", "initial", "span", "graph", "and", "the", "knowledge", "graph", "into", "a", "more", "refined", "graph", "using", "an", "attention", "mechanism", ".", "KECI", "takes", "a", "collective", "approach", "to", "link", "mention", "spans", "to", "entities", "by", "integrating", "global", "relational", "information", "into", "local", "representations", "using", "graph", "convolutional", "networks", ".", "Our", "experimental", "results", "show", "that", "the", "framework", "is", "highly", "effective", ",", "achieving", "new", "state-of-the-art", "results", "in", "two", "different", "benchmark", "datasets", ":", "BioRelEx", "(", "binding", "interaction", "detection", ")", "and", "ADE", "(", "adverse", "drug", "event", "extraction", ")", ".", "For", "example", ",", "KECI", "achieves", "absolute", "improvements", "of", "4.59", "%", "and", "4.91", "%", "in", "F1", "scores", "over", "the", "state-of-the-art", "on", "the", "BioRelEx", "entity", "and", "relation", "extraction", "tasks"], "entities": [{"type": "Operation", "start": 231, "end": 232, "text": "KECI", "sent_idx": 9}, {"type": "Effect", "start": 242, "end": 244, "text": "F1 scores", "sent_idx": 9}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--488"}
{"text": "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input  sequence  to abstract; and (2) where in the output  graph  to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).", "tokens": ["We", "propose", "a", "new", "end-to-end", "model", "that", "treats", "AMR", "parsing", "as", "a", "series", "of", "dual", "decisions", "on", "the", "input", "sequence", "and", "the", "incrementally", "constructed", "graph", ".", "At", "each", "time", "step", ",", "our", "model", "performs", "multiple", "rounds", "of", "attention", ",", "reasoning", ",", "and", "composition", "that", "aim", "to", "answer", "two", "critical", "questions", ":", "(", "1", ")", "which", "part", "of", "the", "input", " ", "sequence", " ", "to", "abstract", ";", "and", "(", "2", ")", "where", "in", "the", "output", " ", "graph", " ", "to", "construct", "the", "new", "concept", ".", "We", "show", "that", "the", "answers", "to", "these", "two", "questions", "are", "mutually", "causalities", ".", "We", "design", "a", "model", "based", "on", "iterative", "inference", "that", "helps", "achieve", "better", "answers", "in", "both", "perspectives", ",", "leading", "to", "greatly", "improved", "parsing", "accuracy", ".", "Our", "experimental", "results", "significantly", "outperform", "all", "previously", "reported", "Smatch", "scores", "by", "large", "margins", ".", "Remarkably", ",", "without", "the", "help", "of", "any", "large-scale", "pre-trained", "language", "model", "(", "e.g.", ",", "BERT", ")", ",", "our", "model", "already", "surpasses", "previous", "state-of-the-art", "using", "BERT", ".", "With", "the", "help", "of", "BERT", ",", "we", "can", "push", "the", "state-of-the-art", "results", "to", "80.2", "%", "on", "LDC2017T10", "(", "AMR", "2.0", ")", "and", "75.4", "%", "on", "LDC2014T12", "(", "AMR", "1.0", ")", "."], "entities": [{"type": "Operation", "start": 97, "end": 103, "text": "a model based on iterative inference", "sent_idx": 3}, {"type": "Effect", "start": 117, "end": 118, "text": "accuracy", "sent_idx": 3}, {"type": "Operation", "start": 4, "end": 6, "text": "end-to-end model", "sent_idx": 0}, {"type": "Effect", "start": 127, "end": 129, "text": "Smatch scores", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--119"}
{"text": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.", "tokens": ["While", "it", "is", "generally", "accepted", "that", "many", "translation", "phenomena", "are", "correlated", "with", "linguistic", "structures", ",", "employing", "linguistic", "syntax", "for", "translation", "has", "proven", "a", "highly", "non-trivial", "task", ".", "The", "key", "assumption", "behind", "many", "approaches", "is", "that", "translation", "is", "guided", "by", "the", "source", "and/or", "target", "language", "parse", ",", "employing", "rules", "extracted", "from", "the", "parse", "tree", "or", "performing", "tree", "transformations", ".", "These", "approaches", "enforce", "strict", "constraints", "and", "might", "overlook", "important", "translation", "phenomena", "that", "cross", "linguistic", "constituents", ".", "We", "propose", "a", "novel", "flexible", "modelling", "approach", "to", "introduce", "linguistic", "information", "of", "varying", "granularity", "from", "the", "source", "side", ".", "Our", "method", "induces", "joint", "probability", "synchronous", "grammars", "and", "estimates", "their", "parameters", ",", "by", "selecting", "and", "weighing", "together", "linguistically", "motivated", "rules", "according", "to", "an", "objective", "function", "directly", "targeting", "generalisation", "over", "future", "data", ".", "We", "obtain", "statistically", "significant", "improvements", "across", "4", "different", "language", "pairs", "with", "English", "as", "source", ",", "mounting", "up", "to", "+", "1.92", "BLEU", "for", "Chinese", "as", "target", "."], "entities": [{"type": "Operation", "start": 82, "end": 92, "text": "introduce linguistic information of varying granularity from the source side", "sent_idx": 3}, {"type": "Effect", "start": 145, "end": 146, "text": "BLEU", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1065"}
{"text": "Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.", "tokens": ["Deployed", "real-world", "machine", "learning", "applications", "are", "often", "subject", "to", "uncontrolled", "and", "even", "potentially", "malicious", "inputs", ".", "Those", "out-of-domain", "inputs", "can", "lead", "to", "unpredictable", "outputs", "and", "sometimes", "catastrophic", "safety", "issues", ".", "Prior", "studies", "on", "out-of-domain", "detection", "require", "in-domain", "task", "labels", "and", "are", "limited", "to", "supervised", "classification", "scenarios", ".", "Our", "work", "tackles", "the", "problem", "of", "detecting", "out-of-domain", "samples", "with", "only", "unsupervised", "in-domain", "data", ".", "We", "utilize", "the", "latent", "representations", "of", "pre-trained", "transformers", "and", "propose", "a", "simple", "yet", "effective", "method", "to", "transform", "features", "across", "all", "layers", "to", "construct", "out-of-domain", "detectors", "efficiently", ".", "Two", "domain-specific", "fine-tuning", "approaches", "are", "further", "proposed", "to", "boost", "detection", "accuracy", ".", "Our", "empirical", "evaluations", "of", "related", "methods", "on", "two", "datasets", "validate", "that", "our", "method", "greatly", "improves", "out-of-domain", "detection", "ability", "in", "a", "more", "general", "scenario", "."], "entities": [{"type": "Operation", "start": 90, "end": 93, "text": "domain-specific fine-tuning approaches", "sent_idx": 5}, {"type": "Effect", "start": 98, "end": 100, "text": "detection accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--85"}
{"text": "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS.", "tokens": ["Multilingual", "neural", "machine", "translation", "aims", "at", "learning", "a", "single", "translation", "model", "for", "multiple", "languages", ".", "These", "jointly", "trained", "models", "often", "suffer", "from", "performance", "degradationon", "rich-resource", "language", "pairs", ".", "We", "attribute", "this", "degeneration", "to", "parameter", "interference", ".", "In", "this", "paper", ",", "we", "propose", "LaSS", "to", "jointly", "train", "a", "single", "unified", "multilingual", "MT", "model", ".", "LaSS", "learns", "Language", "Specific", "Sub-network", "(", "LaSS", ")", "for", "each", "language", "pair", "to", "counter", "parameter", "interference", ".", "Comprehensive", "experiments", "on", "IWSLT", "and", "WMT", "datasets", "with", "various", "Transformer", "architectures", "show", "that", "LaSS", "obtains", "gains", "on", "36", "language", "pairs", "by", "up", "to", "1.2", "BLEU", ".", "Besides", ",", "LaSS", "shows", "its", "strong", "generalization", "performance", "at", "easy", "adaptation", "to", "new", "language", "pairs", "and", "zero-shot", "translation", ".", "LaSS", "boosts", "zero-shot", "translation", "with", "an", "average", "of", "8.3", "BLEU", "on", "30", "language", "pairs", ".", "Codes", "and", "trained", "models", "are", "available", "at", "https://github.com/NLP-Playground/LaSS", "."], "entities": [{"type": "Operation", "start": 115, "end": 116, "text": "LaSS", "sent_idx": 7}, {"type": "Effect", "start": 124, "end": 125, "text": "BLEU", "sent_idx": 7}, {"type": "Operation", "start": 98, "end": 99, "text": "LaSS", "sent_idx": 6}, {"type": "Effect", "start": 102, "end": 104, "text": "generalization performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--25"}
{"text": "Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.", "tokens": ["Pre-trained", "language", "models", "have", "achieved", "huge", "improvement", "on", "many", "NLP", "tasks", ".", "However", ",", "these", "methods", "are", "usually", "designed", "for", "written", "text", ",", "so", "they", "do", "not", "consider", "the", "properties", "of", "spoken", "language", ".", "Therefore", ",", "this", "paper", "aims", "at", "generalizing", "the", "idea", "of", "language", "model", "pre-training", "to", "lattices", "generated", "by", "recognition", "systems", ".", "We", "propose", "a", "framework", "that", "trains", "neural", "lattice", "language", "models", "to", "provide", "contextualized", "representations", "for", "spoken", "language", "understanding", "tasks", ".", "The", "proposed", "two-stage", "pre-training", "approach", "reduces", "the", "demands", "of", "speech", "data", "and", "has", "better", "efficiency", ".", "Experiments", "on", "intent", "detection", "and", "dialogue", "act", "recognition", "datasets", "demonstrate", "that", "our", "proposed", "method", "consistently", "outperforms", "strong", "baselines", "when", "evaluated", "on", "spoken", "inputs", ".", "The", "code", "is", "available", "at", "https://github.com/MiuLab/Lattice-ELMo", "."], "entities": [{"type": "Operation", "start": 76, "end": 79, "text": "two-stage pre-training approach", "sent_idx": 4}, {"type": "Effect", "start": 81, "end": 85, "text": "demands of speech data", "sent_idx": 4}, {"type": "Effect", "start": 88, "end": 89, "text": "efficiency", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--347"}
{"text": "We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.", "tokens": ["We", "propose", "the", "task", "of", "unsupervised", "morphological", "paradigm", "completion", ".", "Given", "only", "raw", "text", "and", "a", "lemma", "list", ",", "the", "task", "consists", "of", "generating", "the", "morphological", "paradigms", ",", "i.e.", ",", "all", "inflected", "forms", ",", "of", "the", "lemmas", ".", "From", "a", "natural", "language", "processing", "(", "NLP", ")", "perspective", ",", "this", "is", "a", "challenging", "unsupervised", "task", ",", "and", "high-performing", "systems", "have", "the", "potential", "to", "improve", "tools", "for", "low-resource", "languages", "or", "to", "assist", "linguistic", "annotators", ".", "From", "a", "cognitive", "science", "perspective", ",", "this", "can", "shed", "light", "on", "how", "children", "acquire", "morphological", "knowledge", ".", "We", "further", "introduce", "a", "system", "for", "the", "task", ",", "which", "generates", "morphological", "paradigms", "via", "the", "following", "steps", ":", "(", "i", ")", "EDIT", "TREE", "retrieval", ",", "(", "ii", ")", "additional", "lemma", "retrieval", ",", "(", "iii", ")", "paradigm", "size", "discovery", ",", "and", "(", "iv", ")", "inflection", "generation", ".", "We", "perform", "an", "evaluation", "on", "14", "typologically", "diverse", "languages", ".", "Our", "system", "outperforms", "trivial", "baselines", "with", "ease", "and", ",", "for", "some", "languages", ",", "even", "obtains", "a", "higher", "accuracy", "than", "minimally", "supervised", "systems", "."], "entities": [{"type": "Operation", "start": 146, "end": 148, "text": "Our system", "sent_idx": 6}, {"type": "Effect", "start": 163, "end": 164, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--598"}
{"text": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.", "tokens": ["Personalized", "news", "recommendation", "methods", "are", "widely", "used", "in", "online", "news", "services", ".", "These", "methods", "usually", "recommend", "news", "based", "on", "the", "matching", "between", "news", "content", "and", "user", "interest", "inferred", "from", "historical", "behaviors", ".", "However", ",", "these", "methods", "usually", "have", "difficulties", "in", "making", "accurate", "recommendations", "to", "cold-start", "users", ",", "and", "tend", "to", "recommend", "similar", "news", "with", "those", "users", "have", "read", ".", "In", "general", ",", "popular", "news", "usually", "contain", "important", "information", "and", "can", "attract", "users", "with", "different", "interests", ".", "Besides", ",", "they", "are", "usually", "diverse", "in", "content", "and", "topic", ".", "Thus", ",", "in", "this", "paper", "we", "propose", "to", "incorporate", "news", "popularity", "information", "to", "alleviate", "the", "cold-start", "and", "diversity", "problems", "for", "personalized", "news", "recommendation", ".", "In", "our", "method", ",", "the", "ranking", "score", "for", "recommending", "a", "candidate", "news", "to", "a", "target", "user", "is", "the", "combination", "of", "a", "personalized", "matching", "score", "and", "a", "news", "popularity", "score", ".", "The", "former", "is", "used", "to", "capture", "the", "personalized", "user", "interest", "in", "news", ".", "The", "latter", "is", "used", "to", "measure", "time-aware", "popularity", "of", "candidate", "news", ",", "which", "is", "predicted", "based", "on", "news", "content", ",", "recency", ",", "and", "real-time", "CTR", "using", "a", "unified", "framework", ".", "Besides", ",", "we", "propose", "a", "popularity-aware", "user", "encoder", "to", "eliminate", "the", "popularity", "bias", "in", "user", "behaviors", "for", "accurate", "interest", "modeling", ".", "Experiments", "on", "two", "real-world", "datasets", "show", "our", "method", "can", "effectively", "improve", "the", "accuracy", "and", "diversity", "for", "news", "recommendation", "."], "entities": [{"type": "Operation", "start": 189, "end": 192, "text": "popularity-aware user encoder", "sent_idx": 9}, {"type": "Effect", "start": 217, "end": 218, "text": "accuracy", "sent_idx": 10}, {"type": "Operation", "start": 95, "end": 99, "text": "incorporate news popularity information", "sent_idx": 5}, {"type": "Effect", "start": 219, "end": 220, "text": "diversity", "sent_idx": 10}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--424"}
{"text": "This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the BLEU score and \"readability\" when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.", "tokens": ["This", "paper", "presents", "an", "attempt", "at", "building", "a", "large", "scale", "distributed", "composite", "language", "model", "that", "simultaneously", "accounts", "for", "local", "word", "lexical", "information", ",", "mid-range", "sentence", "syntactic", "structure", ",", "and", "long-span", "document", "semantic", "content", "under", "a", "directed", "Markov", "random", "field", "paradigm", ".", "The", "composite", "language", "model", "has", "been", "trained", "by", "performing", "a", "convergent", "N-best", "list", "approximate", "EM", "algorithm", "that", "has", "linear", "time", "complexity", "and", "a", "follow-up", "EM", "algorithm", "to", "improve", "word", "prediction", "power", "on", "corpora", "with", "up", "to", "a", "billion", "tokens", "and", "stored", "on", "a", "supercomputer", ".", "The", "large", "scale", "distributed", "composite", "language", "model", "gives", "drastic", "perplexity", "reduction", "over", "n-grams", "and", "achieves", "significantly", "better", "translation", "quality", "measured", "by", "the", "BLEU", "score", "and", "\"", "readability", "\"", "when", "applied", "to", "the", "task", "of", "re-ranking", "the", "N-best", "list", "from", "a", "state-of-the-art", "parsing-based", "machine", "translation", "system", "."], "entities": [{"type": "Operation", "start": 87, "end": 93, "text": "large scale distributed composite language model", "sent_idx": 2}, {"type": "Effect", "start": 94, "end": 96, "text": "drastic perplexity", "sent_idx": 2}, {"type": "Effect", "start": 103, "end": 105, "text": "translation quality", "sent_idx": 2}, {"type": "Effect", "start": 108, "end": 110, "text": "BLEU score", "sent_idx": 2}, {"type": "Effect", "start": 112, "end": 113, "text": "readability", "sent_idx": 2}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 0, "tail": 4}], "id": "P11-1021"}
{"text": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.", "tokens": ["Most", "existing", "joint", "neural", "models", "for", "Information", "Extraction", "(", "IE", ")", "use", "local", "task-specific", "classifiers", "to", "predict", "labels", "for", "individual", "instances", "(", "e.g.", ",", "trigger", ",", "relation", ")", "regardless", "of", "their", "interactions", ".", "For", "example", ",", "a", "victim", "of", "a", "die", "event", "is", "likely", "to", "be", "a", "victim", "of", "an", "attack", "event", "in", "the", "same", "sentence", ".", "In", "order", "to", "capture", "such", "cross-subtask", "and", "cross-instance", "inter-dependencies", ",", "we", "propose", "a", "joint", "neural", "framework", ",", "OneIE", ",", "that", "aims", "to", "extract", "the", "globally", "optimal", "IE", "result", "as", "a", "graph", "from", "an", "input", "sentence", ".", "OneIE", "performs", "end-to-end", "IE", "in", "four", "stages", ":", "(", "1", ")", "Encoding", "a", "given", "sentence", "as", "contextualized", "word", "representations", ";", "(", "2", ")", "Identifying", "entity", "mentions", "and", "event", "triggers", "as", "nodes", ";", "(", "3", ")", "Computing", "label", "scores", "for", "all", "nodes", "and", "their", "pairwise", "links", "using", "local", "classifiers", ";", "(", "4", ")", "Searching", "for", "the", "globally", "optimal", "graph", "with", "a", "beam", "decoder", ".", "At", "the", "decoding", "stage", ",", "we", "incorporate", "global", "features", "to", "capture", "the", "cross-subtask", "and", "cross-instance", "interactions", ".", "Experiments", "show", "that", "adding", "global", "features", "improves", "the", "performance", "of", "our", "model", "and", "achieves", "new", "state", "of-the-art", "on", "all", "subtasks", ".", "In", "addition", ",", "as", "OneIE", "does", "not", "use", "any", "language-specific", "feature", ",", "we", "prove", "it", "can", "be", "easily", "applied", "to", "new", "languages", "or", "trained", "in", "a", "multilingual", "manner", "."], "entities": [{"type": "Operation", "start": 176, "end": 179, "text": "adding global features", "sent_idx": 5}, {"type": "Effect", "start": 181, "end": 182, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--713"}
{"text": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the S\u00f8rensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.", "tokens": ["Many", "NLP", "tasks", "such", "as", "tagging", "and", "machine", "reading", "comprehension", "are", "faced", "with", "the", "severe", "data", "imbalance", "issue", ":", "negative", "examples", "significantly", "outnumber", "positive", "examples", ",", "and", "the", "huge", "number", "of", "easy-negative", "examples", "overwhelms", "the", "training", ".", "The", "most", "commonly", "used", "cross", "entropy", "(", "CE", ")", "criteria", "is", "actually", "an", "accuracy-oriented", "objective", ",", "and", "thus", "creates", "a", "discrepancy", "between", "training", "and", "test", ":", "at", "training", "time", ",", "each", "training", "instance", "contributes", "equally", "to", "the", "objective", "function", ",", "while", "at", "test", "time", "F1", "score", "concerns", "more", "about", "positive", "examples", ".", "In", "this", "paper", ",", "we", "propose", "to", "use", "dice", "loss", "in", "replacement", "of", "the", "standard", "cross-entropy", "objective", "for", "data-imbalanced", "NLP", "tasks", ".", "Dice", "loss", "is", "based", "on", "the", "S\u00f8rensen", "--", "Dice", "coefficient", "or", "Tversky", "index", ",", "which", "attaches", "similar", "importance", "to", "false", "positives", "and", "false", "negatives", ",", "and", "is", "more", "immune", "to", "the", "data-imbalance", "issue", ".", "To", "further", "alleviate", "the", "dominating", "influence", "from", "easy-negative", "examples", "in", "training", ",", "we", "propose", "to", "associate", "training", "examples", "with", "dynamically", "adjusted", "weights", "to", "deemphasize", "easy-negative", "examples", ".", "Theoretical", "analysis", "shows", "that", "this", "strategy", "narrows", "down", "the", "gap", "between", "the", "F1", "score", "in", "evaluation", "and", "the", "dice", "loss", "in", "training", ".", "With", "the", "proposed", "training", "objective", ",", "we", "observe", "significant", "performance", "boost", "on", "a", "wide", "range", "of", "data", "imbalanced", "NLP", "tasks", ".", "Notably", ",", "we", "are", "able", "to", "achieve", "SOTA", "results", "on", "CTB5", ",", "CTB6", "and", "UD1.4", "for", "the", "part", "of", "speech", "tagging", "task", ";", "SOTA", "results", "on", "CoNLL03", ",", "OntoNotes5.0", ",", "MSRA", "and", "OntoNotes4.0", "for", "the", "named", "entity", "recognition", "task", ";", "along", "with", "competitive", "results", "on", "the", "tasks", "of", "machine", "reading", "comprehension", "and", "paraphrase", "identification", "."], "entities": [{"type": "Operation", "start": 96, "end": 99, "text": "use dice loss", "sent_idx": 2}, {"type": "Effect", "start": 204, "end": 205, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 198, "end": 200, "text": "training objective", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--45"}
{"text": "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.", "tokens": ["A", "few", "approaches", "have", "been", "developed", "to", "improve", "neural", "machine", "translation", "(", "NMT", ")", "models", "with", "multiple", "passes", "of", "decoding", ".", "However", ",", "their", "performance", "gains", "are", "limited", "because", "of", "lacking", "proper", "policies", "to", "terminate", "the", "multi-pass", "process", ".", "To", "address", "this", "issue", ",", "we", "introduce", "a", "novel", "architecture", "of", "Rewriter-Evaluator", ".", "Translating", "a", "source", "sentence", "involves", "multiple", "rewriting", "passes", ".", "In", "every", "pass", ",", "a", "rewriter", "generates", "a", "new", "translation", "to", "improve", "the", "past", "translation", ".", "Termination", "of", "this", "multi-pass", "process", "is", "determined", "by", "a", "score", "of", "translation", "quality", "estimated", "by", "an", "evaluator", ".", "We", "also", "propose", "prioritized", "gradient", "descent", "(", "PGD", ")", "to", "jointly", "and", "efficiently", "train", "the", "rewriter", "and", "the", "evaluator", ".", "Extensive", "experiments", "on", "three", "machine", "translation", "tasks", "show", "that", "our", "architecture", "notably", "improves", "the", "performances", "of", "NMT", "models", "and", "significantly", "outperforms", "prior", "methods", ".", "An", "oracle", "experiment", "reveals", "that", "it", "can", "largely", "reduce", "performance", "gaps", "to", "the", "oracle", "policy", ".", "Experiments", "confirm", "that", "the", "evaluator", "trained", "with", "PGD", "is", "more", "accurate", "than", "prior", "methods", "in", "determining", "proper", "numbers", "of", "rewriting", "."], "entities": [{"type": "Operation", "start": 159, "end": 163, "text": "evaluator trained with PGD", "sent_idx": 9}, {"type": "Effect", "start": 172, "end": 175, "text": "numbers of rewriting", "sent_idx": 9}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--443"}
{"text": "The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory. It motivates us to formulate the quest of vocabularization \u2013 finding the best token dictionary with a proper size \u2013 as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT.", "tokens": ["The", "choice", "of", "token", "vocabulary", "affects", "the", "performance", "of", "machine", "translation", ".", "This", "paper", "aims", "to", "figure", "out", "what", "is", "a", "good", "vocabulary", "and", "whether", "we", "can", "find", "the", "optimal", "vocabulary", "without", "trial", "training", ".", "To", "answer", "these", "questions", ",", "we", "first", "provide", "an", "alternative", "understanding", "of", "vocabulary", "from", "the", "perspective", "of", "information", "theory", ".", "It", "motivates", "us", "to", "formulate", "the", "quest", "of", "vocabularization", "\u2013", "finding", "the", "best", "token", "dictionary", "with", "a", "proper", "size", "\u2013", "as", "an", "optimal", "transport", "(", "OT", ")", "problem", ".", "We", "propose", "VOLT", ",", "a", "simple", "and", "efficient", "solution", "without", "trial", "training", ".", "Empirical", "results", "show", "that", "VOLT", "beats", "widely-used", "vocabularies", "in", "diverse", "scenarios", ",", "including", "WMT-14", "English-German", "translation", ",", "TED", "bilingual", "translation", ",", "and", "TED", "multilingual", "translation", ".", "For", "example", ",", "VOLT", "achieves", "70", "%", "vocabulary", "size", "reduction", "and", "0.5", "BLEU", "gain", "on", "English-German", "translation", ".", "Also", ",", "compared", "to", "BPE-search", ",", "VOLT", "reduces", "the", "search", "time", "from", "384", "GPU", "hours", "to", "30", "GPU", "hours", "on", "English-German", "translation", ".", "Codes", "are", "available", "at", "https://github.com/Jingjing-NLP/VOLT", "."], "entities": [{"type": "Operation", "start": 126, "end": 127, "text": "VOLT", "sent_idx": 6}, {"type": "Effect", "start": 130, "end": 132, "text": "vocabulary size", "sent_idx": 6}, {"type": "Effect", "start": 135, "end": 136, "text": "BLEU", "sent_idx": 6}, {"type": "Operation", "start": 147, "end": 148, "text": "VOLT", "sent_idx": 7}, {"type": "Effect", "start": 150, "end": 152, "text": "search time", "sent_idx": 7}, {"type": "Effect", "start": 154, "end": 156, "text": "GPU hours", "sent_idx": 7}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Neg_Affect", "head": 3, "tail": 4}, {"type": "Neg_Affect", "head": 3, "tail": 5}], "id": "abstract-2021--acl-long--571"}
{"text": "This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental results on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.", "tokens": ["This", "paper", "presents", "a", "framework", "for", "automatically", "processing", "information", "coming", "from", "community", "Question", "Answering", "(", "cQA", ")", "portals", "with", "the", "purpose", "of", "generating", "a", "trustful", ",", "complete", ",", "relevant", "and", "succinct", "summary", "in", "response", "to", "a", "question", ".", "We", "exploit", "the", "metadata", "intrinsically", "present", "in", "User", "Generated", "Content", "(", "UGC", ")", "to", "bias", "automatic", "multi-document", "summarization", "techniques", "toward", "high", "quality", "information", ".", "We", "adopt", "a", "representation", "of", "concepts", "alternative", "to", "n-grams", "and", "propose", "two", "concept-scoring", "functions", "based", "on", "semantic", "overlap", ".", "Experimental", "results", "on", "data", "drawn", "from", "Yahoo", "!", "Answers", "demonstrate", "the", "effectiveness", "of", "our", "method", "in", "terms", "of", "ROUGE", "scores", ".", "We", "show", "that", "the", "information", "contained", "in", "the", "best", "answers", "voted", "by", "users", "of", "cQA", "portals", "can", "be", "successfully", "complemented", "by", "our", "method", "."], "entities": [{"type": "Operation", "start": 94, "end": 96, "text": "our method", "sent_idx": 3}, {"type": "Effect", "start": 99, "end": 101, "text": "ROUGE scores", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1078"}
{"text": "Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta.", "tokens": ["Even", "though", "BERT", "has", "achieved", "successful", "performance", "improvements", "in", "various", "supervised", "learning", "tasks", ",", "BERT", "is", "still", "limited", "by", "repetitive", "inferences", "on", "unsupervised", "tasks", "for", "the", "computation", "of", "contextual", "language", "representations", ".", "To", "resolve", "this", "limitation", ",", "we", "propose", "a", "novel", "deep", "bidirectional", "language", "model", "called", "a", "Transformer-based", "Text", "Autoencoder", "(", "T-TA", ")", ".", "The", "T-TA", "computes", "contextual", "language", "representations", "without", "repetition", "and", "displays", "the", "benefits", "of", "a", "deep", "bidirectional", "architecture", ",", "such", "as", "that", "of", "BERT", ".", "In", "computation", "time", "experiments", "in", "a", "CPU", "environment", ",", "the", "proposed", "T-TA", "performs", "over", "six", "times", "faster", "than", "the", "BERT-like", "model", "on", "a", "reranking", "task", "and", "twelve", "times", "faster", "on", "a", "semantic", "similarity", "task", ".", "Furthermore", ",", "the", "T-TA", "shows", "competitive", "or", "even", "better", "accuracies", "than", "those", "of", "BERT", "on", "the", "above", "tasks", ".", "Code", "is", "available", "at", "https://github.com/joongbo/tta", "."], "entities": [{"type": "Operation", "start": 116, "end": 117, "text": "T-TA", "sent_idx": 4}, {"type": "Effect", "start": 122, "end": 123, "text": "accuracies", "sent_idx": 4}, {"type": "Operation", "start": 47, "end": 53, "text": "Transformer-based Text Autoencoder (T-TA)", "sent_idx": 1}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--76"}
{"text": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an  n -gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with  5+  times speed up compared with an autoregressive model.", "tokens": ["The", "masked", "language", "model", "has", "received", "remarkable", "attention", "due", "to", "its", "effectiveness", "on", "various", "natural", "language", "processing", "tasks", ".", "However", ",", "few", "works", "have", "adopted", "this", "technique", "in", "the", "sequence-to-sequence", "models", ".", "In", "this", "work", ",", "we", "introduce", "a", "jointly", "masked", "sequence-to-sequence", "model", "and", "explore", "its", "application", "on", "non-autoregressive", "neural", "machine", "translation~(NAT", ")", ".", "Specifically", ",", "we", "first", "empirically", "study", "the", "functionalities", "of", "the", "encoder", "and", "the", "decoder", "in", "NAT", "models", ",", "and", "find", "that", "the", "encoder", "takes", "a", "more", "important", "role", "than", "the", "decoder", "regarding", "the", "translation", "quality", ".", "Therefore", ",", "we", "propose", "to", "train", "the", "encoder", "more", "rigorously", "by", "masking", "the", "encoder", "input", "while", "training", ".", "As", "for", "the", "decoder", ",", "we", "propose", "to", "train", "it", "based", "on", "the", "consecutive", "masking", "of", "the", "decoder", "input", "with", "an", " ", "n", "-gram", "loss", "function", "to", "alleviate", "the", "problem", "of", "translating", "duplicate", "words", ".", "The", "two", "types", "of", "masks", "are", "applied", "to", "the", "model", "jointly", "at", "the", "training", "stage", ".", "We", "conduct", "experiments", "on", "five", "benchmark", "machine", "translation", "tasks", ",", "and", "our", "model", "can", "achieve", "27.69/32.24", "BLEU", "scores", "on", "WMT14", "English-German/German-English", "tasks", "with", " ", "5", "+", " ", "times", "speed", "up", "compared", "with", "an", "autoregressive", "model", "."], "entities": [{"type": "Operation", "start": 39, "end": 43, "text": "jointly masked sequence-to-sequence model", "sent_idx": 2}, {"type": "Effect", "start": 175, "end": 177, "text": "BLEU scores", "sent_idx": 7}, {"type": "Effect", "start": 187, "end": 188, "text": "speed", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--36"}
{"text": "Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.", "tokens": ["Learning", "disentangled", "representations", "of", "natural", "language", "is", "essential", "for", "many", "NLP", "tasks", ",", "e.g.", ",", "conditional", "text", "generation", ",", "style", "transfer", ",", "personalized", "dialogue", "systems", ",", "etc", ".", "Similar", "problems", "have", "been", "studied", "extensively", "for", "other", "forms", "of", "data", ",", "such", "as", "images", "and", "videos", ".", "However", ",", "the", "discrete", "nature", "of", "natural", "language", "makes", "the", "disentangling", "of", "textual", "representations", "more", "challenging", "(", "e.g.", ",", "the", "manipulation", "over", "the", "data", "space", "can", "not", "be", "easily", "achieved", ")", ".", "Inspired", "by", "information", "theory", ",", "we", "propose", "a", "novel", "method", "that", "effectively", "manifests", "disentangled", "representations", "of", "text", ",", "without", "any", "supervision", "on", "semantics", ".", "A", "new", "mutual", "information", "upper", "bound", "is", "derived", "and", "leveraged", "to", "measure", "dependence", "between", "style", "and", "content", ".", "By", "minimizing", "this", "upper", "bound", ",", "the", "proposed", "method", "induces", "style", "and", "content", "embeddings", "into", "two", "independent", "low-dimensional", "spaces", ".", "Experiments", "on", "both", "conditional", "text", "generation", "and", "text-style", "transfer", "demonstrate", "the", "high", "quality", "of", "our", "disentangled", "representation", "in", "terms", "of", "content", "and", "style", "preservation", "."], "entities": [{"type": "Operation", "start": 155, "end": 157, "text": "disentangled representation", "sent_idx": 6}, {"type": "Effect", "start": 152, "end": 153, "text": "quality", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--673"}
{"text": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.", "tokens": ["We", "present", "a", "simple", "but", "accurate", "parser", "which", "exploits", "both", "large", "tree", "fragments", "and", "symbol", "refinement", ".", "We", "parse", "with", "all", "fragments", "of", "the", "training", "set", ",", "in", "contrast", "to", "much", "recent", "work", "on", "tree", "selection", "in", "data-oriented", "parsing", "and", "tree-substitution", "grammar", "learning", ".", "We", "require", "only", "simple", ",", "deterministic", "grammar", "symbol", "refinement", ",", "in", "contrast", "to", "recent", "work", "on", "latent", "symbol", "refinement", ".", "Moreover", ",", "our", "parser", "requires", "no", "explicit", "lexicon", "machinery", ",", "instead", "parsing", "input", "sentences", "as", "character", "streams", ".", "Despite", "its", "simplicity", ",", "our", "parser", "achieves", "accuracies", "of", "over", "88", "%", "F1", "on", "the", "standard", "English", "WSJ", "task", ",", "which", "is", "competitive", "with", "substantially", "more", "complicated", "state-of-the-art", "lexicalized", "and", "latent-variable", "parsers", ".", "Additional", "specific", "contributions", "center", "on", "making", "implicit", "all-fragments", "parsing", "efficient", ",", "including", "a", "coarse-to-fine", "inference", "scheme", "and", "a", "new", "graph", "encoding", "."], "entities": [{"type": "Operation", "start": 8, "end": 16, "text": "exploits both large tree fragments and symbol refinement", "sent_idx": 0}, {"type": "Effect", "start": 89, "end": 90, "text": "accuracies", "sent_idx": 4}, {"type": "Effect", "start": 94, "end": 95, "text": "F1", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "P10-1112"}
{"text": "Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.", "tokens": ["Most", "supervised", "language", "processing", "systems", "show", "a", "significant", "drop-off", "in", "performance", "when", "they", "are", "tested", "on", "text", "that", "comes", "from", "a", "domain", "significantly", "different", "from", "the", "domain", "of", "the", "training", "data", ".", "Semantic", "role", "labeling", "techniques", "are", "typically", "trained", "on", "newswire", "text", ",", "and", "in", "tests", "their", "performance", "on", "fiction", "is", "as", "much", "as", "19", "%", "worse", "than", "their", "performance", "on", "newswire", "text", ".", "We", "investigate", "techniques", "for", "building", "open-domain", "semantic", "role", "labeling", "systems", "that", "approach", "the", "ideal", "of", "a", "train-once", ",", "use-anywhere", "system", ".", "We", "leverage", "recently-developed", "techniques", "for", "learning", "representations", "of", "text", "using", "latent-variable", "language", "models", ",", "and", "extend", "these", "techniques", "to", "ones", "that", "provide", "the", "kinds", "of", "features", "that", "are", "useful", "for", "semantic", "role", "labeling", ".", "In", "experiments", ",", "our", "novel", "system", "reduces", "error", "by", "16", "%", "relative", "to", "the", "previous", "state", "of", "the", "art", "on", "out-of-domain", "text", "."], "entities": [{"type": "Operation", "start": 86, "end": 89, "text": "leverage recently-developed techniques", "sent_idx": 3}, {"type": "Effect", "start": 126, "end": 127, "text": "error", "sent_idx": 4}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "P10-1099"}
{"text": "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline", "tokens": ["Existing", "multilingual", "machine", "translation", "approaches", "mainly", "focus", "on", "English-centric", "directions", ",", "while", "the", "non-English", "directions", "still", "lag", "behind", ".", "In", "this", "work", ",", "we", "aim", "to", "build", "a", "many-to-many", "translation", "system", "with", "an", "emphasis", "on", "the", "quality", "of", "non-English", "language", "directions", ".", "Our", "intuition", "is", "based", "on", "the", "hypothesis", "that", "a", "universal", "cross-language", "representation", "leads", "to", "better", "multilingual", "translation", "performance", ".", "To", "this", "end", ",", "we", "propose", "mRASP2", ",", "a", "training", "method", "to", "obtain", "a", "single", "unified", "multilingual", "translation", "model", ".", "mRASP2", "is", "empowered", "by", "two", "techniques", ":", "a", ")", "a", "contrastive", "learning", "scheme", "to", "close", "the", "gap", "among", "representations", "of", "different", "languages", ",", "and", "b", ")", "data", "augmentation", "on", "both", "multiple", "parallel", "and", "monolingual", "data", "to", "further", "align", "token", "representations", ".", "For", "English-centric", "directions", ",", "mRASP2", "achieves", "competitive", "or", "even", "better", "performance", "than", "a", "strong", "pre-trained", "model", "mBART", "on", "tens", "of", "WMT", "benchmarks", ".", "For", "non-English", "directions", ",", "mRASP2", "achieves", "an", "improvement", "of", "average", "10", "+", "BLEU", "compared", "with", "the", "multilingual", "baseline"], "entities": [{"type": "Operation", "start": 126, "end": 127, "text": "mRASP2", "sent_idx": 5}, {"type": "Effect", "start": 132, "end": 133, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--21"}
{"text": "Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time. Partial encodings from the transformer model are shared among rerankers, providing further speed-up. When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.", "tokens": ["Large", "transformer-based", "language", "models", "have", "been", "shown", "to", "be", "very", "effective", "in", "many", "classification", "tasks", ".", "However", ",", "their", "computational", "complexity", "prevents", "their", "use", "in", "applications", "requiring", "the", "classification", "of", "a", "large", "set", "of", "candidates", ".", "While", "previous", "works", "have", "investigated", "approaches", "to", "reduce", "model", "size", ",", "relatively", "little", "attention", "has", "been", "paid", "to", "techniques", "to", "improve", "batch", "throughput", "during", "inference", ".", "In", "this", "paper", ",", "we", "introduce", "the", "Cascade", "Transformer", ",", "a", "simple", "yet", "effective", "technique", "to", "adapt", "transformer-based", "models", "into", "a", "cascade", "of", "rankers", ".", "Each", "ranker", "is", "used", "to", "prune", "a", "subset", "of", "candidates", "in", "a", "batch", ",", "thus", "dramatically", "increasing", "throughput", "at", "inference", "time", ".", "Partial", "encodings", "from", "the", "transformer", "model", "are", "shared", "among", "rerankers", ",", "providing", "further", "speed-up", ".", "When", "compared", "to", "a", "state-of-the-art", "transformer", "model", ",", "our", "approach", "reduces", "computation", "by", "37", "%", "with", "almost", "no", "impact", "on", "accuracy", ",", "as", "measured", "on", "two", "English", "Question", "Answering", "datasets", "."], "entities": [{"type": "Operation", "start": 69, "end": 71, "text": "Cascade Transformer", "sent_idx": 3}, {"type": "Effect", "start": 122, "end": 123, "text": "speed-up", "sent_idx": 5}, {"type": "Effect", "start": 135, "end": 136, "text": "computation", "sent_idx": 6}, {"type": "Effect", "start": 144, "end": 145, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--504"}
{"text": "We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an \u201ceasy-to-difficult\u201d scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. As for IC, it progressively strengthens the model\u2019s ability in identifying the mismatching information between the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.", "tokens": ["We", "study", "the", "learning", "of", "a", "matching", "model", "for", "dialogue", "response", "selection", ".", "Motivated", "by", "the", "recent", "finding", "that", "models", "trained", "with", "random", "negative", "samples", "are", "not", "ideal", "in", "real-world", "scenarios", ",", "we", "propose", "a", "hierarchical", "curriculum", "learning", "framework", "that", "trains", "the", "matching", "model", "in", "an", "\u201c", "easy-to-difficult", "\u201d", "scheme", ".", "Our", "learning", "framework", "consists", "of", "two", "complementary", "curricula", ":", "(", "1", ")", "corpus-level", "curriculum", "(", "CC", ")", ";", "and", "(", "2", ")", "instance-level", "curriculum", "(", "IC", ")", ".", "In", "CC", ",", "the", "model", "gradually", "increases", "its", "ability", "in", "finding", "the", "matching", "clues", "between", "the", "dialogue", "context", "and", "a", "response", "candidate", ".", "As", "for", "IC", ",", "it", "progressively", "strengthens", "the", "model", "\u2019s", "ability", "in", "identifying", "the", "mismatching", "information", "between", "the", "dialogue", "context", "and", "a", "response", "candidate", ".", "Empirical", "studies", "on", "three", "benchmark", "datasets", "with", "three", "state-of-the-art", "matching", "models", "demonstrate", "that", "the", "proposed", "learning", "framework", "significantly", "improves", "the", "model", "performance", "across", "various", "evaluation", "metrics", "."], "entities": [{"type": "Operation", "start": 142, "end": 144, "text": "learning framework", "sent_idx": 5}, {"type": "Effect", "start": 148, "end": 149, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--137"}
{"text": "Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicate-argument structures like \"quarterbacks throw passes to receivers\". Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.", "tokens": ["Learning", "by", "Reading", "(", "LbR", ")", "aims", "at", "enabling", "machines", "to", "acquire", "knowledge", "from", "and", "reason", "about", "textual", "input", ".", "This", "requires", "knowledge", "about", "the", "domain", "structure", "(", "such", "as", "entities", ",", "classes", ",", "and", "actions", ")", "in", "order", "to", "do", "inference", ".", "We", "present", "a", "method", "to", "infer", "this", "implicit", "knowledge", "from", "unlabeled", "text", ".", "Unlike", "previous", "approaches", ",", "we", "use", "automatically", "extracted", "classes", "with", "a", "probability", "distribution", "over", "entities", "to", "allow", "for", "context-sensitive", "labeling", ".", "From", "a", "corpus", "of", "1.4", "m", "sentences", ",", "we", "learn", "about", "250k", "simple", "propositions", "about", "American", "football", "in", "the", "form", "of", "predicate-argument", "structures", "like", "\"", "quarterbacks", "throw", "passes", "to", "receivers", "\"", ".", "Using", "several", "statistical", "measures", ",", "we", "show", "that", "our", "model", "is", "able", "to", "generalize", "and", "explain", "the", "data", "statistically", "significantly", "better", "than", "various", "baseline", "approaches", ".", "Human", "subjects", "judged", "up", "to", "96.6", "%", "of", "the", "resulting", "propositions", "to", "be", "sensible", ".", "The", "classes", "and", "probabilistic", "model", "can", "be", "used", "in", "textual", "enrichment", "to", "improve", "the", "performance", "of", "LbR", "end-to-end", "systems", "."], "entities": [{"type": "Operation", "start": 151, "end": 155, "text": "classes and probabilistic model", "sent_idx": 7}, {"type": "Effect", "start": 164, "end": 165, "text": "performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1147"}
{"text": "This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most \"similar\" to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.", "tokens": ["This", "paper", "proposes", "a", "new", "method", "for", "approximate", "string", "search", ",", "specifically", "candidate", "generation", "in", "spelling", "error", "correction", ",", "which", "is", "a", "task", "as", "follows", ".", "Given", "a", "misspelled", "word", ",", "the", "system", "finds", "words", "in", "a", "dictionary", ",", "which", "are", "most", "\"", "similar", "\"", "to", "the", "misspelled", "word", ".", "The", "paper", "proposes", "a", "probabilistic", "approach", "to", "the", "task", ",", "which", "is", "both", "accurate", "and", "efficient", ".", "The", "approach", "includes", "the", "use", "of", "a", "log", "linear", "model", ",", "a", "method", "for", "training", "the", "model", ",", "and", "an", "algorithm", "for", "finding", "the", "top", "k", "candidates", ".", "The", "log", "linear", "model", "is", "defined", "as", "a", "conditional", "probability", "distribution", "of", "a", "corrected", "word", "and", "a", "rule", "set", "for", "the", "correction", "conditioned", "on", "the", "misspelled", "word", ".", "The", "learning", "method", "employs", "the", "criterion", "in", "candidate", "generation", "as", "loss", "function", ".", "The", "retrieval", "algorithm", "is", "efficient", "and", "is", "guaranteed", "to", "find", "the", "optimal", "k", "candidates", ".", "Experimental", "results", "on", "large", "scale", "data", "show", "that", "the", "proposed", "approach", "improves", "upon", "existing", "methods", "in", "terms", "of", "accuracy", "in", "different", "settings", "."], "entities": [{"type": "Operation", "start": 53, "end": 56, "text": "a probabilistic approach", "sent_idx": 2}, {"type": "Effect", "start": 169, "end": 170, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1006"}
{"text": "This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.", "tokens": ["This", "paper", "presents", "the", "Bayesian", "Hierarchical", "Words", "Representation", "(", "BHWR", ")", "learning", "algorithm", ".", "BHWR", "facilitates", "Variational", "Bayes", "word", "representation", "learning", "combined", "with", "semantic", "taxonomy", "modeling", "via", "hierarchical", "priors", ".", "By", "propagating", "relevant", "information", "between", "related", "words", ",", "BHWR", "utilizes", "the", "taxonomy", "to", "improve", "the", "quality", "of", "such", "representations", ".", "Evaluation", "of", "several", "linguistic", "datasets", "demonstrates", "the", "advantages", "of", "BHWR", "over", "suitable", "alternatives", "that", "facilitate", "Bayesian", "modeling", "with", "or", "without", "semantic", "priors", ".", "Finally", ",", "we", "further", "show", "that", "BHWR", "produces", "better", "representations", "for", "rare", "words", "."], "entities": [{"type": "Operation", "start": 38, "end": 39, "text": "BHWR", "sent_idx": 2}, {"type": "Effect", "start": 45, "end": 49, "text": "quality of such representations", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--356"}
{"text": "In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.", "tokens": ["In", "this", "paper", ",", "we", "argue", "that", "elementary", "discourse", "unit", "(", "EDU", ")", "is", "a", "more", "appropriate", "textual", "unit", "of", "content", "selection", "than", "the", "sentence", "unit", "in", "abstractive", "summarization", ".", "To", "well", "handle", "the", "problem", "of", "composing", "EDUs", "into", "an", "informative", "and", "fluent", "summary", ",", "we", "propose", "a", "novel", "summarization", "method", "that", "first", "designs", "an", "EDU", "selection", "model", "to", "extract", "and", "group", "informative", "EDUs", "and", "then", "an", "EDU", "fusion", "model", "to", "fuse", "the", "EDUs", "in", "each", "group", "into", "one", "sentence", ".", "We", "also", "design", "the", "reinforcement", "learning", "mechanism", "to", "use", "EDU", "fusion", "results", "to", "reward", "the", "EDU", "selection", "action", ",", "boosting", "the", "final", "summarization", "performance", ".", "Experiments", "on", "CNN/Daily", "Mail", "have", "demonstrated", "the", "effectiveness", "of", "our", "model", "."], "entities": [{"type": "Operation", "start": 85, "end": 88, "text": "reinforcement learning mechanism", "sent_idx": 2}, {"type": "Effect", "start": 104, "end": 105, "text": "performance", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--551"}
{"text": "We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.", "tokens": ["We", "propose", "a", "sentence-level", "language", "model", "which", "selects", "the", "next", "sentence", "in", "a", "story", "from", "a", "finite", "set", "of", "fluent", "alternatives", ".", "Since", "it", "does", "not", "need", "to", "model", "fluency", ",", "the", "sentence-level", "language", "model", "can", "focus", "on", "longer", "range", "dependencies", ",", "which", "are", "crucial", "for", "multi-sentence", "coherence", ".", "Rather", "than", "dealing", "with", "individual", "words", ",", "our", "method", "treats", "the", "story", "so", "far", "as", "a", "list", "of", "pre-trained", "sentence", "embeddings", "and", "predicts", "an", "embedding", "for", "the", "next", "sentence", ",", "which", "is", "more", "efficient", "than", "predicting", "word", "embeddings", ".", "Notably", "this", "allows", "us", "to", "consider", "a", "large", "number", "of", "candidates", "for", "the", "next", "sentence", "during", "training", ".", "We", "demonstrate", "the", "effectiveness", "of", "our", "approach", "with", "state-of-the-art", "accuracy", "on", "the", "unsupervised", "Story", "Cloze", "task", "and", "with", "promising", "results", "on", "larger-scale", "next", "sentence", "prediction", "tasks", "."], "entities": [{"type": "Operation", "start": 3, "end": 6, "text": "sentence-level language model", "sent_idx": 0}, {"type": "Effect", "start": 115, "end": 116, "text": "accuracy", "sent_idx": 4}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--666"}
{"text": "Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations influence a claim\u2019s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance.", "tokens": ["Discourse", "relations", "among", "arguments", "reveal", "logical", "structures", "of", "a", "debate", "conversation", ".", "However", ",", "no", "prior", "work", "has", "explicitly", "studied", "how", "the", "sequence", "of", "discourse", "relations", "influence", "a", "claim", "\u2019s", "impact", ".", "This", "paper", "empirically", "shows", "that", "the", "discourse", "relations", "between", "two", "arguments", "along", "the", "context", "path", "are", "essential", "factors", "for", "identifying", "the", "persuasive", "power", "of", "an", "argument", ".", "We", "further", "propose", "DisCOC", "to", "inject", "and", "fuse", "the", "sentence-level", "structural", "discourse", "information", "with", "contextualized", "features", "derived", "from", "large-scale", "language", "models", ".", "Experimental", "results", "and", "extensive", "analysis", "show", "that", "the", "attention", "and", "gate", "mechanisms", "that", "explicitly", "model", "contexts", "and", "texts", "can", "indeed", "help", "the", "argument", "impact", "classification", "task", "defined", "by", "Durmus", "et", "al.", "(", "2019", ")", ",", "and", "discourse", "structures", "among", "the", "context", "path", "of", "the", "claim", "to", "be", "classified", "can", "further", "boost", "the", "performance", "."], "entities": [{"type": "Operation", "start": 117, "end": 126, "text": "discourse structures among the context path of the claim", "sent_idx": 4}, {"type": "Effect", "start": 133, "end": 134, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--306"}
{"text": "Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product's attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a human-labeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL-SOT approach is easily generalized to labeling a mix of reviews of more than one products.", "tokens": ["Existing", "works", "on", "sentiment", "analysis", "on", "product", "reviews", "suffer", "from", "the", "following", "limitations", ":", "(", "1", ")", "The", "knowledge", "of", "hierarchical", "relationships", "of", "products", "attributes", "is", "not", "fully", "utilized", ".", "(", "2", ")", "Reviews", "or", "sentences", "mentioning", "several", "attributes", "associated", "with", "complicated", "sentiments", "are", "not", "dealt", "with", "very", "well", ".", "In", "this", "paper", ",", "we", "propose", "a", "novel", "HL-SOT", "approach", "to", "labeling", "a", "product", "'s", "attributes", "and", "their", "associated", "sentiments", "in", "product", "reviews", "by", "a", "Hierarchical", "Learning", "(", "HL", ")", "process", "with", "a", "defined", "Sentiment", "Ontology", "Tree", "(", "SOT", ")", ".", "The", "empirical", "analysis", "against", "a", "human-labeled", "data", "set", "demonstrates", "promising", "and", "reasonable", "performance", "of", "the", "proposed", "HL-SOT", "approach", ".", "While", "this", "paper", "is", "mainly", "on", "sentiment", "analysis", "on", "reviews", "of", "one", "product", ",", "our", "proposed", "HL-SOT", "approach", "is", "easily", "generalized", "to", "labeling", "a", "mix", "of", "reviews", "of", "more", "than", "one", "products", "."], "entities": [{"type": "Operation", "start": 107, "end": 109, "text": "HL-SOT approach", "sent_idx": 3}, {"type": "Effect", "start": 103, "end": 104, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1042"}
{"text": "Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment.", "tokens": ["Deep", "learning", "models", "for", "automatic", "readability", "assessment", "generally", "discard", "linguistic", "features", "traditionally", "used", "in", "machine", "learning", "models", "for", "the", "task", ".", "We", "propose", "to", "incorporate", "linguistic", "features", "into", "neural", "network", "models", "by", "learning", "syntactic", "dense", "embeddings", "based", "on", "linguistic", "features", ".", "To", "cope", "with", "the", "relationships", "between", "the", "features", ",", "we", "form", "a", "correlation", "graph", "among", "features", "and", "use", "it", "to", "learn", "their", "embeddings", "so", "that", "similar", "features", "will", "be", "represented", "by", "similar", "embeddings", ".", "Experiments", "with", "six", "data", "sets", "of", "two", "proficiency", "levels", "demonstrate", "that", "our", "proposed", "methodology", "can", "complement", "BERT-only", "model", "to", "achieve", "significantly", "better", "performances", "for", "automatic", "readability", "assessment", "."], "entities": [{"type": "Operation", "start": 24, "end": 31, "text": "incorporate linguistic features into neural network models", "sent_idx": 1}, {"type": "Effect", "start": 97, "end": 98, "text": "performances", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--235"}
{"text": "Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.", "tokens": ["Many", "studies", "have", "applied", "reinforcement", "learning", "to", "train", "a", "dialog", "policy", "and", "show", "great", "promise", "these", "years", ".", "One", "common", "approach", "is", "to", "employ", "a", "user", "simulator", "to", "obtain", "a", "large", "number", "of", "simulated", "user", "experiences", "for", "reinforcement", "learning", "algorithms", ".", "However", ",", "modeling", "a", "realistic", "user", "simulator", "is", "challenging", ".", "A", "rule-based", "simulator", "requires", "heavy", "domain", "expertise", "for", "complex", "tasks", ",", "and", "a", "data-driven", "simulator", "requires", "considerable", "data", "and", "it", "is", "even", "unclear", "how", "to", "evaluate", "a", "simulator", ".", "To", "avoid", "explicitly", "building", "a", "user", "simulator", "beforehand", ",", "we", "propose", "Multi-Agent", "Dialog", "Policy", "Learning", ",", "which", "regards", "both", "the", "system", "and", "the", "user", "as", "the", "dialog", "agents", ".", "Two", "agents", "interact", "with", "each", "other", "and", "are", "jointly", "learned", "simultaneously", ".", "The", "method", "uses", "the", "actor-critic", "framework", "to", "facilitate", "pretraining", "and", "improve", "scalability", ".", "We", "also", "propose", "Hybrid", "Value", "Network", "for", "the", "role-aware", "reward", "decomposition", "to", "integrate", "role-specific", "domain", "knowledge", "of", "each", "agent", "in", "the", "task-oriented", "dialog", ".", "Results", "show", "that", "our", "method", "can", "successfully", "build", "a", "system", "policy", "and", "a", "user", "policy", "simultaneously", ",", "and", "two", "agents", "can", "achieve", "a", "high", "task", "success", "rate", "through", "conversational", "interaction", "."], "entities": [{"type": "Operation", "start": 123, "end": 127, "text": "uses the actor-critic framework", "sent_idx": 6}, {"type": "Effect", "start": 132, "end": 133, "text": "scalability", "sent_idx": 6}, {"type": "Operation", "start": 176, "end": 178, "text": "two agents", "sent_idx": 8}, {"type": "Effect", "start": 182, "end": 185, "text": "task success rate", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--59"}
{"text": "We present algorithms for higher-order dependency parsing that are \"third-order\" in the sense that they can evaluate substructures containing three dependencies, and \"efficient\" in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.", "tokens": ["We", "present", "algorithms", "for", "higher-order", "dependency", "parsing", "that", "are", "\"", "third-order", "\"", "in", "the", "sense", "that", "they", "can", "evaluate", "substructures", "containing", "three", "dependencies", ",", "and", "\"", "efficient", "\"", "in", "the", "sense", "that", "they", "require", "only", "O(n4", ")", "time", ".", "Importantly", ",", "our", "new", "parsers", "can", "utilize", "both", "sibling-style", "and", "grandchild-style", "interactions", ".", "We", "evaluate", "our", "parsers", "on", "the", "Penn", "Treebank", "and", "Prague", "Dependency", "Treebank", ",", "achieving", "unlabeled", "attachment", "scores", "of", "93.04", "%", "and", "87.38", "%", ",", "respectively", "."], "entities": [{"type": "Operation", "start": 54, "end": 56, "text": "our parsers", "sent_idx": 2}, {"type": "Effect", "start": 66, "end": 69, "text": "unlabeled attachment scores", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P10-1001"}
{"text": "Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.", "tokens": ["Subword", "segmentation", "is", "widely", "used", "to", "address", "the", "open", "vocabulary", "problem", "in", "machine", "translation", ".", "The", "dominant", "approach", "to", "subword", "segmentation", "is", "Byte", "Pair", "Encoding", "(", "BPE", ")", ",", "which", "keeps", "the", "most", "frequent", "words", "intact", "while", "splitting", "the", "rare", "ones", "into", "multiple", "tokens", ".", "While", "multiple", "segmentations", "are", "possible", "even", "with", "the", "same", "vocabulary", ",", "BPE", "splits", "words", "into", "unique", "sequences", ";", "this", "may", "prevent", "a", "model", "from", "better", "learning", "the", "compositionality", "of", "words", "and", "being", "robust", "to", "segmentation", "errors", ".", "So", "far", ",", "the", "only", "way", "to", "overcome", "this", "BPE", "imperfection", ",", "its", "deterministic", "nature", ",", "was", "to", "create", "another", "subword", "segmentation", "algorithm", "(", "Kudo", ",", "2018", ")", ".", "In", "contrast", ",", "we", "show", "that", "BPE", "itself", "incorporates", "the", "ability", "to", "produce", "multiple", "segmentations", "of", "the", "same", "word", ".", "We", "introduce", "BPE-dropout", "-", "simple", "and", "effective", "subword", "regularization", "method", "based", "on", "and", "compatible", "with", "conventional", "BPE", ".", "It", "stochastically", "corrupts", "the", "segmentation", "procedure", "of", "BPE", ",", "which", "leads", "to", "producing", "multiple", "segmentations", "within", "the", "same", "fixed", "BPE", "framework", ".", "Using", "BPE-dropout", "during", "training", "and", "the", "standard", "BPE", "during", "inference", "improves", "translation", "quality", "up", "to", "2.3", "BLEU", "compared", "to", "BPE", "and", "up", "to", "0.9", "BLEU", "compared", "to", "the", "previous", "subword", "regularization", "."], "entities": [{"type": "Operation", "start": 177, "end": 181, "text": "standard BPE during inference", "sent_idx": 7}, {"type": "Effect", "start": 187, "end": 188, "text": "BLEU", "sent_idx": 7}, {"type": "Effect", "start": 195, "end": 196, "text": "BLEU", "sent_idx": 7}, {"type": "Operation", "start": 172, "end": 175, "text": "BPE-dropout during training", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 3, "tail": 1}, {"type": "Pos_Affect", "head": 3, "tail": 2}], "id": "abstract-2020--acl-main--170"}
{"text": "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.", "tokens": ["Question", "Generation", "(", "QG", ")", "is", "fundamentally", "a", "simple", "syntactic", "transformation", ";", "however", ",", "many", "aspects", "of", "semantics", "influence", "what", "questions", "are", "good", "to", "form", ".", "We", "implement", "this", "observation", "by", "developing", "Syn-QG", ",", "a", "set", "of", "transparent", "syntactic", "rules", "leveraging", "universal", "dependencies", ",", "shallow", "semantic", "parsing", ",", "lexical", "resources", ",", "and", "custom", "rules", "which", "transform", "declarative", "sentences", "into", "question-answer", "pairs", ".", "We", "utilize", "PropBank", "argument", "descriptions", "and", "VerbNet", "state", "predicates", "to", "incorporate", "shallow", "semantic", "content", ",", "which", "helps", "generate", "questions", "of", "a", "descriptive", "nature", "and", "produce", "inferential", "and", "semantically", "richer", "questions", "than", "existing", "systems", ".", "In", "order", "to", "improve", "syntactic", "fluency", "and", "eliminate", "grammatically", "incorrect", "questions", ",", "we", "employ", "back-translation", "over", "the", "output", "of", "these", "syntactic", "rules", ".", "A", "set", "of", "crowd-sourced", "evaluations", "shows", "that", "our", "system", "can", "generate", "a", "larger", "number", "of", "highly", "grammatical", "and", "relevant", "questions", "than", "previous", "QG", "systems", "and", "that", "back-translation", "drastically", "improves", "grammaticality", "at", "a", "slight", "cost", "of", "generating", "irrelevant", "questions", "."], "entities": [{"type": "Operation", "start": 145, "end": 146, "text": "back-translation", "sent_idx": 4}, {"type": "Effect", "start": 148, "end": 149, "text": "grammaticality", "sent_idx": 4}, {"type": "Effect", "start": 152, "end": 153, "text": "cost", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--69"}
{"text": "Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that \u201cthe instance does not belong to these complementary labels\u201d. Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model\u2019s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect.", "tokens": ["Distant", "supervision", "for", "relation", "extraction", "provides", "uniform", "bag", "labels", "for", "each", "sentence", "inside", "the", "bag", ",", "while", "accurate", "sentence", "labels", "are", "important", "for", "downstream", "applications", "that", "need", "the", "exact", "relation", "type", ".", "Directly", "using", "bag", "labels", "for", "sentence-level", "training", "will", "introduce", "much", "noise", ",", "thus", "severely", "degrading", "performance", ".", "In", "this", "work", ",", "we", "propose", "the", "use", "of", "negative", "training", "(", "NT", ")", ",", "in", "which", "a", "model", "is", "trained", "using", "complementary", "labels", "regarding", "that", "\u201c", "the", "instance", "does", "not", "belong", "to", "these", "complementary", "labels", "\u201d", ".", "Since", "the", "probability", "of", "selecting", "a", "true", "label", "as", "a", "complementary", "label", "is", "low", ",", "NT", "provides", "less", "noisy", "information", ".", "Furthermore", ",", "the", "model", "trained", "with", "NT", "is", "able", "to", "separate", "the", "noisy", "data", "from", "the", "training", "data", ".", "Based", "on", "NT", ",", "we", "propose", "a", "sentence-level", "framework", ",", "SENT", ",", "for", "distant", "relation", "extraction", ".", "SENT", "not", "only", "filters", "the", "noisy", "data", "to", "construct", "a", "cleaner", "dataset", ",", "but", "also", "performs", "a", "re-labeling", "process", "to", "transform", "the", "noisy", "data", "into", "useful", "training", "data", ",", "thus", "further", "benefiting", "the", "model", "\u2019s", "performance", ".", "Experimental", "results", "show", "the", "significant", "improvement", "of", "the", "proposed", "method", "over", "previous", "methods", "on", "sentence-level", "evaluation", "and", "de-noise", "effect", "."], "entities": [{"type": "Operation", "start": 159, "end": 163, "text": "performs a re-labeling process", "sent_idx": 6}, {"type": "Effect", "start": 179, "end": 180, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 147, "end": 151, "text": "filters the noisy data", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--484"}
{"text": "In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent\u2019s high-level strategy in negotiation tasks. Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent\u2019s personality type during both learning and inference. We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our method using ToM inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents. We also demonstrate that our model displays diverse negotiation behavior with different types of opponents.", "tokens": ["In", "this", "paper", ",", "we", "explore", "the", "ability", "to", "model", "and", "infer", "personality", "types", "of", "opponents", ",", "predict", "their", "responses", ",", "and", "use", "this", "information", "to", "adapt", "a", "dialog", "agent", "\u2019s", "high-level", "strategy", "in", "negotiation", "tasks", ".", "Inspired", "by", "the", "idea", "of", "incorporating", "a", "theory", "of", "mind", "(", "ToM", ")", "into", "machines", ",", "we", "introduce", "a", "probabilistic", "formulation", "to", "encapsulate", "the", "opponent", "\u2019s", "personality", "type", "during", "both", "learning", "and", "inference", ".", "We", "test", "our", "approach", "on", "the", "CraigslistBargain", "dataset", "(", "He", "et", "al.", "2018", ")", "and", "show", "that", "our", "method", "using", "ToM", "inference", "achieves", "a", "20", "%", "higher", "dialog", "agreement", "rate", "compared", "to", "baselines", "on", "a", "mixed", "population", "of", "opponents", ".", "We", "also", "demonstrate", "that", "our", "model", "displays", "diverse", "negotiation", "behavior", "with", "different", "types", "of", "opponents", "."], "entities": [{"type": "Operation", "start": 90, "end": 93, "text": "using ToM inference", "sent_idx": 2}, {"type": "Effect", "start": 98, "end": 101, "text": "dialog agreement rate", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--56"}
{"text": "Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing.", "tokens": ["Conversational", "semantic", "parsers", "map", "user", "utterances", "to", "executable", "programs", "given", "dialogue", "histories", "composed", "of", "previous", "utterances", ",", "programs", ",", "and", "system", "responses", ".", "Existing", "parsers", "typically", "condition", "on", "rich", "representations", "of", "history", "that", "include", "the", "complete", "set", "of", "values", "and", "computations", "previously", "discussed", ".", "We", "propose", "a", "model", "that", "abstracts", "over", "values", "to", "focus", "prediction", "on", "type-", "and", "function-level", "context", ".", "This", "approach", "provides", "a", "compact", "encoding", "of", "dialogue", "histories", "and", "predicted", "programs", ",", "improving", "generalization", "and", "computational", "efficiency", ".", "Our", "model", "incorporates", "several", "other", "components", ",", "including", "an", "atomic", "span", "copy", "operation", "and", "structural", "enforcement", "of", "well-formedness", "constraints", "on", "predicted", "programs", ",", "that", "are", "particularly", "advantageous", "in", "the", "low-data", "regime", ".", "Trained", "on", "the", "SMCalFlow", "and", "TreeDST", "datasets", ",", "our", "model", "outperforms", "prior", "work", "by", "7.3", "%", "and", "10.6", "%", "respectively", "in", "terms", "of", "absolute", "accuracy", ".", "Trained", "on", "only", "a", "thousand", "examples", "from", "each", "dataset", ",", "it", "outperforms", "strong", "baselines", "by", "12.4", "%", "and", "6.4", "%", ".", "These", "results", "indicate", "that", "simple", "representations", "are", "key", "to", "effective", "generalization", "in", "conversational", "semantic", "parsing", "."], "entities": [{"type": "Operation", "start": 63, "end": 73, "text": "provides a compact encoding of dialogue histories and predicted programs", "sent_idx": 3}, {"type": "Effect", "start": 75, "end": 76, "text": "generalization", "sent_idx": 3}, {"type": "Effect", "start": 77, "end": 79, "text": "computational efficiency", "sent_idx": 3}, {"type": "Effect", "start": 135, "end": 137, "text": "absolute accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Pos_Affect", "head": 0, "tail": 3}], "id": "abstract-2021--acl-long--284"}
{"text": "Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.", "tokens": ["Neural", "sequence", "to", "sequence", "text", "generation", "has", "been", "proved", "to", "be", "a", "viable", "approach", "to", "paraphrase", "generation", ".", "Despite", "promising", "results", ",", "paraphrases", "generated", "by", "these", "models", "mostly", "suffer", "from", "lack", "of", "quality", "and", "diversity", ".", "To", "address", "these", "problems", ",", "we", "propose", "a", "novel", "retrieval-based", "method", "for", "paraphrase", "generation", ".", "Our", "model", "first", "retrieves", "a", "paraphrase", "pair", "similar", "to", "the", "input", "sentence", "from", "a", "pre-defined", "index", ".", "With", "its", "novel", "editor", "module", ",", "the", "model", "then", "paraphrases", "the", "input", "sequence", "by", "editing", "it", "using", "the", "extracted", "relations", "between", "the", "retrieved", "pair", "of", "sentences", ".", "In", "order", "to", "have", "fine-grained", "control", "over", "the", "editing", "process", ",", "our", "model", "uses", "the", "newly", "introduced", "concept", "of", "Micro", "Edit", "Vectors", ".", "It", "both", "extracts", "and", "exploits", "these", "vectors", "using", "the", "attention", "mechanism", "in", "the", "Transformer", "architecture", ".", "Experimental", "results", "show", "the", "superiority", "of", "our", "paraphrase", "generation", "method", "in", "terms", "of", "both", "automatic", "metrics", ",", "and", "human", "evaluation", "of", "relevance", ",", "grammaticality", ",", "and", "diversity", "of", "generated", "paraphrases", "."], "entities": [{"type": "Operation", "start": 45, "end": 47, "text": "retrieval-based method", "sent_idx": 2}, {"type": "Effect", "start": 155, "end": 156, "text": "relevance", "sent_idx": 7}, {"type": "Effect", "start": 157, "end": 158, "text": "grammaticality", "sent_idx": 7}, {"type": "Effect", "start": 160, "end": 161, "text": "diversity", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "abstract-2020--acl-main--535"}
{"text": "In this work, we present a novel approach to the generation task of ordering prenominal modifiers. We take a maximum entropy reranking approach to the problem which admits arbitrary features on a permutation of modifiers, exploiting hundreds of thousands of features in total. We compare our error rates to the state-of-the-art and to a strong Google n-gram count baseline. We attain a maximum error reduction of 69.8% and average error reduction across all test sets of 59.1% compared to the state-of-the-art and a maximum error reduction of 68.4% and average error reduction across all test sets of 41.8% compared to our Google n-gram count baseline.", "tokens": ["In", "this", "work", ",", "we", "present", "a", "novel", "approach", "to", "the", "generation", "task", "of", "ordering", "prenominal", "modifiers", ".", "We", "take", "a", "maximum", "entropy", "reranking", "approach", "to", "the", "problem", "which", "admits", "arbitrary", "features", "on", "a", "permutation", "of", "modifiers", ",", "exploiting", "hundreds", "of", "thousands", "of", "features", "in", "total", ".", "We", "compare", "our", "error", "rates", "to", "the", "state-of-the-art", "and", "to", "a", "strong", "Google", "n-gram", "count", "baseline", ".", "We", "attain", "a", "maximum", "error", "reduction", "of", "69.8", "%", "and", "average", "error", "reduction", "across", "all", "test", "sets", "of", "59.1", "%", "compared", "to", "the", "state-of-the-art", "and", "a", "maximum", "error", "reduction", "of", "68.4", "%", "and", "average", "error", "reduction", "across", "all", "test", "sets", "of", "41.8", "%", "compared", "to", "our", "Google", "n-gram", "count", "baseline", "."], "entities": [{"type": "Operation", "start": 21, "end": 25, "text": "maximum entropy reranking approach", "sent_idx": 1}, {"type": "Effect", "start": 50, "end": 52, "text": "error rates", "sent_idx": 2}, {"type": "Effect", "start": 67, "end": 69, "text": "maximum error", "sent_idx": 3}, {"type": "Effect", "start": 74, "end": 76, "text": "average error", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "P11-1111"}
{"text": "The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.", "tokens": ["The", "Neural", "Machine", "Translation", "(", "NMT", ")", "model", "is", "essentially", "a", "joint", "language", "model", "conditioned", "on", "both", "the", "source", "sentence", "and", "partial", "translation", ".", "Therefore", ",", "the", "NMT", "model", "naturally", "involves", "the", "mechanism", "of", "the", "Language", "Model", "(", "LM", ")", "that", "predicts", "the", "next", "token", "only", "based", "on", "partial", "translation", ".", "Despite", "its", "success", ",", "NMT", "still", "suffers", "from", "the", "hallucination", "problem", ",", "generating", "fluent", "but", "inadequate", "translations", ".", "The", "main", "reason", "is", "that", "NMT", "pays", "excessive", "attention", "to", "the", "partial", "translation", "while", "neglecting", "the", "source", "sentence", "to", "some", "extent", ",", "namely", "overconfidence", "of", "the", "LM", ".", "Accordingly", ",", "we", "define", "the", "Margin", "between", "the", "NMT", "and", "the", "LM", ",", "calculated", "by", "subtracting", "the", "predicted", "probability", "of", "the", "LM", "from", "that", "of", "the", "NMT", "model", "for", "each", "token", ".", "The", "Margin", "is", "negatively", "correlated", "to", "the", "overconfidence", "degree", "of", "the", "LM", ".", "Based", "on", "the", "property", ",", "we", "propose", "a", "Margin-based", "Token-level", "Objective", "(", "MTO", ")", "and", "a", "Margin-based", "Sentence-level", "Objective", "(", "MSO", ")", "to", "maximize", "the", "Margin", "for", "preventing", "the", "LM", "from", "being", "overconfident", ".", "Experiments", "on", "WMT14", "English-to-German", ",", "WMT19", "Chinese-to-English", ",", "and", "WMT14", "English-to-French", "translation", "tasks", "demonstrate", "the", "effectiveness", "of", "our", "approach", ",", "with", "1.36", ",", "1.50", ",", "and", "0.63", "BLEU", "improvements", ",", "respectively", ",", "compared", "to", "the", "Transformer", "baseline", ".", "The", "human", "evaluation", "further", "verifies", "that", "our", "approaches", "improve", "translation", "adequacy", "as", "well", "as", "fluency", "."], "entities": [{"type": "Operation", "start": 150, "end": 156, "text": "Margin-based Token-level Objective (MTO)", "sent_idx": 6}, {"type": "Effect", "start": 203, "end": 204, "text": "BLEU", "sent_idx": 7}, {"type": "Operation", "start": 158, "end": 164, "text": "Margin-based Sentence-level Objective (MSO)", "sent_idx": 6}, {"type": "Effect", "start": 223, "end": 225, "text": "translation adequacy", "sent_idx": 8}, {"type": "Effect", "start": 228, "end": 229, "text": "fluency", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 0, "tail": 4}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 4}], "id": "abstract-2021--acl-long--268"}
{"text": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy\u2019s layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.", "tokens": ["In", "hierarchical", "text", "classification", ",", "we", "perform", "a", "sequence", "of", "inference", "steps", "to", "predict", "the", "category", "of", "a", "document", "from", "top", "to", "bottom", "of", "a", "given", "class", "taxonomy", ".", "Most", "of", "the", "studies", "have", "focused", "on", "developing", "novels", "neural", "network", "architectures", "to", "deal", "with", "the", "hierarchical", "structure", ",", "but", "we", "prefer", "to", "look", "for", "efficient", "ways", "to", "strengthen", "a", "baseline", "model", ".", "We", "first", "define", "the", "task", "as", "a", "sequence-to-sequence", "problem", ".", "Afterwards", ",", "we", "propose", "an", "auxiliary", "synthetic", "task", "of", "bottom-up-classification", ".", "Then", ",", "from", "external", "dictionaries", ",", "we", "retrieve", "textual", "definitions", "for", "the", "classes", "of", "all", "the", "hierarchy", "\u2019s", "layers", ",", "and", "map", "them", "into", "the", "word", "vector", "space", ".", "We", "use", "the", "class-definition", "embeddings", "as", "an", "additional", "input", "to", "condition", "the", "prediction", "of", "the", "next", "layer", "and", "in", "an", "adapted", "beam", "search", ".", "Whereas", "the", "modified", "search", "did", "not", "provide", "large", "gains", ",", "the", "combination", "of", "the", "auxiliary", "task", "and", "the", "additional", "input", "of", "class-definitions", "significantly", "enhance", "the", "classification", "accuracy", ".", "With", "our", "efficient", "approaches", ",", "we", "outperform", "previous", "studies", ",", "using", "a", "drastically", "reduced", "number", "of", "parameters", ",", "in", "two", "well-known", "English", "datasets", "."], "entities": [{"type": "Operation", "start": 147, "end": 158, "text": "combination of the auxiliary task and the additional input of class-definitions", "sent_idx": 6}, {"type": "Effect", "start": 162, "end": 163, "text": "accuracy", "sent_idx": 6}, {"type": "Effect", "start": 178, "end": 181, "text": "number of parameters", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--205"}
{"text": "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance.", "tokens": ["Natural", "language", "is", "compositional", ";", "the", "meaning", "of", "a", "sentence", "is", "a", "function", "of", "the", "meaning", "of", "its", "parts", ".", "This", "property", "allows", "humans", "to", "create", "and", "interpret", "novel", "sentences", ",", "generalizing", "robustly", "outside", "their", "prior", "experience", ".", "Neural", "networks", "have", "been", "shown", "to", "struggle", "with", "this", "kind", "of", "generalization", ",", "in", "particular", "performing", "poorly", "on", "tasks", "designed", "to", "assess", "compositional", "generalization", "(", "i.e.", "where", "training", "and", "testing", "distributions", "differ", "in", "ways", "that", "would", "be", "trivial", "for", "a", "compositional", "strategy", "to", "resolve", ")", ".", "Their", "poor", "performance", "on", "these", "tasks", "may", "in", "part", "be", "due", "to", "the", "nature", "of", "supervised", "learning", "which", "assumes", "training", "and", "testing", "data", "to", "be", "drawn", "from", "the", "same", "distribution", ".", "We", "implement", "a", "meta-learning", "augmented", "version", "of", "supervised", "learning", "whose", "objective", "directly", "optimizes", "for", "out-of-distribution", "generalization", ".", "We", "construct", "pairs", "of", "tasks", "for", "meta-learning", "by", "sub-sampling", "existing", "training", "data", ".", "Each", "pair", "of", "tasks", "is", "constructed", "to", "contain", "relevant", "examples", ",", "as", "determined", "by", "a", "similarity", "metric", ",", "in", "an", "effort", "to", "inhibit", "models", "from", "memorizing", "their", "input", ".", "Experimental", "results", "on", "the", "COGS", "and", "SCAN", "datasets", "show", "that", "our", "similarity-driven", "meta-learning", "can", "improve", "generalization", "performance", "."], "entities": [{"type": "Operation", "start": 185, "end": 187, "text": "similarity-driven meta-learning", "sent_idx": 7}, {"type": "Effect", "start": 189, "end": 191, "text": "generalization performance", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--258"}
{"text": "Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.", "tokens": ["Event", "forecasting", "is", "a", "challenging", ",", "yet", "important", "task", ",", "as", "humans", "seek", "to", "constantly", "plan", "for", "the", "future", ".", "Existing", "automated", "forecasting", "studies", "rely", "mostly", "on", "structured", "data", ",", "such", "as", "time-series", "or", "event-based", "knowledge", "graphs", ",", "to", "help", "predict", "future", "events", ".", "In", "this", "work", ",", "we", "aim", "to", "formulate", "a", "task", ",", "construct", "a", "dataset", ",", "and", "provide", "benchmarks", "for", "developing", "methods", "for", "event", "forecasting", "with", "large", "volumes", "of", "unstructured", "text", "data", ".", "To", "simulate", "the", "forecasting", "scenario", "on", "temporal", "news", "documents", ",", "we", "formulate", "the", "problem", "as", "a", "restricted-domain", ",", "multiple-choice", ",", "question-answering", "(", "QA", ")", "task", ".", "Unlike", "existing", "QA", "tasks", ",", "our", "task", "limits", "accessible", "information", ",", "and", "thus", "a", "model", "has", "to", "make", "a", "forecasting", "judgement", ".", "To", "showcase", "the", "usefulness", "of", "this", "task", "formulation", ",", "we", "introduce", "ForecastQA", ",", "a", "question-answering", "dataset", "consisting", "of", "10,392", "event", "forecasting", "questions", ",", "which", "have", "been", "collected", "and", "verified", "via", "crowdsourcing", "efforts", ".", "We", "present", "our", "experiments", "on", "ForecastQA", "using", "BERTbased", "models", "and", "find", "that", "our", "best", "model", "achieves", "61.0", "%", "accuracy", "on", "the", "dataset", ",", "which", "still", "lags", "behind", "human", "performance", "by", "about", "19", "%", ".", "We", "hope", "ForecastQA", "will", "support", "future", "research", "efforts", "in", "bridging", "this", "gap", "."], "entities": [{"type": "Operation", "start": 164, "end": 166, "text": "BERTbased models", "sent_idx": 6}, {"type": "Effect", "start": 175, "end": 176, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--357"}
{"text": "Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%.", "tokens": ["Name", "ambiguity", "problem", "has", "raised", "urgent", "demands", "for", "efficient", ",", "high-quality", "named", "entity", "disambiguation", "methods", ".", "In", "recent", "years", ",", "the", "increasing", "availability", "of", "large-scale", ",", "rich", "semantic", "knowledge", "sources", "(", "such", "as", "Wikipedia", "and", "WordNet", ")", "creates", "new", "opportunities", "to", "enhance", "the", "named", "entity", "disambiguation", "by", "developing", "algorithms", "which", "can", "exploit", "these", "knowledge", "sources", "at", "best", ".", "The", "problem", "is", "that", "these", "knowledge", "sources", "are", "heterogeneous", "and", "most", "of", "the", "semantic", "knowledge", "within", "them", "is", "embedded", "in", "complex", "structures", ",", "such", "as", "graphs", "and", "networks", ".", "This", "paper", "proposes", "a", "knowledge-based", "method", ",", "called", "Structural", "Semantic", "Relatedness", "(", "SSR", ")", ",", "which", "can", "enhance", "the", "named", "entity", "disambiguation", "by", "capturing", "and", "leveraging", "the", "structural", "semantic", "knowledge", "in", "multiple", "knowledge", "sources", ".", "Empirical", "results", "show", "that", ",", "in", "comparison", "with", "the", "classical", "BOW", "based", "methods", "and", "social", "network", "based", "methods", ",", "our", "method", "can", "significantly", "improve", "the", "disambiguation", "performance", "by", "respectively", "8.7", "%", "and", "14.7", "%", "."], "entities": [{"type": "Operation", "start": 91, "end": 93, "text": "knowledge-based method", "sent_idx": 3}, {"type": "Effect", "start": 148, "end": 149, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1006"}
{"text": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.", "tokens": ["We", "present", "Knowledge", "Enhanced", "Multimodal", "BART", "(", "KM-BART", ")", ",", "which", "is", "a", "Transformer-based", "sequence-to-sequence", "model", "capable", "of", "reasoning", "about", "commonsense", "knowledge", "from", "multimodal", "inputs", "of", "images", "and", "texts", ".", "We", "adapt", "the", "generative", "BART", "architecture", "(", "Lewis", "et", "al.", ",", "2020", ")", "to", "a", "multimodal", "model", "with", "visual", "and", "textual", "inputs", ".", "We", "further", "develop", "novel", "pretraining", "tasks", "to", "improve", "the", "model", "performance", "on", "the", "Visual", "Commonsense", "Generation", "(", "VCG", ")", "task", ".", "In", "particular", ",", "our", "pretraining", "task", "of", "Knowledge-based", "Commonsense", "Generation", "(", "KCG", ")", "boosts", "model", "performance", "on", "the", "VCG", "task", "by", "leveraging", "commonsense", "knowledge", "from", "a", "large", "language", "model", "pretrained", "on", "external", "commonsense", "knowledge", "graphs", ".", "To", "the", "best", "of", "our", "knowledge", ",", "we", "are", "the", "first", "to", "propose", "a", "dedicated", "task", "for", "improving", "model", "performance", "on", "the", "VCG", "task", ".", "Experimental", "results", "show", "that", "our", "model", "reaches", "state-of-the-art", "performance", "on", "the", "VCG", "task", "(", "Park", "et", "al.", ",", "2020", ")", "by", "applying", "these", "novel", "pretraining", "tasks", "."], "entities": [{"type": "Operation", "start": 55, "end": 59, "text": "develop novel pretraining tasks", "sent_idx": 2}, {"type": "Effect", "start": 63, "end": 64, "text": "performance", "sent_idx": 2}, {"type": "Operation", "start": 2, "end": 9, "text": "Knowledge Enhanced Multimodal BART (KM-BART)", "sent_idx": 0}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--44"}
{"text": "Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.", "tokens": ["Natural", "language", "understanding", "(", "NLU", ")", "and", "natural", "language", "generation", "(", "NLG", ")", "are", "two", "fundamental", "and", "related", "tasks", "in", "building", "task-oriented", "dialogue", "systems", "with", "opposite", "objectives", ":", "NLU", "tackles", "the", "transformation", "from", "natural", "language", "to", "formal", "representations", ",", "whereas", "NLG", "does", "the", "reverse", ".", "A", "key", "to", "success", "in", "either", "task", "is", "parallel", "training", "data", "which", "is", "expensive", "to", "obtain", "at", "a", "large", "scale", ".", "In", "this", "work", ",", "we", "propose", "a", "generative", "model", "which", "couples", "NLU", "and", "NLG", "through", "a", "shared", "latent", "variable", ".", "This", "approach", "allows", "us", "to", "explore", "both", "spaces", "of", "natural", "language", "and", "formal", "representations", ",", "and", "facilitates", "information", "sharing", "through", "the", "latent", "space", "to", "eventually", "benefit", "NLU", "and", "NLG", ".", "Our", "model", "achieves", "state-of-the-art", "performance", "on", "two", "dialogue", "datasets", "with", "both", "flat", "and", "tree-structured", "formal", "representations", ".", "We", "also", "show", "that", "the", "model", "can", "be", "trained", "in", "a", "semi-supervised", "fashion", "by", "utilising", "unlabelled", "data", "to", "boost", "its", "performance", "."], "entities": [{"type": "Operation", "start": 147, "end": 150, "text": "utilising unlabelled data", "sent_idx": 5}, {"type": "Effect", "start": 153, "end": 154, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--163"}
{"text": "We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58% macro-F1 to almost 85%. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020.", "tokens": ["We", "introduce", "the", "well-established", "social", "scientific", "concept", "of", "social", "solidarity", "and", "its", "contestation", ",", "anti-solidarity", ",", "as", "a", "new", "problem", "setting", "to", "supervised", "machine", "learning", "in", "NLP", "to", "assess", "how", "European", "solidarity", "discourses", "changed", "before", "and", "after", "the", "COVID-19", "outbreak", "was", "declared", "a", "global", "pandemic", ".", "To", "this", "end", ",", "we", "annotate", "2.3k", "English", "and", "German", "tweets", "for", "(anti-)solidarity", "expressions", ",", "utilizing", "multiple", "human", "annotators", "and", "two", "annotation", "approaches", "(", "experts", "vs.", "crowds", ")", ".", "We", "use", "these", "annotations", "to", "train", "a", "BERT", "model", "with", "multiple", "data", "augmentation", "strategies", ".", "Our", "augmented", "BERT", "model", "that", "combines", "both", "expert", "and", "crowd", "annotations", "outperforms", "the", "baseline", "BERT", "classifier", "trained", "with", "expert", "annotations", "only", "by", "over", "25", "points", ",", "from", "58", "%", "macro-F1", "to", "almost", "85", "%", ".", "We", "use", "this", "high-quality", "model", "to", "automatically", "label", "over", "270k", "tweets", "between", "September", "2019", "and", "December", "2020", ".", "We", "then", "assess", "the", "automatically", "labeled", "data", "for", "how", "statements", "related", "to", "European", "(anti-)solidarity", "discourses", "developed", "over", "time", "and", "in", "relation", "to", "one", "another", ",", "before", "and", "during", "the", "COVID-19", "crisis", ".", "Our", "results", "show", "that", "solidarity", "became", "increasingly", "salient", "and", "contested", "during", "the", "crisis", ".", "While", "the", "number", "of", "solidarity", "tweets", "remained", "on", "a", "higher", "level", "and", "dominated", "the", "discourse", "in", "the", "scrutinized", "time", "frame", ",", "anti-solidarity", "tweets", "initially", "spiked", ",", "then", "decreased", "to", "(", "almost", ")", "pre-COVID-19", "values", "before", "rising", "to", "a", "stable", "higher", "level", "until", "the", "end", "of", "2020", "."], "entities": [{"type": "Operation", "start": 91, "end": 94, "text": "augmented BERT model", "sent_idx": 3}, {"type": "Effect", "start": 119, "end": 120, "text": "macro-F1", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--129"}
{"text": "The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.", "tokens": ["The", "availability", "of", "large-scale", "datasets", "has", "driven", "the", "development", "of", "neural", "models", "that", "create", "generic", "summaries", "from", "single", "or", "multiple", "documents", ".", "In", "this", "work", "we", "consider", "query", "focused", "summarization", "(", "QFS", ")", ",", "a", "task", "for", "which", "training", "data", "in", "the", "form", "of", "queries", ",", "documents", ",", "and", "summaries", "is", "not", "readily", "available", ".", "We", "propose", "to", "decompose", "QFS", "into", "(", "1", ")", "query", "modeling", "(", "i.e.", ",", "finding", "supportive", "evidence", "within", "a", "set", "of", "documents", "for", "a", "query", ")", "and", "(", "2", ")", "conditional", "language", "modeling", "(", "i.e.", ",", "summary", "generation", ")", ".", "We", "introduce", "MaRGE", ",", "a", "Masked", "ROUGE", "Regression", "framework", "for", "evidence", "estimation", "and", "ranking", "which", "relies", "on", "a", "unified", "representation", "for", "summaries", "and", "queries", ",", "so", "that", "summaries", "in", "generic", "data", "can", "be", "converted", "into", "proxy", "queries", "for", "learning", "a", "query", "model", ".", "Experiments", "across", "QFS", "benchmarks", "and", "query", "types", "show", "that", "our", "model", "achieves", "state-of-the-art", "performance", "despite", "learning", "from", "weak", "supervision", "."], "entities": [{"type": "Operation", "start": 97, "end": 98, "text": "MaRGE", "sent_idx": 3}, {"type": "Effect", "start": 151, "end": 152, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--475"}
{"text": "Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a validation metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning \u2014 we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.", "tokens": ["Language", "model", "fine-tuning", "is", "essential", "for", "modern", "natural", "language", "processing", ",", "but", "is", "computationally", "expensive", "and", "time-consuming", ".", "Further", ",", "the", "effectiveness", "of", "fine-tuning", "is", "limited", "by", "the", "inclusion", "of", "training", "examples", "that", "negatively", "affect", "performance", ".", "Here", "we", "present", "a", "general", "fine-tuning", "method", "that", "we", "call", "information", "gain", "filtration", "for", "improving", "the", "overall", "training", "efficiency", "and", "final", "performance", "of", "language", "model", "fine-tuning", ".", "We", "define", "the", "information", "gain", "of", "an", "example", "as", "the", "improvement", "on", "a", "validation", "metric", "after", "training", "on", "that", "example", ".", "A", "secondary", "learner", "is", "then", "trained", "to", "approximate", "this", "quantity", ".", "During", "fine-tuning", ",", "this", "learner", "selects", "informative", "examples", "and", "skips", "uninformative", "ones", ".", "We", "show", "that", "our", "method", "has", "consistent", "improvement", "across", "datasets", ",", "fine-tuning", "tasks", ",", "and", "language", "model", "architectures", ".", "For", "example", ",", "we", "achieve", "a", "median", "perplexity", "of", "54.0", "on", "a", "books", "dataset", "compared", "to", "57.3", "for", "standard", "fine-tuning", ".", "We", "present", "statistical", "evidence", "that", "offers", "insight", "into", "the", "improvements", "of", "our", "method", "over", "standard", "fine-tuning", ".", "The", "generality", "of", "our", "method", "leads", "us", "to", "propose", "a", "new", "paradigm", "for", "language", "model", "fine-tuning", "\u2014", "we", "encourage", "researchers", "to", "release", "pretrained", "secondary", "learners", "on", "common", "corpora", "to", "promote", "efficient", "and", "effective", "fine-tuning", ",", "thereby", "improving", "the", "performance", "and", "reducing", "the", "overall", "energy", "footprint", "of", "language", "model", "fine-tuning", "."], "entities": [{"type": "Operation", "start": 47, "end": 50, "text": "information gain filtration", "sent_idx": 2}, {"type": "Effect", "start": 134, "end": 136, "text": "median perplexity", "sent_idx": 7}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--87"}
{"text": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "tokens": ["Open-domain", "question", "answering", "can", "be", "reformulated", "as", "a", "phrase", "retrieval", "problem", ",", "without", "the", "need", "for", "processing", "documents", "on-demand", "during", "inference", "(", "Seo", "et", "al.", ",", "2019", ")", ".", "However", ",", "current", "phrase", "retrieval", "models", "heavily", "depend", "on", "sparse", "representations", "and", "still", "underperform", "retriever-reader", "approaches", ".", "In", "this", "work", ",", "we", "show", "for", "the", "first", "time", "that", "we", "can", "learn", "dense", "representations", "of", "phrases", "alone", "that", "achieve", "much", "stronger", "performance", "in", "open-domain", "QA", ".", "We", "present", "an", "effective", "method", "to", "learn", "phrase", "representations", "from", "the", "supervision", "of", "reading", "comprehension", "tasks", ",", "coupled", "with", "novel", "negative", "sampling", "methods", ".", "We", "also", "propose", "a", "query-side", "fine-tuning", "strategy", ",", "which", "can", "support", "transfer", "learning", "and", "reduce", "the", "discrepancy", "between", "training", "and", "inference", ".", "On", "five", "popular", "open-domain", "QA", "datasets", ",", "our", "model", "DensePhrases", "improves", "over", "previous", "phrase", "retrieval", "models", "by", "15%-25", "%", "absolute", "accuracy", "and", "matches", "the", "performance", "of", "state-of-the-art", "retriever-reader", "models", ".", "Our", "model", "is", "easy", "to", "parallelize", "due", "to", "pure", "dense", "representations", "and", "processes", "more", "than", "10", "questions", "per", "second", "on", "CPUs", ".", "Finally", ",", "we", "directly", "use", "our", "pre-indexed", "dense", "phrase", "representations", "for", "two", "slot", "filling", "tasks", ",", "showing", "the", "promise", "of", "utilizing", "DensePhrases", "as", "a", "dense", "knowledge", "base", "for", "downstream", "tasks", "."], "entities": [{"type": "Operation", "start": 129, "end": 130, "text": "DensePhrases", "sent_idx": 5}, {"type": "Effect", "start": 140, "end": 141, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--518"}
{"text": "Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.", "tokens": ["Despite", "the", "achievements", "of", "large-scale", "multimodal", "pre-training", "approaches", ",", "cross-modal", "retrieval", ",", "e.g.", ",", "image-text", "retrieval", ",", "remains", "a", "challenging", "task", ".", "To", "bridge", "the", "semantic", "gap", "between", "the", "two", "modalities", ",", "previous", "studies", "mainly", "focus", "on", "word-region", "alignment", "at", "the", "object", "level", ",", "lacking", "the", "matching", "between", "the", "linguistic", "relation", "among", "the", "words", "and", "the", "visual", "relation", "among", "the", "regions", ".", "The", "neglect", "of", "such", "relation", "consistency", "impairs", "the", "contextualized", "representation", "of", "image-text", "pairs", "and", "hinders", "the", "model", "performance", "and", "the", "interpretability", ".", "In", "this", "paper", ",", "we", "first", "propose", "a", "novel", "metric", ",", "Intra-modal", "Self-attention", "Distance", "(", "ISD", ")", ",", "to", "quantify", "the", "relation", "consistency", "by", "measuring", "the", "semantic", "distance", "between", "linguistic", "and", "visual", "relations", ".", "In", "response", ",", "we", "present", "Inter-modal", "Alignment", "on", "Intra-modal", "Self-attentions", "(", "IAIS", ")", ",", "a", "regularized", "training", "method", "to", "optimize", "the", "ISD", "and", "calibrate", "intra-modal", "self-attentions", "from", "the", "two", "modalities", "mutually", "via", "inter-modal", "alignment", ".", "The", "IAIS", "regularizer", "boosts", "the", "performance", "of", "prevailing", "models", "on", "Flickr30k", "and", "MS", "COCO", "datasets", "by", "a", "considerable", "margin", ",", "which", "demonstrates", "the", "superiority", "of", "our", "approach", "."], "entities": [{"type": "Operation", "start": 154, "end": 156, "text": "IAIS regularizer", "sent_idx": 5}, {"type": "Effect", "start": 158, "end": 159, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--43"}
{"text": "We propose a principled and efficient phrase-to-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include \"gappy phrases\" (such as French ne * pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.", "tokens": ["We", "propose", "a", "principled", "and", "efficient", "phrase-to-phrase", "alignment", "model", ",", "useful", "in", "machine", "translation", "as", "well", "as", "other", "related", "natural", "language", "processing", "problems", ".", "In", "a", "hidden", "semi-Markov", "model", ",", "word-to-phrase", "and", "phrase-to-word", "translations", "are", "modeled", "directly", "by", "the", "system", ".", "Agreement", "between", "two", "directional", "models", "encourages", "the", "selection", "of", "parsimonious", "phrasal", "alignments", ",", "avoiding", "the", "overfitting", "commonly", "encountered", "in", "unsupervised", "training", "with", "multi-word", "units", ".", "Expanding", "the", "state", "space", "to", "include", "\"", "gappy", "phrases", "\"", "(", "such", "as", "French", "ne", "*", "pas", ")", "makes", "the", "alignment", "space", "more", "symmetric", ";", "thus", ",", "it", "allows", "agreement", "between", "discontinuous", "alignments", ".", "The", "resulting", "system", "shows", "substantial", "improvements", "in", "both", "alignment", "quality", "and", "translation", "quality", "over", "word-based", "Hidden", "Markov", "Models", ",", "while", "maintaining", "asymptotically", "equivalent", "runtime", "."], "entities": [{"type": "Operation", "start": 6, "end": 9, "text": "phrase-to-phrase alignment model", "sent_idx": 0}, {"type": "Effect", "start": 108, "end": 110, "text": "alignment quality", "sent_idx": 4}, {"type": "Effect", "start": 111, "end": 113, "text": "translation quality", "sent_idx": 4}, {"type": "Effect", "start": 123, "end": 124, "text": "runtime", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}, {"type": "Affect", "head": 0, "tail": 3}], "id": "P11-1131"}
{"text": "Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn't scale to the thousands of relations encoded in Web text. \n \nThis paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations --- more than an order of magnitude greater than any previous approach --- with an average F1 score of 61%. Crucial to LUCHS's performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse.", "tokens": ["Many", "researchers", "are", "trying", "to", "use", "information", "extraction", "(", "IE", ")", "to", "create", "large-scale", "knowledge", "bases", "from", "natural", "language", "text", "on", "the", "Web", ".", "However", ",", "the", "primary", "approach", "(", "supervised", "learning", "of", "relation-specific", "extractors", ")", "requires", "manually-labeled", "training", "data", "for", "each", "relation", "and", "does", "n't", "scale", "to", "the", "thousands", "of", "relations", "encoded", "in", "Web", "text", ".", "\n \n", "This", "paper", "presents", "LUCHS", ",", "a", "self-supervised", ",", "relation-specific", "IE", "system", "which", "learns", "5025", "relations", "---", "more", "than", "an", "order", "of", "magnitude", "greater", "than", "any", "previous", "approach", "---", "with", "an", "average", "F1", "score", "of", "61", "%", ".", "Crucial", "to", "LUCHS", "'s", "performance", "is", "an", "automated", "system", "for", "dynamic", "lexicon", "learning", ",", "which", "allows", "it", "to", "learn", "accurately", "from", "heuristically-generated", "training", "data", ",", "which", "is", "often", "noisy", "and", "sparse", "."], "entities": [{"type": "Operation", "start": 61, "end": 62, "text": "LUCHS", "sent_idx": 2}, {"type": "Effect", "start": 89, "end": 91, "text": "F1 score", "sent_idx": 2}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1030"}
{"text": "The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.", "tokens": ["The", "rise", "of", "online", "communication", "platforms", "has", "been", "accompanied", "by", "some", "undesirable", "effects", ",", "such", "as", "the", "proliferation", "of", "aggressive", "and", "abusive", "behaviour", "online", ".", "Aiming", "to", "tackle", "this", "problem", ",", "the", "natural", "language", "processing", "(", "NLP", ")", "community", "has", "experimented", "with", "a", "range", "of", "techniques", "for", "abuse", "detection", ".", "While", "achieving", "substantial", "success", ",", "these", "methods", "have", "so", "far", "only", "focused", "on", "modelling", "the", "linguistic", "properties", "of", "the", "comments", "and", "the", "online", "communities", "of", "users", ",", "disregarding", "the", "emotional", "state", "of", "the", "users", "and", "how", "this", "might", "affect", "their", "language", ".", "The", "latter", "is", ",", "however", ",", "inextricably", "linked", "to", "abusive", "behaviour", ".", "In", "this", "paper", ",", "we", "present", "the", "first", "joint", "model", "of", "emotion", "and", "abusive", "language", "detection", ",", "experimenting", "in", "a", "multi-task", "learning", "framework", "that", "allows", "one", "task", "to", "inform", "the", "other", ".", "Our", "results", "demonstrate", "that", "incorporating", "affective", "features", "leads", "to", "significant", "improvements", "in", "abuse", "detection", "performance", "across", "datasets", "."], "entities": [{"type": "Operation", "start": 140, "end": 143, "text": "incorporating affective features", "sent_idx": 5}, {"type": "Effect", "start": 150, "end": 151, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--394"}
{"text": "While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.", "tokens": ["While", "sophisticated", "neural-based", "models", "have", "achieved", "remarkable", "success", "in", "Visual", "Question", "Answering", "(", "VQA", ")", ",", "these", "models", "tend", "to", "answer", "questions", "only", "according", "to", "superficial", "correlations", "between", "question", "and", "answer", ".", "Several", "recent", "approaches", "have", "been", "developed", "to", "address", "this", "language", "priors", "problem", ".", "However", ",", "most", "of", "them", "predict", "the", "correct", "answer", "according", "to", "one", "best", "output", "without", "checking", "the", "authenticity", "of", "answers", ".", "Besides", ",", "they", "only", "explore", "the", "interaction", "between", "image", "and", "question", ",", "ignoring", "the", "semantics", "of", "candidate", "answers", ".", "In", "this", "paper", ",", "we", "propose", "a", "select-and-rerank", "(", "SAR", ")", "progressive", "framework", "based", "on", "Visual", "Entailment", ".", "Specifically", ",", "we", "first", "select", "the", "candidate", "answers", "relevant", "to", "the", "question", "or", "the", "image", ",", "then", "we", "rerank", "the", "candidate", "answers", "by", "a", "visual", "entailment", "task", ",", "which", "verifies", "whether", "the", "image", "semantically", "entails", "the", "synthetic", "statement", "of", "the", "question", "and", "each", "candidate", "answer", ".", "Experimental", "results", "show", "the", "effectiveness", "of", "our", "proposed", "framework", ",", "which", "establishes", "a", "new", "state-of-the-art", "accuracy", "on", "VQA-CP", "v2", "with", "a", "7.55", "%", "improvement", "."], "entities": [{"type": "Operation", "start": 92, "end": 98, "text": "select-and-rerank (SAR) progressive framework", "sent_idx": 4}, {"type": "Effect", "start": 164, "end": 165, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--317"}
{"text": "Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.", "tokens": ["Encoder", "pre-training", "is", "promising", "in", "end-to-end", "Speech", "Translation", "(", "ST", ")", ",", "given", "the", "fact", "that", "speech-to-translation", "data", "is", "scarce", ".", "But", "ST", "encoders", "are", "not", "simple", "instances", "of", "Automatic", "Speech", "Recognition", "(", "ASR", ")", "or", "Machine", "Translation", "(", "MT", ")", "encoders", ".", "For", "example", ",", "we", "find", "that", "ASR", "encoders", "lack", "the", "global", "context", "representation", ",", "which", "is", "necessary", "for", "translation", ",", "whereas", "MT", "encoders", "are", "not", "designed", "to", "deal", "with", "long", "but", "locally", "attentive", "acoustic", "sequences", ".", "In", "this", "work", ",", "we", "propose", "a", "Stacked", "Acoustic-and-Textual", "Encoding", "(", "SATE", ")", "method", "for", "speech", "translation", ".", "Our", "encoder", "begins", "with", "processing", "the", "acoustic", "sequence", "as", "usual", ",", "but", "later", "behaves", "more", "like", "an", "MT", "encoder", "for", "a", "global", "representation", "of", "the", "input", "sequence", ".", "In", "this", "way", ",", "it", "is", "straightforward", "to", "incorporate", "the", "pre-trained", "models", "into", "the", "system", ".", "Also", ",", "we", "develop", "an", "adaptor", "module", "to", "alleviate", "the", "representation", "inconsistency", "between", "the", "pre-trained", "ASR", "encoder", "and", "MT", "encoder", ",", "and", "develop", "a", "multi-teacher", "knowledge", "distillation", "method", "to", "preserve", "the", "pre-training", "knowledge", ".", "Experimental", "results", "on", "the", "LibriSpeech", "En-Fr", "and", "MuST-C", "En-De", "ST", "tasks", "show", "that", "our", "method", "achieves", "state-of-the-art", "BLEU", "scores", "of", "18.3", "and", "25.2", ".", "To", "our", "knowledge", ",", "we", "are", "the", "first", "to", "develop", "an", "end-to-end", "ST", "system", "that", "achieves", "comparable", "or", "even", "better", "BLEU", "performance", "than", "the", "cascaded", "ST", "counterpart", "when", "large-scale", "ASR", "and", "MT", "data", "is", "available", "."], "entities": [{"type": "Operation", "start": 86, "end": 93, "text": "Stacked Acoustic-and-Textual Encoding (SATE) method", "sent_idx": 3}, {"type": "Effect", "start": 192, "end": 194, "text": "BLEU scores", "sent_idx": 7}, {"type": "Operation", "start": 210, "end": 213, "text": "end-to-end ST system", "sent_idx": 8}, {"type": "Effect", "start": 219, "end": 220, "text": "BLEU", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--204"}
{"text": "The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.", "tokens": ["The", "success", "of", "a", "text", "simplification", "system", "heavily", "depends", "on", "the", "quality", "and", "quantity", "of", "complex-simple", "sentence", "pairs", "in", "the", "training", "corpus", ",", "which", "are", "extracted", "by", "aligning", "sentences", "between", "parallel", "articles", ".", "To", "evaluate", "and", "improve", "sentence", "alignment", "quality", ",", "we", "create", "two", "manually", "annotated", "sentence-aligned", "datasets", "from", "two", "commonly", "used", "text", "simplification", "corpora", ",", "Newsela", "and", "Wikipedia", ".", "We", "propose", "a", "novel", "neural", "CRF", "alignment", "model", "which", "not", "only", "leverages", "the", "sequential", "nature", "of", "sentences", "in", "parallel", "documents", "but", "also", "utilizes", "a", "neural", "sentence", "pair", "model", "to", "capture", "semantic", "similarity", ".", "Experiments", "demonstrate", "that", "our", "proposed", "approach", "outperforms", "all", "the", "previous", "work", "on", "monolingual", "sentence", "alignment", "task", "by", "more", "than", "5", "points", "in", "F1", ".", "We", "apply", "our", "CRF", "aligner", "to", "construct", "two", "new", "text", "simplification", "datasets", ",", "Newsela-Auto", "and", "Wiki-Auto", ",", "which", "are", "much", "larger", "and", "of", "better", "quality", "compared", "to", "the", "existing", "datasets", ".", "A", "Transformer-based", "seq2seq", "model", "trained", "on", "our", "datasets", "establishes", "a", "new", "state-of-the-art", "for", "text", "simplification", "in", "both", "automatic", "and", "human", "evaluation", "."], "entities": [{"type": "Operation", "start": 65, "end": 68, "text": "CRF alignment model", "sent_idx": 2}, {"type": "Effect", "start": 115, "end": 116, "text": "F1", "sent_idx": 3}, {"type": "Operation", "start": 120, "end": 122, "text": "CRF aligner", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 142, "text": "quality", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2020--acl-main--709"}
{"text": "In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for \u201czero-shot\u201d open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.", "tokens": ["In", "many", "documents", ",", "such", "as", "semi-structured", "webpages", ",", "textual", "semantics", "are", "augmented", "with", "additional", "information", "conveyed", "using", "visual", "elements", "including", "layout", ",", "font", "size", ",", "and", "color", ".", "Prior", "work", "on", "information", "extraction", "from", "semi-structured", "websites", "has", "required", "learning", "an", "extraction", "model", "specific", "to", "a", "given", "template", "via", "either", "manually", "labeled", "or", "distantly", "supervised", "data", "from", "that", "template", ".", "In", "this", "work", ",", "we", "propose", "a", "solution", "for", "\u201c", "zero-shot", "\u201d", "open-domain", "relation", "extraction", "from", "webpages", "with", "a", "previously", "unseen", "template", ",", "including", "from", "websites", "with", "little", "overlap", "with", "existing", "sources", "of", "knowledge", "for", "distant", "supervision", "and", "websites", "in", "entirely", "new", "subject", "verticals", ".", "Our", "model", "uses", "a", "graph", "neural", "network-based", "approach", "to", "build", "a", "rich", "representation", "of", "text", "fields", "on", "a", "webpage", "and", "the", "relationships", "between", "them", ",", "enabling", "generalization", "to", "new", "templates", ".", "Experiments", "show", "this", "approach", "provides", "a", "31", "%", "F1", "gain", "over", "a", "baseline", "for", "zero-shot", "extraction", "in", "a", "new", "subject", "vertical", "."], "entities": [{"type": "Operation", "start": 84, "end": 97, "text": "from websites with little overlap with existing sources of knowledge for distant supervision", "sent_idx": 2}, {"type": "Effect", "start": 144, "end": 145, "text": "F1", "sent_idx": 4}, {"type": "Operation", "start": 98, "end": 104, "text": "websites in entirely new subject verticals", "sent_idx": 2}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2020--acl-main--721"}
{"text": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict word-level and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.", "tokens": ["State-of-the-art", "statistical", "machine", "translation", "(", "MT", ")", "systems", "have", "made", "significant", "progress", "towards", "producing", "user-acceptable", "translation", "output", ".", "However", ",", "there", "is", "still", "no", "efficient", "way", "for", "MT", "systems", "to", "inform", "users", "which", "words", "are", "likely", "translated", "correctly", "and", "how", "confident", "it", "is", "about", "the", "whole", "sentence", ".", "We", "propose", "a", "novel", "framework", "to", "predict", "word-level", "and", "sentence-level", "MT", "errors", "with", "a", "large", "number", "of", "novel", "features", ".", "Experimental", "results", "show", "that", "the", "MT", "error", "prediction", "accuracy", "is", "increased", "from", "69.1", "to", "72.2", "in", "F-score", ".", "The", "Pearson", "correlation", "between", "the", "proposed", "confidence", "measure", "and", "the", "human-targeted", "translation", "edit", "rate", "(", "HTER", ")", "is", "0.6", ".", "Improvements", "between", "0.4", "and", "0.9", "TER", "reduction", "are", "obtained", "with", "the", "n-best", "list", "reranking", "task", "using", "the", "proposed", "confidence", "measure", ".", "Also", ",", "we", "present", "a", "visualization", "prototype", "of", "MT", "errors", "at", "the", "word", "and", "sentence", "levels", "with", "the", "objective", "to", "improve", "post-editor", "productivity", "."], "entities": [{"type": "Operation", "start": 131, "end": 134, "text": "a visualization prototype", "sent_idx": 6}, {"type": "Effect", "start": 149, "end": 150, "text": "productivity", "sent_idx": 6}, {"type": "Operation", "start": 50, "end": 53, "text": "a novel framework", "sent_idx": 2}, {"type": "Effect", "start": 76, "end": 77, "text": "accuracy", "sent_idx": 3}, {"type": "Effect", "start": 84, "end": 85, "text": "F-score", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 4}], "id": "P11-1022"}
{"text": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents. Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model. We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points.", "tokens": ["Recent", "work", "on", "training", "neural", "retrievers", "for", "open-domain", "question", "answering", "(", "OpenQA", ")", "has", "employed", "both", "supervised", "and", "unsupervised", "approaches", ".", "However", ",", "it", "remains", "unclear", "how", "unsupervised", "and", "supervised", "methods", "can", "be", "used", "most", "effectively", "for", "neural", "retrievers", ".", "In", "this", "work", ",", "we", "systematically", "study", "retriever", "pre-training", ".", "We", "first", "propose", "an", "approach", "of", "unsupervised", "pre-training", "with", "the", "Inverse", "Cloze", "Task", "and", "masked", "salient", "spans", ",", "followed", "by", "supervised", "finetuning", "using", "question-context", "pairs", ".", "This", "approach", "leads", "to", "absolute", "gains", "of", "2", "+", "points", "over", "the", "previous", "best", "result", "in", "the", "top-20", "retrieval", "accuracy", "on", "Natural", "Questions", "and", "TriviaQA", "datasets", ".", "We", "next", "explore", "two", "approaches", "for", "end-to-end", "training", "of", "the", "reader", "and", "retriever", "components", "in", "OpenQA", "models", ",", "which", "differ", "in", "the", "manner", "the", "reader", "ingests", "the", "retrieved", "documents", ".", "Our", "experiments", "demonstrate", "the", "effectiveness", "of", "these", "approaches", "as", "we", "obtain", "state-of-the-art", "results", ".", "On", "the", "Natural", "Questions", "dataset", ",", "we", "obtain", "a", "top-20", "retrieval", "accuracy", "of", "84", "%", ",", "an", "improvement", "of", "5", "points", "over", "the", "recent", "DPR", "model", ".", "We", "also", "achieve", "good", "results", "on", "answer", "extraction", ",", "outperforming", "recent", "models", "like", "REALM", "and", "RAG", "by", "3", "+", "points", "."], "entities": [{"type": "Operation", "start": 56, "end": 67, "text": "unsupervised pre-training with the Inverse Cloze Task and masked salient spans", "sent_idx": 3}, {"type": "Effect", "start": 93, "end": 96, "text": "top-20 retrieval accuracy", "sent_idx": 4}, {"type": "Operation", "start": 70, "end": 75, "text": "supervised finetuning using question-context pairs", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--519"}
{"text": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.", "tokens": ["Temporal", "Knowledge", "Graphs", "(", "Temporal", "KGs", ")", "extend", "regular", "Knowledge", "Graphs", "by", "providing", "temporal", "scopes", "(", "start", "and", "end", "times", ")", "on", "each", "edge", "in", "the", "KG", ".", "While", "Question", "Answering", "over", "KG", "(", "KGQA", ")", "has", "received", "some", "attention", "from", "the", "research", "community", ",", "QA", "over", "Temporal", "KGs", "(", "Temporal", "KGQA", ")", "is", "a", "relatively", "unexplored", "area", ".", "Lack", "of", "broad", "coverage", "datasets", "has", "been", "another", "factor", "limiting", "progress", "in", "this", "area", ".", "We", "address", "this", "challenge", "by", "presenting", "CRONQUESTIONS", ",", "the", "largest", "known", "Temporal", "KGQA", "dataset", ",", "clearly", "stratified", "into", "buckets", "of", "structural", "complexity", ".", "CRONQUESTIONS", "expands", "the", "only", "known", "previous", "dataset", "by", "a", "factor", "of", "340x", ".", "We", "find", "that", "various", "state-of-the-art", "KGQA", "methods", "fall", "far", "short", "of", "the", "desired", "performance", "on", "this", "new", "dataset", ".", "In", "response", ",", "we", "also", "propose", "CRONKGQA", ",", "a", "transformer-based", "solution", "that", "exploits", "recent", "advances", "in", "Temporal", "KG", "embeddings", ",", "and", "achieves", "performance", "superior", "to", "all", "baselines", ",", "with", "an", "increase", "of", "120", "%", "in", "accuracy", "over", "the", "next", "best", "performing", "method", ".", "Through", "extensive", "experiments", ",", "we", "give", "detailed", "insights", "into", "the", "workings", "of", "CRONKGQA", ",", "as", "well", "as", "situations", "where", "significant", "further", "improvements", "appear", "possible", ".", "In", "addition", "to", "the", "dataset", ",", "we", "have", "released", "our", "code", "as", "well", "."], "entities": [{"type": "Operation", "start": 135, "end": 136, "text": "CRONKGQA", "sent_idx": 6}, {"type": "Effect", "start": 151, "end": 152, "text": "performance", "sent_idx": 6}, {"type": "Operation", "start": 141, "end": 148, "text": "exploits recent advances in Temporal KG embeddings", "sent_idx": 6}, {"type": "Effect", "start": 164, "end": 165, "text": "accuracy", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--520"}
{"text": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.", "tokens": ["Non-autoregressive", "(", "NAR", ")", "models", "generate", "all", "the", "tokens", "of", "a", "sequence", "in", "parallel", ",", "resulting", "in", "faster", "generation", "speed", "compared", "to", "their", "autoregressive", "(", "AR", ")", "counterparts", "but", "at", "the", "cost", "of", "lower", "accuracy", ".", "Different", "techniques", "including", "knowledge", "distillation", "and", "source-target", "alignment", "have", "been", "proposed", "to", "bridge", "the", "gap", "between", "AR", "and", "NAR", "models", "in", "various", "tasks", "such", "as", "neural", "machine", "translation", "(", "NMT", ")", ",", "automatic", "speech", "recognition", "(", "ASR", ")", ",", "and", "text", "to", "speech", "(", "TTS", ")", ".", "With", "the", "help", "of", "those", "techniques", ",", "NAR", "models", "can", "catch", "up", "with", "the", "accuracy", "of", "AR", "models", "in", "some", "tasks", "but", "not", "in", "some", "others", ".", "In", "this", "work", ",", "we", "conduct", "a", "study", "to", "understand", "the", "difficulty", "of", "NAR", "sequence", "generation", "and", "try", "to", "answer", ":", "(", "1", ")", "Why", "NAR", "models", "can", "catch", "up", "with", "AR", "models", "in", "some", "tasks", "but", "not", "all", "?", "(", "2", ")", "Why", "techniques", "like", "knowledge", "distillation", "and", "source-target", "alignment", "can", "help", "NAR", "models", ".", "Since", "the", "main", "difference", "between", "AR", "and", "NAR", "models", "is", "that", "NAR", "models", "do", "not", "use", "dependency", "among", "target", "tokens", "while", "AR", "models", "do", ",", "intuitively", "the", "difficulty", "of", "NAR", "sequence", "generation", "heavily", "depends", "on", "the", "strongness", "of", "dependency", "among", "target", "tokens", ".", "To", "quantify", "such", "dependency", ",", "we", "propose", "an", "analysis", "model", "called", "CoMMA", "to", "characterize", "the", "difficulty", "of", "different", "NAR", "sequence", "generation", "tasks", ".", "We", "have", "several", "interesting", "findings", ":", "1", ")", "Among", "the", "NMT", ",", "ASR", "and", "TTS", "tasks", ",", "ASR", "has", "the", "most", "target-token", "dependency", "while", "TTS", "has", "the", "least", ".", "2", ")", "Knowledge", "distillation", "reduces", "the", "target-token", "dependency", "in", "target", "sequence", "and", "thus", "improves", "the", "accuracy", "of", "NAR", "models", ".", "3", ")", "Source-target", "alignment", "constraint", "encourages", "dependency", "of", "a", "target", "token", "on", "source", "tokens", "and", "thus", "eases", "the", "training", "of", "NAR", "models", "."], "entities": [{"type": "Operation", "start": 263, "end": 265, "text": "Knowledge distillation", "sent_idx": 7}, {"type": "Effect", "start": 276, "end": 277, "text": "accuracy", "sent_idx": 7}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--15"}
{"text": "We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar 'translation example' retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English--Chinese technical documents.", "tokens": ["We", "present", "a", "discriminative", "learning", "method", "to", "improve", "the", "consistency", "of", "translations", "in", "phrase-based", "Statistical", "Machine", "Translation", "(", "SMT", ")", "systems", ".", "Our", "method", "is", "inspired", "by", "Translation", "Memory", "(", "TM", ")", "systems", "which", "are", "widely", "used", "by", "human", "translators", "in", "industrial", "settings", ".", "We", "constrain", "the", "translation", "of", "an", "input", "sentence", "using", "the", "most", "similar", "'", "translation", "example", "'", "retrieved", "from", "the", "TM", ".", "Differently", "from", "previous", "research", "which", "used", "simple", "fuzzy", "match", "thresholds", ",", "these", "constraints", "are", "imposed", "using", "discriminative", "learning", "to", "optimise", "the", "translation", "performance", ".", "We", "observe", "that", "using", "this", "method", "can", "benefit", "the", "SMT", "system", "by", "not", "only", "producing", "consistent", "translations", ",", "but", "also", "improved", "translation", "outputs", ".", "We", "report", "a", "0.9", "point", "improvement", "in", "terms", "of", "BLEU", "score", "on", "English", "--", "Chinese", "technical", "documents", "."], "entities": [{"type": "Operation", "start": 3, "end": 6, "text": "discriminative learning method", "sent_idx": 0}, {"type": "Effect", "start": 122, "end": 124, "text": "BLEU score", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P11-1124"}
{"text": "We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus --- nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.", "tokens": ["We", "show", "how", "web", "mark-up", "can", "be", "used", "to", "improve", "unsupervised", "dependency", "parsing", ".", "Starting", "from", "raw", "bracketings", "of", "four", "common", "HTML", "tags", "(", "anchors", ",", "bold", ",", "italics", "and", "underlines", ")", ",", "we", "refine", "approximate", "partial", "phrase", "boundaries", "to", "yield", "accurate", "parsing", "constraints", ".", "Conversion", "procedures", "fall", "out", "of", "our", "linguistic", "analysis", "of", "a", "newly", "available", "million-word", "hyper-text", "corpus", ".", "We", "demonstrate", "that", "derived", "constraints", "aid", "grammar", "induction", "by", "training", "Klein", "and", "Manning", "'s", "Dependency", "Model", "with", "Valence", "(", "DMV", ")", "on", "this", "data", "set", ":", "parsing", "accuracy", "on", "Section", "23", "(", "all", "sentences", ")", "of", "the", "Wall", "Street", "Journal", "corpus", "jumps", "to", "50.4", "%", ",", "beating", "previous", "state-of-the-art", "by", "more", "than", "5", "%", ".", "Web-scale", "experiments", "show", "that", "the", "DMV", ",", "perhaps", "because", "it", "is", "unlexicalized", ",", "does", "not", "benefit", "from", "orders", "of", "magnitude", "more", "annotated", "but", "noisier", "data", ".", "Our", "model", ",", "trained", "on", "a", "single", "blog", ",", "generalizes", "to", "53.3", "%", "accuracy", "out-of-domain", ",", "against", "the", "Brown", "corpus", "---", "nearly", "10", "%", "higher", "than", "the", "previous", "published", "best", ".", "The", "fact", "that", "web", "mark-up", "strongly", "correlates", "with", "syntactic", "structure", "may", "have", "broad", "applicability", "in", "NLP", "."], "entities": [{"type": "Operation", "start": 71, "end": 82, "text": "Klein and Manning's Dependency Model with Valence (DMV)", "sent_idx": 3}, {"type": "Effect", "start": 88, "end": 89, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1130"}
{"text": "Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.", "tokens": ["Cross-document", "coreference", ",", "the", "task", "of", "grouping", "all", "the", "mentions", "of", "each", "entity", "in", "a", "document", "collection", ",", "arises", "in", "information", "extraction", "and", "automated", "knowledge", "base", "construction", ".", "For", "large", "collections", ",", "it", "is", "clearly", "impractical", "to", "consider", "all", "possible", "groupings", "of", "mentions", "into", "distinct", "entities", ".", "To", "solve", "the", "problem", "we", "propose", "two", "ideas", ":", "(", "a", ")", "a", "distributed", "inference", "technique", "that", "uses", "parallelism", "to", "enable", "large", "scale", "processing", ",", "and", "(", "b", ")", "a", "hierarchical", "model", "of", "coreference", "that", "represents", "uncertainty", "over", "multiple", "granularities", "of", "entities", "to", "facilitate", "more", "effective", "approximate", "inference", ".", "To", "evaluate", "these", "ideas", ",", "we", "constructed", "a", "labeled", "corpus", "of", "1.5", "million", "disambiguated", "mentions", "in", "Web", "pages", "by", "selecting", "link", "anchors", "referring", "to", "Wikipedia", "entities", ".", "We", "show", "that", "the", "combination", "of", "the", "hierarchical", "model", "with", "distributed", "inference", "quickly", "obtains", "high", "accuracy", "(", "with", "error", "reduction", "of", "38", "%", ")", "on", "this", "large", "dataset", ",", "demonstrating", "the", "scalability", "of", "our", "approach", "."], "entities": [{"type": "Operation", "start": 127, "end": 135, "text": "combination of the hierarchical model with distributed inference", "sent_idx": 4}, {"type": "Effect", "start": 138, "end": 139, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 141, "end": 142, "text": "error", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 0, "tail": 2}], "id": "P11-1080"}
{"text": "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model\u2019s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.", "tokens": ["In", "this", "work", ",", "we", "provide", "a", "systematic", "and", "comprehensive", "empirical", "comparison", "of", "pretrained", "multilingual", "language", "models", "versus", "their", "monolingual", "counterparts", "with", "regard", "to", "their", "monolingual", "task", "performance", ".", "We", "study", "a", "set", "of", "nine", "typologically", "diverse", "languages", "with", "readily", "available", "pretrained", "monolingual", "models", "on", "a", "set", "of", "five", "diverse", "monolingual", "downstream", "tasks", ".", "We", "first", "aim", "to", "establish", ",", "via", "fair", "and", "controlled", "comparisons", ",", "if", "a", "gap", "between", "the", "multilingual", "and", "the", "corresponding", "monolingual", "representation", "of", "that", "language", "exists", ",", "and", "subsequently", "investigate", "the", "reason", "for", "any", "performance", "difference", ".", "To", "disentangle", "conflating", "factors", ",", "we", "train", "new", "monolingual", "models", "on", "the", "same", "data", ",", "with", "monolingually", "and", "multilingually", "trained", "tokenizers", ".", "We", "find", "that", "while", "the", "pretraining", "data", "size", "is", "an", "important", "factor", ",", "a", "designated", "monolingual", "tokenizer", "plays", "an", "equally", "important", "role", "in", "the", "downstream", "performance", ".", "Our", "results", "show", "that", "languages", "that", "are", "adequately", "represented", "in", "the", "multilingual", "model", "\u2019s", "vocabulary", "exhibit", "negligible", "performance", "decreases", "over", "their", "monolingual", "counterparts", ".", "We", "further", "find", "that", "replacing", "the", "original", "multilingual", "tokenizer", "with", "the", "specialized", "monolingual", "tokenizer", "improves", "the", "downstream", "performance", "of", "the", "multilingual", "model", "for", "almost", "every", "task", "and", "language", "."], "entities": [{"type": "Operation", "start": 169, "end": 179, "text": "replacing the original multilingual tokenizer with the specialized monolingual tokenizer", "sent_idx": 6}, {"type": "Effect", "start": 181, "end": 183, "text": "downstream performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--243"}
{"text": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.", "tokens": ["Document-level", "event", "extraction", "aims", "to", "recognize", "event", "information", "from", "a", "whole", "piece", "of", "article", ".", "Existing", "methods", "are", "not", "effective", "due", "to", "two", "challenges", "of", "this", "task", ":", "a", ")", "the", "target", "event", "arguments", "are", "scattered", "across", "sentences", ";", "b", ")", "the", "correlation", "among", "events", "in", "a", "document", "is", "non-trivial", "to", "model", ".", "In", "this", "paper", ",", "we", "propose", "Heterogeneous", "Graph-based", "Interaction", "Model", "with", "a", "Tracker", "(", "GIT", ")", "to", "solve", "the", "aforementioned", "two", "challenges", ".", "For", "the", "first", "challenge", ",", "GIT", "constructs", "a", "heterogeneous", "graph", "interaction", "network", "to", "capture", "global", "interactions", "among", "different", "sentences", "and", "entity", "mentions", ".", "For", "the", "second", ",", "GIT", "introduces", "a", "Tracker", "module", "to", "track", "the", "extracted", "events", "and", "hence", "capture", "the", "interdependency", "among", "the", "events", ".", "Experiments", "on", "a", "large-scale", "dataset", "(", "Zheng", "et", "al", ",", "2019", ")", "show", "GIT", "outperforms", "the", "previous", "methods", "by", "2.8", "F1", ".", "Further", "analysis", "reveals", "is", "effective", "in", "extracting", "multiple", "correlated", "events", "and", "event", "arguments", "that", "scatter", "across", "the", "document", "."], "entities": [{"type": "Operation", "start": 135, "end": 136, "text": "GIT", "sent_idx": 6}, {"type": "Effect", "start": 142, "end": 143, "text": "F1", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--274"}
{"text": "In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word\u2019s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model \u2013 TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.", "tokens": ["In", "this", "paper", ",", "we", "explore", "the", "slot", "tagging", "with", "only", "a", "few", "labeled", "support", "sentences", "(", "a.k.a", ".", "few-shot", ")", ".", "Few-shot", "slot", "tagging", "faces", "a", "unique", "challenge", "compared", "to", "the", "other", "fewshot", "classification", "problems", "as", "it", "calls", "for", "modeling", "the", "dependencies", "between", "labels", ".", "But", "it", "is", "hard", "to", "apply", "previously", "learned", "label", "dependencies", "to", "an", "unseen", "domain", ",", "due", "to", "the", "discrepancy", "of", "label", "sets", ".", "To", "tackle", "this", ",", "we", "introduce", "a", "collapsed", "dependency", "transfer", "mechanism", "into", "the", "conditional", "random", "field", "(", "CRF", ")", "to", "transfer", "abstract", "label", "dependency", "patterns", "as", "transition", "scores", ".", "In", "the", "few-shot", "setting", ",", "the", "emission", "score", "of", "CRF", "can", "be", "calculated", "as", "a", "word", "\u2019s", "similarity", "to", "the", "representation", "of", "each", "label", ".", "To", "calculate", "such", "similarity", ",", "we", "propose", "a", "Label-enhanced", "Task-Adaptive", "Projection", "Network", "(", "L-TapNet", ")", "based", "on", "the", "state-of-the-art", "few-shot", "classification", "model", "\u2013", "TapNet", ",", "by", "leveraging", "label", "name", "semantics", "in", "representing", "labels", ".", "Experimental", "results", "show", "that", "our", "model", "significantly", "outperforms", "the", "strongest", "few-shot", "learning", "baseline", "by", "14.64", "F1", "scores", "in", "the", "one-shot", "setting", "."], "entities": [{"type": "Operation", "start": 74, "end": 88, "text": "introduce a collapsed dependency transfer mechanism into the conditional random field (CRF)", "sent_idx": 2}, {"type": "Effect", "start": 172, "end": 174, "text": "F1 scores", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--128"}
{"text": "Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic consistency constraint is presented to improve the translation performance, allowing us directly to discard decoder and only embraces encoder of Transformer. This could contribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Moreover, the convolution block is utilized to further highlight explicit interactions among those translations. For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies. The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance. Additionally, CTFN still maintains robustness when considering missing modality.", "tokens": ["Multimodal", "sentiment", "analysis", "is", "the", "challenging", "research", "area", "that", "attends", "to", "the", "fusion", "of", "multiple", "heterogeneous", "modalities", ".", "The", "main", "challenge", "is", "the", "occurrence", "of", "some", "missing", "modalities", "during", "the", "multimodal", "fusion", "procedure", ".", "However", ",", "the", "existing", "techniques", "require", "all", "modalities", "as", "input", ",", "thus", "are", "sensitive", "to", "missing", "modalities", "at", "predicting", "time", ".", "In", "this", "work", ",", "the", "coupled-translation", "fusion", "network", "(", "CTFN", ")", "is", "firstly", "proposed", "to", "model", "bi-direction", "interplay", "via", "couple", "learning", ",", "ensuring", "the", "robustness", "in", "respect", "to", "missing", "modalities", ".", "Specifically", ",", "the", "cyclic", "consistency", "constraint", "is", "presented", "to", "improve", "the", "translation", "performance", ",", "allowing", "us", "directly", "to", "discard", "decoder", "and", "only", "embraces", "encoder", "of", "Transformer", ".", "This", "could", "contribute", "to", "a", "much", "lighter", "model", ".", "Due", "to", "the", "couple", "learning", ",", "CTFN", "is", "able", "to", "conduct", "bi-direction", "cross-modality", "intercorrelation", "parallelly", ".", "Based", "on", "CTFN", ",", "a", "hierarchical", "architecture", "is", "further", "established", "to", "exploit", "multiple", "bi-direction", "translations", ",", "leading", "to", "double", "multimodal", "fusing", "embeddings", "compared", "with", "traditional", "translation", "methods", ".", "Moreover", ",", "the", "convolution", "block", "is", "utilized", "to", "further", "highlight", "explicit", "interactions", "among", "those", "translations", ".", "For", "evaluation", ",", "CTFN", "was", "verified", "on", "two", "multimodal", "benchmarks", "with", "extensive", "ablation", "studies", ".", "The", "experiments", "demonstrate", "that", "the", "proposed", "framework", "achieves", "state-of-the-art", "or", "often", "competitive", "performance", ".", "Additionally", ",", "CTFN", "still", "maintains", "robustness", "when", "considering", "missing", "modality", "."], "entities": [{"type": "Operation", "start": 89, "end": 92, "text": "cyclic consistency constraint", "sent_idx": 4}, {"type": "Effect", "start": 97, "end": 99, "text": "translation performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--412"}
{"text": "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods.", "tokens": ["Professional", "summaries", "are", "written", "with", "document-level", "information", ",", "such", "as", "the", "theme", "of", "the", "document", ",", "in", "mind", ".", "This", "is", "in", "contrast", "with", "most", "seq2seq", "decoders", "which", "simultaneously", "learn", "to", "focus", "on", "salient", "content", ",", "while", "deciding", "what", "to", "generate", ",", "at", "each", "decoding", "step", ".", "With", "the", "motivation", "to", "narrow", "this", "gap", ",", "we", "introduce", "Focus", "Attention", "Mechanism", ",", "a", "simple", "yet", "effective", "method", "to", "encourage", "decoders", "to", "proactively", "generate", "tokens", "that", "are", "similar", "or", "topical", "to", "the", "input", "document", ".", "Further", ",", "we", "propose", "a", "Focus", "Sampling", "method", "to", "enable", "generation", "of", "diverse", "summaries", ",", "an", "area", "currently", "understudied", "in", "summarization", ".", "When", "evaluated", "on", "the", "BBC", "extreme", "summarization", "task", ",", "two", "state-of-the-art", "models", "augmented", "with", "Focus", "Attention", "generate", "summaries", "that", "are", "closer", "to", "the", "target", "and", "more", "faithful", "to", "their", "input", "documents", ",", "outperforming", "their", "vanilla", "counterparts", "on", "ROUGE", "and", "multiple", "faithfulness", "measures", ".", "We", "also", "empirically", "demonstrate", "that", "Focus", "Sampling", "is", "more", "effective", "in", "generating", "diverse", "and", "faithful", "summaries", "than", "top-k", "or", "nucleus", "sampling-based", "decoding", "methods", "."], "entities": [{"type": "Operation", "start": 116, "end": 121, "text": "models augmented with Focus Attention", "sent_idx": 4}, {"type": "Effect", "start": 142, "end": 143, "text": "ROUGE", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--474"}
{"text": "We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.", "tokens": ["We", "propose", "a", "novel", "method", "for", "hierarchical", "entity", "classification", "that", "embraces", "ontological", "structure", "at", "both", "training", "and", "during", "prediction", ".", "At", "training", ",", "our", "novel", "multi-level", "learning-to-rank", "loss", "compares", "positive", "types", "against", "negative", "siblings", "according", "to", "the", "type", "tree", ".", "During", "prediction", ",", "we", "define", "a", "coarse-to-fine", "decoder", "that", "restricts", "viable", "candidates", "at", "each", "level", "of", "the", "ontology", "based", "on", "already", "predicted", "parent", "type(s", ")", ".", "Our", "approach", "significantly", "outperform", "prior", "work", "on", "strict", "accuracy", ",", "demonstrating", "the", "effectiveness", "of", "our", "method", "."], "entities": [{"type": "Operation", "start": 10, "end": 19, "text": "embraces ontological structure at both training and during prediction", "sent_idx": 0}, {"type": "Effect", "start": 73, "end": 75, "text": "strict accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--749"}
{"text": "Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs. In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly. We conclude by discussing how our findings provide insights for system optimization and usage.", "tokens": ["Research", "on", "the", "application", "of", "NLP", "in", "symbol-based", "Augmentative", "and", "Alternative", "Communication", "(", "AAC", ")", "tools", "for", "improving", "social", "interaction", "support", "is", "scarce", ".", "We", "contribute", "a", "novel", "method", "for", "generating", "context-related", "vocabulary", "from", "photographs", "of", "personally", "relevant", "events", "aimed", "at", "supporting", "people", "with", "language", "impairments", "in", "retelling", "their", "past", "experiences", ".", "Performance", "was", "calculated", "with", "information", "retrieval", "concepts", "on", "the", "relevance", "of", "vocabulary", "generated", "for", "communicating", "a", "corpus", "of", "9730", "narrative", "phrases", "about", "events", "depicted", "in", "1946", "photographs", ".", "In", "comparison", "to", "a", "baseline", "generation", "composed", "of", "frequent", "English", "words", ",", "our", "method", "generated", "vocabulary", "with", "a", "4.6", "gain", "in", "mean", "average", "precision", ",", "regardless", "of", "the", "level", "of", "contextual", "information", "in", "the", "input", "photographs", ",", "and", "6.9", "for", "photographs", "in", "which", "contextual", "information", "was", "extracted", "correctly", ".", "We", "conclude", "by", "discussing", "how", "our", "findings", "provide", "insights", "for", "system", "optimization", "and", "usage", "."], "entities": [{"type": "Operation", "start": 92, "end": 94, "text": "our method", "sent_idx": 3}, {"type": "Effect", "start": 101, "end": 104, "text": "mean average precision", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--108"}
{"text": "Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.", "tokens": ["Various", "natural", "language", "processing", "tasks", "are", "structured", "prediction", "problems", "where", "outputs", "are", "constructed", "with", "multiple", "interdependent", "decisions", ".", "Past", "work", "has", "shown", "that", "domain", "knowledge", ",", "framed", "as", "constraints", "over", "the", "output", "space", ",", "can", "help", "improve", "predictive", "accuracy", ".", "However", ",", "designing", "good", "constraints", "often", "relies", "on", "domain", "expertise", ".", "In", "this", "paper", ",", "we", "study", "the", "problem", "of", "learning", "such", "constraints", ".", "We", "frame", "the", "problem", "as", "that", "of", "training", "a", "two-layer", "rectifier", "network", "to", "identify", "valid", "structures", "or", "substructures", ",", "and", "show", "a", "construction", "for", "converting", "a", "trained", "network", "into", "a", "system", "of", "linear", "constraints", "over", "the", "inference", "variables", ".", "Our", "experiments", "on", "several", "NLP", "tasks", "show", "that", "the", "learned", "constraints", "can", "improve", "the", "prediction", "accuracy", ",", "especially", "when", "the", "number", "of", "training", "examples", "is", "small", "."], "entities": [{"type": "Operation", "start": 112, "end": 114, "text": "learned constraints", "sent_idx": 5}, {"type": "Effect", "start": 118, "end": 119, "text": "accuracy", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--438"}
{"text": "While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder. Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models.", "tokens": ["While", "pre-training", "techniques", "are", "working", "very", "well", "in", "natural", "language", "processing", ",", "how", "to", "pre-train", "a", "decoder", "and", "effectively", "use", "it", "for", "neural", "machine", "translation", "(", "NMT", ")", "still", "remains", "a", "tricky", "issue", ".", "The", "main", "reason", "is", "that", "the", "cross-attention", "module", "between", "the", "encoder", "and", "decoder", "can", "not", "be", "pre-trained", ",", "and", "the", "combined", "encoder-decoder", "model", "can", "not", "work", "well", "in", "the", "fine-tuning", "stage", "because", "the", "inputs", "of", "the", "decoder", "cross-attention", "come", "from", "unknown", "encoder", "outputs", ".", "In", "this", "paper", ",", "we", "propose", "a", "better", "pre-training", "method", "for", "NMT", "by", "defining", "a", "semantic", "interface", "(", "SemFace", ")", "between", "the", "pre-trained", "encoder", "and", "the", "pre-trained", "decoder", ".", "Specifically", ",", "we", "propose", "two", "types", "of", "semantic", "interfaces", ",", "including", "CL-SemFace", "which", "regards", "cross-lingual", "embeddings", "as", "an", "interface", ",", "and", "VQ-SemFace", "which", "employs", "vector", "quantized", "embeddings", "to", "constrain", "the", "encoder", "outputs", "and", "decoder", "inputs", "in", "the", "same", "language-independent", "space", ".", "We", "conduct", "massive", "experiments", "on", "six", "supervised", "translation", "pairs", "and", "three", "unsupervised", "pairs", ".", "Experimental", "results", "demonstrate", "that", "our", "proposed", "SemFace", "can", "effectively", "connect", "the", "pre-trained", "encoder", "and", "decoder", ",", "and", "achieves", "significant", "improvement", "by", "3.7", "and", "1.5", "BLEU", "points", "on", "the", "two", "tasks", "respectively", "compared", "with", "previous", "pre-training-based", "NMT", "models", "."], "entities": [{"type": "Operation", "start": 168, "end": 169, "text": "SemFace", "sent_idx": 5}, {"type": "Effect", "start": 186, "end": 187, "text": "BLEU", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--348"}
{"text": "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.", "tokens": ["Understanding", "emotion", "expressed", "in", "language", "has", "a", "wide", "range", "of", "applications", ",", "from", "building", "empathetic", "chatbots", "to", "detecting", "harmful", "online", "behavior", ".", "Advancement", "in", "this", "area", "can", "be", "improved", "using", "large-scale", "datasets", "with", "a", "fine-grained", "typology", ",", "adaptable", "to", "multiple", "downstream", "tasks", ".", "We", "introduce", "GoEmotions", ",", "the", "largest", "manually", "annotated", "dataset", "of", "58k", "English", "Reddit", "comments", ",", "labeled", "for", "27", "emotion", "categories", "or", "Neutral", ".", "We", "demonstrate", "the", "high", "quality", "of", "the", "annotations", "via", "Principal", "Preserved", "Component", "Analysis", ".", "We", "conduct", "transfer", "learning", "experiments", "with", "existing", "emotion", "benchmarks", "to", "show", "that", "our", "dataset", "generalizes", "well", "to", "other", "domains", "and", "different", "emotion", "taxonomies", ".", "Our", "BERT-based", "model", "achieves", "an", "average", "F1-score", "of", ".46", "across", "our", "proposed", "taxonomy", ",", "leaving", "much", "room", "for", "improvement", "."], "entities": [{"type": "Operation", "start": 105, "end": 107, "text": "BERT-based model", "sent_idx": 5}, {"type": "Effect", "start": 110, "end": 111, "text": "F1-score", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--372"}
{"text": "Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn\u2019t true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency \u2013 a distinction that the model otherwise struggles to make.", "tokens": ["Parsing", "spoken", "dialogue", "poses", "unique", "difficulties", ",", "including", "disfluencies", "and", "unmarked", "boundaries", "between", "sentence-like", "units", ".", "Previous", "work", "has", "shown", "that", "prosody", "can", "help", "with", "parsing", "disfluent", "speech", "(", "Tran", "et", "al.", "2018", ")", ",", "but", "has", "assumed", "that", "the", "input", "to", "the", "parser", "is", "already", "segmented", "into", "sentence-like", "units", "(", "SUs", ")", ",", "which", "is", "n\u2019t", "true", "in", "existing", "speech", "applications", ".", "We", "investigate", "how", "prosody", "affects", "a", "parser", "that", "receives", "an", "entire", "dialogue", "turn", "as", "input", "(", "a", "turn-based", "model", ")", ",", "instead", "of", "gold", "standard", "pre-segmented", "SUs", "(", "an", "SU-based", "model", ")", ".", "In", "experiments", "on", "the", "English", "Switchboard", "corpus", ",", "we", "find", "that", "when", "using", "transcripts", "alone", ",", "the", "turn-based", "model", "has", "trouble", "segmenting", "SUs", ",", "leading", "to", "worse", "parse", "performance", "than", "the", "SU-based", "model", ".", "However", ",", "prosody", "can", "effectively", "replace", "gold", "standard", "SU", "boundaries", ":", "with", "prosody", ",", "the", "turn-based", "model", "performs", "as", "well", "as", "the", "SU-based", "model", "(", "91.38", "vs.", "91.06", "F1", "score", ",", "respectively", ")", ",", "despite", "performing", "two", "tasks", "(", "SU", "segmentation", "and", "parsing", ")", "rather", "than", "one", "(", "parsing", "alone", ")", ".", "Analysis", "shows", "that", "pitch", "and", "intensity", "features", "are", "the", "most", "important", "for", "this", "corpus", ",", "since", "they", "allow", "the", "model", "to", "correctly", "distinguish", "an", "SU", "boundary", "from", "a", "speech", "disfluency", "\u2013", "a", "distinction", "that", "the", "model", "otherwise", "struggles", "to", "make", "."], "entities": [{"type": "Operation", "start": 145, "end": 147, "text": "turn-based model", "sent_idx": 4}, {"type": "Effect", "start": 158, "end": 160, "text": "F1 score", "sent_idx": 4}, {"type": "Operation", "start": 112, "end": 115, "text": "the turn-based model", "sent_idx": 3}, {"type": "Effect", "start": 123, "end": 125, "text": "parse performance", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Neg_Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--79"}
{"text": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens\u2019 position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.", "tokens": ["Generating", "a", "readable", "summary", "that", "describes", "the", "functionality", "of", "a", "program", "is", "known", "as", "source", "code", "summarization", ".", "In", "this", "task", ",", "learning", "code", "representation", "by", "modeling", "the", "pairwise", "relationship", "between", "code", "tokens", "to", "capture", "their", "long-range", "dependencies", "is", "crucial", ".", "To", "learn", "code", "representation", "for", "summarization", ",", "we", "explore", "the", "Transformer", "model", "that", "uses", "a", "self-attention", "mechanism", "and", "has", "shown", "to", "be", "effective", "in", "capturing", "long-range", "dependencies", ".", "In", "this", "work", ",", "we", "show", "that", "despite", "the", "approach", "is", "simple", ",", "it", "outperforms", "the", "state-of-the-art", "techniques", "by", "a", "significant", "margin", ".", "We", "perform", "extensive", "analysis", "and", "ablation", "studies", "that", "reveal", "several", "important", "findings", ",", "e.g.", ",", "the", "absolute", "encoding", "of", "source", "code", "tokens", "\u2019", "position", "hinders", ",", "while", "relative", "encoding", "significantly", "improves", "the", "summarization", "performance", ".", "We", "have", "made", "our", "code", "publicly", "available", "to", "facilitate", "future", "research", "."], "entities": [{"type": "Operation", "start": 119, "end": 121, "text": "relative encoding", "sent_idx": 4}, {"type": "Effect", "start": 124, "end": 126, "text": "summarization performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--449"}
{"text": "This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.", "tokens": ["This", "paper", "proposes", "the", "problem", "of", "Deep", "Question", "Generation", "(", "DQG", ")", ",", "which", "aims", "to", "generate", "complex", "questions", "that", "require", "reasoning", "over", "multiple", "pieces", "of", "information", "about", "the", "input", "passage", ".", "In", "order", "to", "capture", "the", "global", "structure", "of", "the", "document", "and", "facilitate", "reasoning", ",", "we", "propose", "a", "novel", "framework", "that", "first", "constructs", "a", "semantic-level", "graph", "for", "the", "input", "document", "and", "then", "encodes", "the", "semantic", "graph", "by", "introducing", "an", "attention-based", "GGNN", "(", "Att-GGNN", ")", ".", "Afterward", ",", "we", "fuse", "the", "document-level", "and", "graph-level", "representations", "to", "perform", "joint", "training", "of", "content", "selection", "and", "question", "decoding", ".", "On", "the", "HotpotQA", "deep-question", "centric", "dataset", ",", "our", "model", "greatly", "improves", "performance", "over", "questions", "requiring", "reasoning", "over", "multiple", "facts", ",", "leading", "to", "state-of-the-art", "performance", ".", "The", "code", "is", "publicly", "available", "at", "https://github.com/WING-NUS/SG-Deep-Question-Generation", "."], "entities": [{"type": "Operation", "start": 103, "end": 105, "text": "our model", "sent_idx": 3}, {"type": "Effect", "start": 107, "end": 108, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--135"}
{"text": "Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.", "tokens": ["Despite", "pre-trained", "language", "models", "have", "proven", "useful", "for", "learning", "high-quality", "semantic", "representations", ",", "these", "models", "are", "still", "vulnerable", "to", "simple", "perturbations", ".", "Recent", "works", "aimed", "to", "improve", "the", "robustness", "of", "pre-trained", "models", "mainly", "focus", "on", "adversarial", "training", "from", "perturbed", "examples", "with", "similar", "semantics", ",", "neglecting", "the", "utilization", "of", "different", "or", "even", "opposite", "semantics", ".", "Different", "from", "the", "image", "processing", "field", ",", "the", "text", "is", "discrete", "and", "few", "word", "substitutions", "can", "cause", "significant", "semantic", "changes", ".", "To", "study", "the", "impact", "of", "semantics", "caused", "by", "small", "perturbations", ",", "we", "conduct", "a", "series", "of", "pilot", "experiments", "and", "surprisingly", "find", "that", "adversarial", "training", "is", "useless", "or", "even", "harmful", "for", "the", "model", "to", "detect", "these", "semantic", "changes", ".", "To", "address", "this", "problem", ",", "we", "propose", "Contrastive", "Learning", "with", "semantIc", "Negative", "Examples", "(", "CLINE", ")", ",", "which", "constructs", "semantic", "negative", "examples", "unsupervised", "to", "improve", "the", "robustness", "under", "semantically", "adversarial", "attacking", ".", "By", "comparing", "with", "similar", "and", "opposite", "semantic", "examples", ",", "the", "model", "can", "effectively", "perceive", "the", "semantic", "changes", "caused", "by", "small", "perturbations", ".", "Empirical", "results", "show", "that", "our", "approach", "yields", "substantial", "improvements", "on", "a", "range", "of", "sentiment", "analysis", ",", "reasoning", ",", "and", "reading", "comprehension", "tasks", ".", "And", "CLINE", "also", "ensures", "the", "compactness", "within", "the", "same", "semantics", "and", "separability", "across", "different", "semantics", "in", "sentence-level", "."], "entities": [{"type": "Operation", "start": 120, "end": 129, "text": "Contrastive Learning with semantIc Negative Examples (CLINE)", "sent_idx": 4}, {"type": "Effect", "start": 139, "end": 140, "text": "robustness", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--181"}
{"text": "Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "tokens": ["Language", "models", "pretrained", "on", "text", "from", "a", "wide", "variety", "of", "sources", "form", "the", "foundation", "of", "today", "\u2019s", "NLP", ".", "In", "light", "of", "the", "success", "of", "these", "broad-coverage", "models", ",", "we", "investigate", "whether", "it", "is", "still", "helpful", "to", "tailor", "a", "pretrained", "model", "to", "the", "domain", "of", "a", "target", "task", ".", "We", "present", "a", "study", "across", "four", "domains", "(", "biomedical", "and", "computer", "science", "publications", ",", "news", ",", "and", "reviews", ")", "and", "eight", "classification", "tasks", ",", "showing", "that", "a", "second", "phase", "of", "pretraining", "in-domain", "(", "domain-adaptive", "pretraining", ")", "leads", "to", "performance", "gains", ",", "under", "both", "high-", "and", "low-resource", "settings", ".", "Moreover", ",", "adapting", "to", "the", "task", "\u2019s", "unlabeled", "data", "(", "task-adaptive", "pretraining", ")", "improves", "performance", "even", "after", "domain-adaptive", "pretraining", ".", "Finally", ",", "we", "show", "that", "adapting", "to", "a", "task", "corpus", "augmented", "using", "simple", "data", "selection", "strategies", "is", "an", "effective", "alternative", ",", "especially", "when", "resources", "for", "domain-adaptive", "pretraining", "might", "be", "unavailable", ".", "Overall", ",", "we", "consistently", "find", "that", "multi-phase", "adaptive", "pretraining", "offers", "large", "gains", "in", "task", "performance", "."], "entities": [{"type": "Operation", "start": 154, "end": 157, "text": "multi-phase adaptive pretraining", "sent_idx": 5}, {"type": "Effect", "start": 162, "end": 163, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--740"}
{"text": "We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation.", "tokens": ["We", "present", "algorithms", "for", "aligning", "components", "of", "Abstract", "Meaning", "Representation", "(", "AMR", ")", "graphs", "to", "spans", "in", "English", "sentences", ".", "We", "leverage", "unsupervised", "learning", "in", "combination", "with", "heuristics", ",", "taking", "the", "best", "of", "both", "worlds", "from", "previous", "AMR", "aligners", ".", "Our", "unsupervised", "models", ",", "however", ",", "are", "more", "sensitive", "to", "graph", "substructures", ",", "without", "requiring", "a", "separate", "syntactic", "parse", ".", "Our", "approach", "covers", "a", "wider", "variety", "of", "AMR", "substructures", "than", "previously", "considered", ",", "achieves", "higher", "coverage", "of", "nodes", "and", "edges", ",", "and", "does", "so", "with", "higher", "accuracy", ".", "We", "will", "release", "our", "LEAMR", "datasets", "and", "aligner", "for", "use", "in", "research", "on", "AMR", "parsing", ",", "generation", ",", "and", "evaluation", "."], "entities": [{"type": "Operation", "start": 21, "end": 28, "text": "leverage unsupervised learning in combination with heuristics", "sent_idx": 1}, {"type": "Effect", "start": 75, "end": 80, "text": "coverage of nodes and edges", "sent_idx": 3}, {"type": "Effect", "start": 86, "end": 87, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--257"}
{"text": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.", "tokens": ["Confidence", "calibration", ",", "which", "aims", "to", "make", "model", "predictions", "equal", "to", "the", "true", "correctness", "measures", ",", "is", "important", "for", "neural", "machine", "translation", "(", "NMT", ")", "because", "it", "is", "able", "to", "offer", "useful", "indicators", "of", "translation", "errors", "in", "the", "generated", "output", ".", "While", "prior", "studies", "have", "shown", "that", "NMT", "models", "trained", "with", "label", "smoothing", "are", "well-calibrated", "on", "the", "ground-truth", "training", "data", ",", "we", "find", "that", "miscalibration", "still", "remains", "a", "severe", "challenge", "for", "NMT", "during", "inference", "due", "to", "the", "discrepancy", "between", "training", "and", "inference", ".", "By", "carefully", "designing", "experiments", "on", "three", "language", "pairs", ",", "our", "work", "provides", "in-depth", "analyses", "of", "the", "correlation", "between", "calibration", "and", "translation", "performance", "as", "well", "as", "linguistic", "properties", "of", "miscalibration", "and", "reports", "a", "number", "of", "interesting", "findings", "that", "might", "help", "humans", "better", "analyze", ",", "understand", "and", "improve", "NMT", "models", ".", "Based", "on", "these", "observations", ",", "we", "further", "propose", "a", "new", "graduated", "label", "smoothing", "method", "that", "can", "improve", "both", "inference", "calibration", "and", "translation", "performance", "."], "entities": [{"type": "Operation", "start": 142, "end": 146, "text": "graduated label smoothing method", "sent_idx": 3}, {"type": "Effect", "start": 150, "end": 152, "text": "inference calibration", "sent_idx": 3}, {"type": "Effect", "start": 153, "end": 155, "text": "translation performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--278"}
{"text": "Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information\u2019s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.", "tokens": ["Dialogue", "state", "tracker", "is", "responsible", "for", "inferring", "user", "intentions", "through", "dialogue", "history", ".", "Previous", "methods", "have", "difficulties", "in", "handling", "dialogues", "with", "long", "interaction", "context", ",", "due", "to", "the", "excessive", "information", ".", "We", "propose", "a", "Dialogue", "State", "Tracker", "with", "Slot", "Attention", "and", "Slot", "Information", "Sharing", "(", "SAS", ")", "to", "reduce", "redundant", "information", "\u2019s", "interference", "and", "improve", "long", "dialogue", "context", "tracking", ".", "Specially", ",", "we", "first", "apply", "a", "Slot", "Attention", "to", "learn", "a", "set", "of", "slot-specific", "features", "from", "the", "original", "dialogue", "and", "then", "integrate", "them", "using", "a", "slot", "information", "sharing", "module", ".", "Our", "model", "yields", "a", "significantly", "improved", "performance", "compared", "to", "previous", "state-of", "the-art", "models", "on", "the", "MultiWOZ", "dataset", "."], "entities": [{"type": "Operation", "start": 34, "end": 47, "text": "Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS)", "sent_idx": 2}, {"type": "Effect", "start": 96, "end": 97, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--567"}
{"text": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.", "tokens": ["Non-autoregressive", "neural", "machine", "translation", "(", "NAT", ")", "predicts", "the", "entire", "target", "sequence", "simultaneously", "and", "significantly", "accelerates", "inference", "process", ".", "However", ",", "NAT", "discards", "the", "dependency", "information", "in", "a", "sentence", ",", "and", "thus", "inevitably", "suffers", "from", "the", "multi-modality", "problem", ":", "the", "target", "tokens", "may", "be", "provided", "by", "different", "possible", "translations", ",", "often", "causing", "token", "repetitions", "or", "missing", ".", "To", "alleviate", "this", "problem", ",", "we", "propose", "a", "novel", "semi-autoregressive", "model", "RecoverSAT", "in", "this", "work", ",", "which", "generates", "a", "translation", "as", "a", "sequence", "of", "segments", ".", "The", "segments", "are", "generated", "simultaneously", "while", "each", "segment", "is", "predicted", "token-by-token", ".", "By", "dynamically", "determining", "segment", "length", "and", "deleting", "repetitive", "segments", ",", "RecoverSAT", "is", "capable", "of", "recovering", "from", "repetitive", "and", "missing", "token", "errors", ".", "Experimental", "results", "on", "three", "widely-used", "benchmark", "datasets", "show", "that", "our", "proposed", "model", "achieves", "more", "than", "4", "times", "speedup", "while", "maintaining", "comparable", "performance", "compared", "with", "the", "corresponding", "autoregressive", "model", "."], "entities": [{"type": "Operation", "start": 66, "end": 69, "text": "semi-autoregressive model RecoverSAT", "sent_idx": 2}, {"type": "Effect", "start": 134, "end": 135, "text": "speedup", "sent_idx": 5}, {"type": "Effect", "start": 138, "end": 139, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--277"}
{"text": "Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.", "tokens": ["Consumers", "increasingly", "rate", ",", "review", "and", "research", "products", "online", "(", "Jansen", ",", "2010", ";", "Litvin", "et", "al.", ",", "2008", ")", ".", "Consequently", ",", "websites", "containing", "consumer", "reviews", "are", "becoming", "targets", "of", "opinion", "spam", ".", "While", "recent", "work", "has", "focused", "primarily", "on", "manually", "identifiable", "instances", "of", "opinion", "spam", ",", "in", "this", "work", "we", "study", "deceptive", "opinion", "spam", "---", "fictitious", "opinions", "that", "have", "been", "deliberately", "written", "to", "sound", "authentic", ".", "Integrating", "work", "from", "psychology", "and", "computational", "linguistics", ",", "we", "develop", "and", "compare", "three", "approaches", "to", "detecting", "deceptive", "opinion", "spam", ",", "and", "ultimately", "develop", "a", "classifier", "that", "is", "nearly", "90", "%", "accurate", "on", "our", "gold-standard", "opinion", "spam", "dataset", ".", "Based", "on", "feature", "analysis", "of", "our", "learned", "models", ",", "we", "additionally", "make", "several", "theoretical", "contributions", ",", "including", "revealing", "a", "relationship", "between", "deceptive", "opinions", "and", "imaginative", "writing", "."], "entities": [{"type": "Operation", "start": 92, "end": 93, "text": "classifier", "sent_idx": 3}, {"type": "Effect", "start": 98, "end": 99, "text": "accurate", "sent_idx": 3}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "P11-1032"}
{"text": "Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders\u2014a sequential document encoder and a graph-structured encoder\u2014to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.", "tokens": ["Sequence-to-sequence", "models", "for", "abstractive", "summarization", "have", "been", "studied", "extensively", ",", "yet", "the", "generated", "summaries", "commonly", "suffer", "from", "fabricated", "content", ",", "and", "are", "often", "found", "to", "be", "near-extractive", ".", "We", "argue", "that", ",", "to", "address", "these", "issues", ",", "the", "summarizer", "should", "acquire", "semantic", "interpretation", "over", "input", ",", "e.g.", ",", "via", "structured", "representation", ",", "to", "allow", "the", "generation", "of", "more", "informative", "summaries", ".", "In", "this", "paper", ",", "we", "present", "ASGARD", ",", "a", "novel", "framework", "for", "Abstractive", "Summarization", "with", "Graph-Augmentation", "and", "semantic-driven", "RewarD.", "We", "propose", "the", "use", "of", "dual", "encoders", "\u2014", "a", "sequential", "document", "encoder", "and", "a", "graph-structured", "encoder", "\u2014", "to", "maintain", "the", "global", "context", "and", "local", "characteristics", "of", "entities", ",", "complementing", "each", "other", ".", "We", "further", "design", "a", "reward", "based", "on", "a", "multiple", "choice", "cloze", "test", "to", "drive", "the", "model", "to", "better", "capture", "entity", "interactions", ".", "Results", "show", "that", "our", "models", "produce", "significantly", "higher", "ROUGE", "scores", "than", "a", "variant", "without", "knowledge", "graph", "as", "input", "on", "both", "New", "York", "Times", "and", "CNN/Daily", "Mail", "datasets", ".", "We", "also", "obtain", "better", "or", "comparable", "performance", "compared", "to", "systems", "that", "are", "fine-tuned", "from", "large", "pretrained", "language", "models", ".", "Human", "judges", "further", "rate", "our", "model", "outputs", "as", "more", "informative", "and", "containing", "fewer", "unfaithful", "errors", "."], "entities": [{"type": "Operation", "start": 67, "end": 68, "text": "ASGARD", "sent_idx": 2}, {"type": "Effect", "start": 142, "end": 144, "text": "ROUGE scores", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--457"}
{"text": "Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.", "tokens": ["Extracting", "structured", "knowledge", "from", "product", "profiles", "is", "crucial", "for", "various", "applications", "in", "e-Commerce", ".", "State-of-the-art", "approaches", "for", "knowledge", "extraction", "were", "each", "designed", "for", "a", "single", "category", "of", "product", ",", "and", "thus", "do", "not", "apply", "to", "real-life", "e-Commerce", "scenarios", ",", "which", "often", "contain", "thousands", "of", "diverse", "categories", ".", "This", "paper", "proposes", "TXtract", ",", "a", "taxonomy-aware", "knowledge", "extraction", "model", "that", "applies", "to", "thousands", "of", "product", "categories", "organized", "in", "a", "hierarchical", "taxonomy", ".", "Through", "category", "conditional", "self-attention", "and", "multi-task", "learning", ",", "our", "approach", "is", "both", "scalable", ",", "as", "it", "trains", "a", "single", "model", "for", "thousands", "of", "categories", ",", "and", "effective", ",", "as", "it", "extracts", "category-specific", "attribute", "values", ".", "Experiments", "on", "products", "from", "a", "taxonomy", "with", "4,000", "categories", "show", "that", "TXtract", "outperforms", "state-of-the-art", "approaches", "by", "up", "to", "10", "%", "in", "F1", "and", "15", "%", "in", "coverage", "across", "all", "categories", "."], "entities": [{"type": "Operation", "start": 116, "end": 117, "text": "TXtract", "sent_idx": 4}, {"type": "Effect", "start": 126, "end": 127, "text": "F1", "sent_idx": 4}, {"type": "Effect", "start": 131, "end": 132, "text": "coverage", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--751"}
{"text": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.", "tokens": ["Existing", "end-to-end", "dialog", "systems", "perform", "less", "effectively", "when", "data", "is", "scarce", ".", "To", "obtain", "an", "acceptable", "success", "in", "real-life", "online", "services", "with", "only", "a", "handful", "of", "training", "examples", ",", "both", "fast", "adaptability", "and", "reliable", "performance", "are", "highly", "desirable", "for", "dialog", "systems", ".", "In", "this", "paper", ",", "we", "propose", "the", "Meta-Dialog", "System", "(", "MDS", ")", ",", "which", "combines", "the", "advantages", "of", "both", "meta-learning", "approaches", "and", "human-machine", "collaboration", ".", "We", "evaluate", "our", "methods", "on", "a", "new", "extended-bAbI", "dataset", "and", "a", "transformed", "MultiWOZ", "dataset", "for", "low-resource", "goal-oriented", "dialog", "learning", ".", "Experimental", "results", "show", "that", "MDS", "significantly", "outperforms", "non-meta-learning", "baselines", "and", "can", "achieve", "more", "than", "90", "%", "per-turn", "accuracies", "with", "only", "10", "dialogs", "on", "the", "extended-bAbI", "dataset", "."], "entities": [{"type": "Operation", "start": 91, "end": 92, "text": "MDS", "sent_idx": 4}, {"type": "Effect", "start": 103, "end": 105, "text": "per-turn accuracies", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--57"}
{"text": "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new state-of-the-art for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.", "tokens": ["We", "present", "a", "simple", "yet", "effective", "approach", "to", "build", "multilingual", "speech-to-text", "(", "ST", ")", "translation", "through", "efficient", "transfer", "learning", "from", "a", "pretrained", "speech", "encoder", "and", "text", "decoder", ".", "Our", "key", "finding", "is", "that", "a", "minimalistic", "LNA", "(", "LayerNorm", "and", "Attention", ")", "finetuning", "can", "achieve", "zero-shot", "crosslingual", "and", "cross-modality", "transfer", "ability", "by", "only", "finetuning", "10", "50", "%", "of", "the", "pretrained", "parameters", ".", "This", "effectively", "leverages", "large", "pretrained", "models", "at", "low", "training", "cost", "such", "as", "wav2vec", "2.0", "for", "acoustic", "modeling", ",", "and", "mBART", "for", "multilingual", "text", "generation", ".", "This", "sets", "a", "new", "state-of-the-art", "for", "36", "translation", "directions", "(", "and", "surpassing", "cascaded", "ST", "for", "26", "of", "them", ")", "on", "the", "large-scale", "multilingual", "ST", "benchmark", "CoVoST", "2", "(", "+", "6.4", "BLEU", "on", "average", "for", "En-X", "directions", "and", "+", "6.7", "BLEU", "for", "X-En", "directions", ")", ".", "Our", "approach", "demonstrates", "strong", "zero-shot", "performance", "in", "a", "many-to-many", "multilingual", "model", "(", "+", "5.6", "BLEU", "on", "average", "across", "28", "non-English", "directions", ")", ",", "making", "it", "an", "appealing", "approach", "for", "attaining", "high-quality", "speech", "translation", "with", "improved", "parameter", "and", "data", "efficiency", "."], "entities": [{"type": "Operation", "start": 34, "end": 41, "text": "minimalistic LNA (LayerNorm and Attention)", "sent_idx": 1}, {"type": "Effect", "start": 116, "end": 117, "text": "BLEU", "sent_idx": 3}, {"type": "Operation", "start": 17, "end": 27, "text": "transfer learning from a pretrained speech encoder and text decoder", "sent_idx": 0}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}], "id": "abstract-2021--acl-long--68"}
{"text": "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.", "tokens": ["Recent", "years", "have", "witnessed", "a", "surge", "of", "interests", "of", "using", "neural", "topic", "models", "for", "automatic", "topic", "extraction", "from", "text", ",", "since", "they", "avoid", "the", "complicated", "mathematical", "derivations", "for", "model", "inference", "as", "in", "traditional", "topic", "models", "such", "as", "Latent", "Dirichlet", "Allocation", "(", "LDA", ")", ".", "However", ",", "these", "models", "either", "typically", "assume", "improper", "prior", "(", "e.g.", "Gaussian", "or", "Logistic", "Normal", ")", "over", "latent", "topic", "space", "or", "could", "not", "infer", "topic", "distribution", "for", "a", "given", "document", ".", "To", "address", "these", "limitations", ",", "we", "propose", "a", "neural", "topic", "modeling", "approach", ",", "called", "Bidirectional", "Adversarial", "Topic", "(", "BAT", ")", "model", ",", "which", "represents", "the", "first", "attempt", "of", "applying", "bidirectional", "adversarial", "training", "for", "neural", "topic", "modeling", ".", "The", "proposed", "BAT", "builds", "a", "two-way", "projection", "between", "the", "document-topic", "distribution", "and", "the", "document-word", "distribution", ".", "It", "uses", "a", "generator", "to", "capture", "the", "semantic", "patterns", "from", "texts", "and", "an", "encoder", "for", "topic", "inference", ".", "Furthermore", ",", "to", "incorporate", "word", "relatedness", "information", ",", "the", "Bidirectional", "Adversarial", "Topic", "model", "with", "Gaussian", "(", "Gaussian-BAT", ")", "is", "extended", "from", "BAT", ".", "To", "verify", "the", "effectiveness", "of", "BAT", "and", "Gaussian-BAT", ",", "three", "benchmark", "corpora", "are", "used", "in", "our", "experiments", ".", "The", "experimental", "results", "show", "that", "BAT", "and", "Gaussian-BAT", "obtain", "more", "coherent", "topics", ",", "outperforming", "several", "competitive", "baselines", ".", "Moreover", ",", "when", "performing", "text", "clustering", "based", "on", "the", "extracted", "topics", ",", "our", "models", "outperform", "all", "the", "baselines", ",", "with", "more", "significant", "improvements", "achieved", "by", "Gaussian-BAT", "where", "an", "increase", "of", "near", "6", "%", "is", "observed", "in", "accuracy", "."], "entities": [{"type": "Operation", "start": 230, "end": 231, "text": "Gaussian-BAT", "sent_idx": 8}, {"type": "Effect", "start": 241, "end": 242, "text": "accuracy", "sent_idx": 8}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--32"}
{"text": "Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.", "tokens": ["Despite", "its", "substantial", "coverage", ",", "NomBank", "does", "not", "account", "for", "all", "within-sentence", "arguments", "and", "ignores", "extra-sentential", "arguments", "altogether", ".", "These", "arguments", ",", "which", "we", "call", "implicit", ",", "are", "important", "to", "semantic", "processing", ",", "and", "their", "recovery", "could", "potentially", "benefit", "many", "NLP", "applications", ".", "We", "present", "a", "study", "of", "implicit", "arguments", "for", "a", "select", "group", "of", "frequent", "nominal", "predicates", ".", "We", "show", "that", "implicit", "arguments", "are", "pervasive", "for", "these", "predicates", ",", "adding", "65", "%", "to", "the", "coverage", "of", "NomBank", ".", "We", "demonstrate", "the", "feasibility", "of", "recovering", "implicit", "arguments", "with", "a", "supervised", "classification", "model", ".", "Our", "results", "and", "analyses", "provide", "a", "baseline", "for", "future", "work", "on", "this", "emerging", "task", "."], "entities": [{"type": "Operation", "start": 62, "end": 64, "text": "implicit arguments", "sent_idx": 3}, {"type": "Effect", "start": 75, "end": 78, "text": "coverage of NomBank", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "P10-1160"}
{"text": "Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.", "tokens": ["Despite", "achieving", "prominent", "performance", "on", "many", "important", "tasks", ",", "it", "has", "been", "reported", "that", "neural", "networks", "are", "vulnerable", "to", "adversarial", "examples", ".", "Previously", "studies", "along", "this", "line", "mainly", "focused", "on", "semantic", "tasks", "such", "as", "sentiment", "analysis", ",", "question", "answering", "and", "reading", "comprehension", ".", "In", "this", "study", ",", "we", "show", "that", "adversarial", "examples", "also", "exist", "in", "dependency", "parsing", ":", "we", "propose", "two", "approaches", "to", "study", "where", "and", "how", "parsers", "make", "mistakes", "by", "searching", "over", "perturbations", "to", "existing", "texts", "at", "sentence", "and", "phrase", "levels", ",", "and", "design", "algorithms", "to", "construct", "such", "examples", "in", "both", "of", "the", "black-box", "and", "white-box", "settings", ".", "Our", "experiments", "with", "one", "of", "state-of-the-art", "parsers", "on", "the", "English", "Penn", "Treebank", "(", "PTB", ")", "show", "that", "up", "to", "77", "%", "of", "input", "examples", "admit", "adversarial", "perturbations", ",", "and", "we", "also", "show", "that", "the", "robustness", "of", "parsing", "models", "can", "be", "improved", "by", "crafting", "high-quality", "adversaries", "and", "including", "them", "in", "the", "training", "stage", ",", "while", "suffering", "little", "to", "no", "performance", "drop", "on", "the", "clean", "input", "data", "."], "entities": [{"type": "Operation", "start": 141, "end": 151, "text": "crafting high-quality adversaries and including them in the training stage", "sent_idx": 3}, {"type": "Effect", "start": 157, "end": 158, "text": "performance", "sent_idx": 3}, {"type": "Effect", "start": 133, "end": 134, "text": "robustness", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--590"}
{"text": "Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.", "tokens": ["Cross-domain", "sentiment", "analysis", "has", "received", "significant", "attention", "in", "recent", "years", ",", "prompted", "by", "the", "need", "to", "combat", "the", "domain", "gap", "between", "different", "applications", "that", "make", "use", "of", "sentiment", "analysis", ".", "In", "this", "paper", ",", "we", "take", "a", "novel", "perspective", "on", "this", "task", "by", "exploring", "the", "role", "of", "external", "commonsense", "knowledge", ".", "We", "introduce", "a", "new", "framework", ",", "KinGDOM", ",", "which", "utilizes", "the", "ConceptNet", "knowledge", "graph", "to", "enrich", "the", "semantics", "of", "a", "document", "by", "providing", "both", "domain-specific", "and", "domain-general", "background", "concepts", ".", "These", "concepts", "are", "learned", "by", "training", "a", "graph", "convolutional", "autoencoder", "that", "leverages", "inter-domain", "concepts", "in", "a", "domain-invariant", "manner", ".", "Conditioning", "a", "popular", "domain-adversarial", "baseline", "method", "with", "these", "learned", "concepts", "helps", "improve", "its", "performance", "over", "state-of-the-art", "approaches", ",", "demonstrating", "the", "efficacy", "of", "our", "proposed", "framework", "."], "entities": [{"type": "Operation", "start": 100, "end": 110, "text": "Conditioning a popular domain-adversarial baseline method with these learned concepts", "sent_idx": 4}, {"type": "Effect", "start": 113, "end": 114, "text": "performance", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--292"}
{"text": "Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora.", "tokens": ["Following", "each", "patient", "visit", ",", "physicians", "draft", "long", "semi-structured", "clinical", "summaries", "called", "SOAP", "notes", ".", "While", "invaluable", "to", "clinicians", "and", "researchers", ",", "creating", "digital", "SOAP", "notes", "is", "burdensome", ",", "contributing", "to", "physician", "burnout", ".", "In", "this", "paper", ",", "we", "introduce", "the", "first", "complete", "pipelines", "to", "leverage", "deep", "summarization", "models", "to", "generate", "these", "notes", "based", "on", "transcripts", "of", "conversations", "between", "physicians", "and", "patients", ".", "After", "exploring", "a", "spectrum", "of", "methods", "across", "the", "extractive-abstractive", "spectrum", ",", "we", "propose", "Cluster2Sent", ",", "an", "algorithm", "that", "(", "i", ")", "extracts", "important", "utterances", "relevant", "to", "each", "summary", "section", ";", "(", "ii", ")", "clusters", "together", "related", "utterances", ";", "and", "then", "(", "iii", ")", "generates", "one", "summary", "sentence", "per", "cluster", ".", "Cluster2Sent", "outperforms", "its", "purely", "abstractive", "counterpart", "by", "8", "ROUGE-1", "points", ",", "and", "produces", "significantly", "more", "factual", "and", "coherent", "sentences", "as", "assessed", "by", "expert", "human", "evaluators", ".", "For", "reproducibility", ",", "we", "demonstrate", "similar", "benefits", "on", "the", "publicly", "available", "AMI", "dataset", ".", "Our", "results", "speak", "to", "the", "benefits", "of", "structuring", "summaries", "into", "sections", "and", "annotating", "supporting", "evidence", "when", "constructing", "summarization", "corpora", "."], "entities": [{"type": "Operation", "start": 113, "end": 114, "text": "Cluster2Sent", "sent_idx": 4}, {"type": "Effect", "start": 121, "end": 123, "text": "ROUGE-1 points", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--384"}
{"text": "This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.", "tokens": ["This", "work", "presents", "a", "new", "approach", "to", "unsupervised", "abstractive", "summarization", "based", "on", "maximizing", "a", "combination", "of", "coverage", "and", "fluency", "for", "a", "given", "length", "constraint", ".", "It", "introduces", "a", "novel", "method", "that", "encourages", "the", "inclusion", "of", "key", "terms", "from", "the", "original", "document", "into", "the", "summary", ":", "key", "terms", "are", "masked", "out", "of", "the", "original", "document", "and", "must", "be", "filled", "in", "by", "a", "coverage", "model", "using", "the", "current", "generated", "summary", ".", "A", "novel", "unsupervised", "training", "procedure", "leverages", "this", "coverage", "model", "along", "with", "a", "fluency", "model", "to", "generate", "and", "score", "summaries", ".", "When", "tested", "on", "popular", "news", "summarization", "datasets", ",", "the", "method", "outperforms", "previous", "unsupervised", "methods", "by", "more", "than", "2", "R-1", "points", ",", "and", "approaches", "results", "of", "competitive", "supervised", "methods", ".", "Our", "model", "attains", "higher", "levels", "of", "abstraction", "with", "copied", "passages", "roughly", "two", "times", "shorter", "than", "prior", "work", ",", "and", "learns", "to", "compress", "and", "merge", "sentences", "without", "supervision", "."], "entities": [{"type": "Operation", "start": 31, "end": 44, "text": "encourages the inclusion of key terms from the original document into the summary", "sent_idx": 1}, {"type": "Effect", "start": 107, "end": 109, "text": "R-1 points", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--460"}
{"text": "Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. By leveraging BERT\u2019s idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization. Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets.", "tokens": ["Large-scale", "pre-trained", "language", "model", "such", "as", "BERT", "has", "achieved", "great", "success", "in", "language", "understanding", "tasks", ".", "However", ",", "it", "remains", "an", "open", "question", "how", "to", "utilize", "BERT", "for", "language", "generation", ".", "In", "this", "paper", ",", "we", "present", "a", "novel", "approach", ",", "Conditional", "Masked", "Language", "Modeling", "(", "C-MLM", ")", ",", "to", "enable", "the", "finetuning", "of", "BERT", "on", "target", "generation", "tasks", ".", "The", "finetuned", "BERT", "(", "teacher", ")", "is", "exploited", "as", "extra", "supervision", "to", "improve", "conventional", "Seq2Seq", "models", "(", "student", ")", "for", "better", "text", "generation", "performance", ".", "By", "leveraging", "BERT", "\u2019s", "idiosyncratic", "bidirectional", "nature", ",", "distilling", "knowledge", "learned", "in", "BERT", "can", "encourage", "auto-regressive", "Seq2Seq", "models", "to", "plan", "ahead", ",", "imposing", "global", "sequence-level", "supervision", "for", "coherent", "text", "generation", ".", "Experiments", "show", "that", "the", "proposed", "approach", "significantly", "outperforms", "strong", "Transformer", "baselines", "on", "multiple", "language", "generation", "tasks", "such", "as", "machine", "translation", "and", "text", "summarization", ".", "Our", "proposed", "model", "also", "achieves", "new", "state", "of", "the", "art", "on", "IWSLT", "German-English", "and", "English-Vietnamese", "MT", "datasets", "."], "entities": [{"type": "Operation", "start": 61, "end": 66, "text": "finetuned BERT (teacher)", "sent_idx": 3}, {"type": "Effect", "start": 83, "end": 84, "text": "performance", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--705"}
{"text": "Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection. Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.", "tokens": ["Intent", "classification", "is", "a", "major", "task", "in", "spoken", "language", "understanding", "(", "SLU", ")", ".", "Since", "most", "models", "are", "built", "with", "pre-collected", "in-domain", "(", "IND", ")", "training", "utterances", ",", "their", "ability", "to", "detect", "unsupported", "out-of-domain", "(", "OOD", ")", "utterances", "has", "a", "critical", "effect", "in", "practical", "use", ".", "Recent", "works", "have", "shown", "that", "using", "extra", "data", "and", "labels", "can", "improve", "the", "OOD", "detection", "performance", ",", "yet", "it", "could", "be", "costly", "to", "collect", "such", "data", ".", "This", "paper", "proposes", "to", "train", "a", "model", "with", "only", "IND", "data", "while", "supporting", "both", "IND", "intent", "classification", "and", "OOD", "detection", ".", "Our", "method", "designs", "a", "novel", "domain-regularized", "module", "(", "DRM", ")", "to", "reduce", "the", "overconfident", "phenomenon", "of", "a", "vanilla", "classifier", ",", "achieving", "a", "better", "generalization", "in", "both", "cases", ".", "Besides", ",", "DRM", "can", "be", "used", "as", "a", "drop-in", "replacement", "for", "the", "last", "layer", "in", "any", "neural", "network-based", "intent", "classifier", ",", "providing", "a", "low-cost", "strategy", "for", "a", "significant", "improvement", ".", "The", "evaluation", "on", "four", "datasets", "shows", "that", "our", "method", "built", "on", "BERT", "and", "RoBERTa", "models", "achieves", "state-of-the-art", "performance", "against", "existing", "approaches", "and", "the", "strong", "baselines", "we", "created", "for", "the", "comparisons", "."], "entities": [{"type": "Operation", "start": 99, "end": 104, "text": "domain-regularized module (DRM)", "sent_idx": 4}, {"type": "Effect", "start": 117, "end": 118, "text": "generalization", "sent_idx": 4}, {"type": "Operation", "start": 163, "end": 167, "text": "BERT and RoBERTa models", "sent_idx": 6}, {"type": "Effect", "start": 169, "end": 170, "text": "performance", "sent_idx": 6}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 3}], "id": "abstract-2021--acl-long--190"}
{"text": "Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients\u2019 welfare.", "tokens": ["Sequence-to-sequence", "(", "seq2seq", ")", "network", "is", "a", "well-established", "model", "for", "text", "summarization", "task", ".", "It", "can", "learn", "to", "produce", "readable", "content", ";", "however", ",", "it", "falls", "short", "in", "effectively", "identifying", "key", "regions", "of", "the", "source", ".", "In", "this", "paper", ",", "we", "approach", "the", "content", "selection", "problem", "for", "clinical", "abstractive", "summarization", "by", "augmenting", "salient", "ontological", "terms", "into", "the", "summarizer", ".", "Our", "experiments", "on", "two", "publicly", "available", "clinical", "data", "sets", "(", "107,372", "reports", "of", "MIMIC-CXR", ",", "and", "3,366", "reports", "of", "OpenI", ")", "show", "that", "our", "model", "statistically", "significantly", "boosts", "state-of-the-art", "results", "in", "terms", "of", "ROUGE", "metrics", "(", "with", "improvements", ":", "2.9", "%", "RG-1", ",", "2.5", "%", "RG-2", ",", "1.9", "%", "RG-L", ")", ",", "in", "the", "healthcare", "domain", "where", "any", "range", "of", "improvement", "impacts", "patients", "\u2019", "welfare", "."], "entities": [{"type": "Operation", "start": 51, "end": 58, "text": "augmenting salient ontological terms into the summarizer", "sent_idx": 2}, {"type": "Effect", "start": 92, "end": 93, "text": "ROUGE", "sent_idx": 3}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--172"}
{"text": "In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy\u2013efficiency trade-offs. Source code for this paper can be found at https://github.com/castorini/transformers-selective.", "tokens": ["In", "selective", "prediction", ",", "a", "classifier", "is", "allowed", "to", "abstain", "from", "making", "predictions", "on", "low-confidence", "examples", ".", "Though", "this", "setting", "is", "interesting", "and", "important", ",", "selective", "prediction", "has", "rarely", "been", "examined", "in", "natural", "language", "processing", "(", "NLP", ")", "tasks", ".", "To", "fill", "this", "void", "in", "the", "literature", ",", "we", "study", "in", "this", "paper", "selective", "prediction", "for", "NLP", ",", "comparing", "different", "models", "and", "confidence", "estimators", ".", "We", "further", "propose", "a", "simple", "error", "regularization", "trick", "that", "improves", "confidence", "estimation", "without", "substantially", "increasing", "the", "computation", "budget", ".", "We", "show", "that", "recent", "pre-trained", "transformer", "models", "simultaneously", "improve", "both", "model", "accuracy", "and", "confidence", "estimation", "effectiveness", ".", "We", "also", "find", "that", "our", "proposed", "regularization", "improves", "confidence", "estimation", "and", "can", "be", "applied", "to", "other", "relevant", "scenarios", ",", "such", "as", "using", "classifier", "cascades", "for", "accuracy", "\u2013", "efficiency", "trade-offs", ".", "Source", "code", "for", "this", "paper", "can", "be", "found", "at", "https://github.com/castorini/transformers-selective", "."], "entities": [{"type": "Operation", "start": 88, "end": 91, "text": "pre-trained transformer models", "sent_idx": 4}, {"type": "Effect", "start": 95, "end": 96, "text": "accuracy", "sent_idx": 4}, {"type": "Effect", "start": 97, "end": 100, "text": "confidence estimation effectiveness", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2021--acl-long--84"}
{"text": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released.", "tokens": ["The", "rapid", "development", "of", "large", "pre-trained", "language", "models", "has", "greatly", "increased", "the", "demand", "for", "model", "compression", "techniques", ",", "among", "which", "quantization", "is", "a", "popular", "solution", ".", "In", "this", "paper", ",", "we", "propose", "BinaryBERT", ",", "which", "pushes", "BERT", "quantization", "to", "the", "limit", "by", "weight", "binarization", ".", "We", "find", "that", "a", "binary", "BERT", "is", "hard", "to", "be", "trained", "directly", "than", "a", "ternary", "counterpart", "due", "to", "its", "complex", "and", "irregular", "loss", "landscape", ".", "Therefore", ",", "we", "propose", "ternary", "weight", "splitting", ",", "which", "initializes", "BinaryBERT", "by", "equivalently", "splitting", "from", "a", "half-sized", "ternary", "network", ".", "The", "binary", "model", "thus", "inherits", "the", "good", "performance", "of", "the", "ternary", "one", ",", "and", "can", "be", "further", "enhanced", "by", "fine-tuning", "the", "new", "architecture", "after", "splitting", ".", "Empirical", "results", "show", "that", "our", "BinaryBERT", "has", "only", "a", "slight", "performance", "drop", "compared", "with", "the", "full-precision", "model", "while", "being", "24x", "smaller", ",", "achieving", "the", "state-of-the-art", "compression", "results", "on", "the", "GLUE", "and", "SQuAD", "benchmarks", ".", "Code", "will", "be", "released", "."], "entities": [{"type": "Operation", "start": 121, "end": 122, "text": "BinaryBERT", "sent_idx": 5}, {"type": "Effect", "start": 126, "end": 127, "text": "performance", "sent_idx": 5}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2021--acl-long--334"}
{"text": "Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model\u2019s scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem.", "tokens": ["Question", "answering", "(", "QA", ")", "is", "an", "important", "aspect", "of", "open-domain", "conversational", "agents", ",", "garnering", "specific", "research", "focus", "in", "the", "conversational", "QA", "(", "ConvQA", ")", "subtask", ".", "One", "notable", "limitation", "of", "recent", "ConvQA", "efforts", "is", "the", "response", "being", "answer", "span", "extraction", "from", "the", "target", "corpus", ",", "thus", "ignoring", "the", "natural", "language", "generation", "(", "NLG", ")", "aspect", "of", "high-quality", "conversational", "agents", ".", "In", "this", "work", ",", "we", "propose", "a", "method", "for", "situating", "QA", "responses", "within", "a", "SEQ2SEQ", "NLG", "approach", "to", "generate", "fluent", "grammatical", "answer", "responses", "while", "maintaining", "correctness", ".", "From", "a", "technical", "perspective", ",", "we", "use", "data", "augmentation", "to", "generate", "training", "data", "for", "an", "end-to-end", "system", ".", "Specifically", ",", "we", "develop", "Syntactic", "Transformations", "(", "STs", ")", "to", "produce", "question-specific", "candidate", "answer", "responses", "and", "rank", "them", "using", "a", "BERT-based", "classifier", "(", "Devlin", "et", "al.", ",", "2019", ")", ".", "Human", "evaluation", "on", "SQuAD", "2.0", "data", "(", "Rajpurkar", "et", "al.", ",", "2018", ")", "demonstrate", "that", "the", "proposed", "model", "outperforms", "baseline", "CoQA", "and", "QuAC", "models", "in", "generating", "conversational", "responses", ".", "We", "further", "show", "our", "model", "\u2019s", "scalability", "by", "conducting", "tests", "on", "the", "CoQA", "dataset", ".", "The", "code", "and", "data", "are", "available", "at", "https://github.com/abaheti95/QADialogSystem", "."], "entities": [{"type": "Operation", "start": 75, "end": 78, "text": "SEQ2SEQ NLG approach", "sent_idx": 2}, {"type": "Effect", "start": 86, "end": 87, "text": "correctness", "sent_idx": 2}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--19"}
{"text": "We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.", "tokens": ["We", "propose", "a", "language-independent", "method", "for", "the", "automatic", "extraction", "of", "transliteration", "pairs", "from", "parallel", "corpora", ".", "In", "contrast", "to", "previous", "work", ",", "our", "method", "uses", "no", "form", "of", "supervision", ",", "and", "does", "not", "require", "linguistically", "informed", "preprocessing", ".", "We", "conduct", "experiments", "on", "data", "sets", "from", "the", "NEWS", "2010", "shared", "task", "on", "transliteration", "mining", "and", "achieve", "an", "F-measure", "of", "up", "to", "92", "%", ",", "outperforming", "most", "of", "the", "semi-supervised", "systems", "that", "were", "submitted", ".", "We", "also", "apply", "our", "method", "to", "English/Hindi", "and", "English/Arabic", "parallel", "corpora", "and", "compare", "the", "results", "with", "manually", "built", "gold", "standards", "which", "mark", "transliterated", "word", "pairs", ".", "Finally", ",", "we", "integrate", "the", "transliteration", "module", "into", "the", "GIZA++", "word", "aligner", "and", "evaluate", "it", "on", "two", "word", "alignment", "tasks", "achieving", "improvements", "in", "both", "precision", "and", "recall", "measured", "against", "gold", "standard", "word", "alignments", "."], "entities": [{"type": "Operation", "start": 3, "end": 5, "text": "language-independent method", "sent_idx": 0}, {"type": "Effect", "start": 56, "end": 57, "text": "F-measure", "sent_idx": 2}, {"type": "Operation", "start": 102, "end": 111, "text": "integrate the transliteration module into the GIZA++ word aligner", "sent_idx": 4}, {"type": "Effect", "start": 123, "end": 124, "text": "precision", "sent_idx": 4}, {"type": "Effect", "start": 125, "end": 126, "text": "recall", "sent_idx": 4}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 2, "tail": 3}, {"type": "Pos_Affect", "head": 2, "tail": 4}], "id": "P11-1044"}
{"text": "Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.", "tokens": ["Automatic", "sentence", "summarization", "produces", "a", "shorter", "version", "of", "a", "sentence", ",", "while", "preserving", "its", "most", "important", "information", ".", "A", "good", "summary", "is", "characterized", "by", "language", "fluency", "and", "high", "information", "overlap", "with", "the", "source", "sentence", ".", "We", "model", "these", "two", "aspects", "in", "an", "unsupervised", "objective", "function", ",", "consisting", "of", "language", "modeling", "and", "semantic", "similarity", "metrics", ".", "We", "search", "for", "a", "high-scoring", "summary", "by", "discrete", "optimization", ".", "Our", "proposed", "method", "achieves", "a", "new", "state-of-the", "art", "for", "unsupervised", "sentence", "summarization", "according", "to", "ROUGE", "scores", ".", "Additionally", ",", "we", "demonstrate", "that", "the", "commonly", "reported", "ROUGE", "F1", "metric", "is", "sensitive", "to", "summary", "length", ".", "Since", "this", "is", "unwillingly", "exploited", "in", "recent", "work", ",", "we", "emphasize", "that", "future", "evaluation", "should", "explicitly", "group", "summarization", "systems", "by", "output", "length", "brackets", "."], "entities": [{"type": "Operation", "start": 96, "end": 98, "text": "summary length", "sent_idx": 5}, {"type": "Effect", "start": 90, "end": 92, "text": "ROUGE F1", "sent_idx": 5}], "relations": [{"type": "Affect", "head": 0, "tail": 1}], "id": "abstract-2020--acl-main--452"}
{"text": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).", "tokens": ["This", "paper", "introduces", "Dynamic", "Programming", "Encoding", "(", "DPE", ")", ",", "a", "new", "segmentation", "algorithm", "for", "tokenizing", "sentences", "into", "subword", "units", ".", "We", "view", "the", "subword", "segmentation", "of", "output", "sentences", "as", "a", "latent", "variable", "that", "should", "be", "marginalized", "out", "for", "learning", "and", "inference", ".", "A", "mixed", "character-subword", "transformer", "is", "proposed", ",", "which", "enables", "exact", "log", "marginal", "likelihood", "estimation", "and", "exact", "MAP", "inference", "to", "find", "target", "segmentations", "with", "maximum", "posterior", "probability", ".", "DPE", "uses", "a", "lightweight", "mixed", "character-subword", "transformer", "as", "a", "means", "of", "pre-processing", "parallel", "data", "to", "segment", "output", "sentences", "using", "dynamic", "programming", ".", "Empirical", "results", "on", "machine", "translation", "suggest", "that", "DPE", "is", "effective", "for", "segmenting", "output", "sentences", "and", "can", "be", "combined", "with", "BPE", "dropout", "for", "stochastic", "segmentation", "of", "source", "sentences", ".", "DPE", "achieves", "an", "average", "improvement", "of", "0.9", "BLEU", "over", "BPE", "(", "Sennrich", "et", "al.", ",", "2016", ")", "and", "an", "average", "improvement", "of", "0.55", "BLEU", "over", "BPE", "dropout", "(", "Provilkov", "et", "al.", ",", "2019", ")", "on", "several", "WMT", "datasets", "including", "English", "<", "=", ">", "(", "German", ",", "Romanian", ",", "Estonian", ",", "Finnish", ",", "Hungarian", ")", "."], "entities": [{"type": "Operation", "start": 120, "end": 121, "text": "DPE", "sent_idx": 5}, {"type": "Effect", "start": 127, "end": 128, "text": "BLEU", "sent_idx": 5}, {"type": "Effect", "start": 143, "end": 144, "text": "BLEU", "sent_idx": 5}], "relations": [{"type": "Pos_Affect", "head": 0, "tail": 1}, {"type": "Pos_Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--275"}
{"text": "Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.", "tokens": ["Historical", "text", "normalization", ",", "the", "task", "of", "mapping", "historical", "word", "forms", "to", "their", "modern", "counterparts", ",", "has", "recently", "attracted", "a", "lot", "of", "interest", "(", "Bollmann", ",", "2019", ";", "Tang", "et", "al.", ",", "2018", ";", "Lusetti", "et", "al.", ",", "2018", ";", "Bollmann", "et", "al.", ",", "2018;Robertson", "and", "Goldwater", ",", "2018", ";", "Bollmannet", "al.", ",", "2017", ";", "Korchagina", ",", "2017", ")", ".", "Yet", ",", "virtually", "all", "approaches", "suffer", "from", "the", "two", "limitations", ":", "1", ")", "They", "consider", "a", "fully", "supervised", "setup", ",", "often", "with", "impractically", "large", "manually", "normalized", "datasets", ";", "2", ")", "Normalization", "happens", "on", "words", "in", "isolation", ".", "By", "utilizing", "a", "simple", "generative", "normalization", "model", "and", "obtaining", "powerful", "contextualization", "from", "the", "target-side", "language", "model", ",", "we", "train", "accurate", "models", "with", "unlabeled", "historical", "data", ".", "In", "realistic", "training", "scenarios", ",", "our", "approach", "often", "leads", "to", "reduction", "in", "manually", "normalized", "data", "at", "the", "same", "accuracy", "levels", "."], "entities": [{"type": "Operation", "start": 115, "end": 122, "text": "train accurate models with unlabeled historical data", "sent_idx": 2}, {"type": "Effect", "start": 135, "end": 138, "text": "manually normalized data", "sent_idx": 3}, {"type": "Effect", "start": 141, "end": 142, "text": "accuracy", "sent_idx": 3}], "relations": [{"type": "Neg_Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 0, "tail": 2}], "id": "abstract-2020--acl-main--650"}
{"text": "We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example\u2019s domain. It requires no labeled data from the target domain, making it completely unsupervised. Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data. Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data. Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively.", "tokens": ["We", "tackle", "the", "task", "of", "building", "supervised", "event", "trigger", "identification", "models", "which", "can", "generalize", "better", "across", "domains", ".", "Our", "work", "leverages", "the", "adversarial", "domain", "adaptation", "(", "ADA", ")", "framework", "to", "introduce", "domain-invariance", ".", "ADA", "uses", "adversarial", "training", "to", "construct", "representations", "that", "are", "predictive", "for", "trigger", "identification", ",", "but", "not", "predictive", "of", "the", "example", "\u2019s", "domain", ".", "It", "requires", "no", "labeled", "data", "from", "the", "target", "domain", ",", "making", "it", "completely", "unsupervised", ".", "Experiments", "with", "two", "domains", "(", "English", "literature", "and", "news", ")", "show", "that", "ADA", "leads", "to", "an", "average", "F1", "score", "improvement", "of", "3.9", "on", "out-of-domain", "data", ".", "Our", "best", "performing", "model", "(", "BERT-A", ")", "reaches", "44", "-", "49", "F1", "across", "both", "domains", ",", "using", "no", "labeled", "target", "data", ".", "Preliminary", "experiments", "reveal", "that", "finetuning", "on", "1", "%", "labeled", "data", ",", "followed", "by", "self-training", "leads", "to", "substantial", "improvement", ",", "reaching", "51.5", "and", "67.2", "F1", "on", "literature", "and", "news", "respectively", "."], "entities": [{"type": "Operation", "start": 102, "end": 103, "text": "BERT-A", "sent_idx": 5}, {"type": "Effect", "start": 108, "end": 110, "text": "F1 across", "sent_idx": 5}, {"type": "Operation", "start": 113, "end": 118, "text": "using no labeled target data", "sent_idx": 5}, {"type": "Operation", "start": 132, "end": 133, "text": "self-training", "sent_idx": 6}, {"type": "Effect", "start": 142, "end": 143, "text": "F1", "sent_idx": 6}], "relations": [{"type": "Affect", "head": 0, "tail": 1}, {"type": "Affect", "head": 2, "tail": 1}, {"type": "Affect", "head": 3, "tail": 4}], "id": "abstract-2020--acl-main--681"}
