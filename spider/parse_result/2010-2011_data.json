[
  {
    "id": "P10-1000",
    "text": "",
    "pdf_url": "https://aclanthology.org/P10-1000.pdf",
    "title": "",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1001",
    "text": "We present algorithms for higher-order dependency parsing that are \"third-order\" in the sense that they can evaluate substructures containing three dependencies, and \"efficient\" in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.",
    "pdf_url": "https://aclanthology.org/P10-1001.pdf",
    "title": "Efficient Third-Order Dependency Parsers",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1002",
    "text": "In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.",
    "pdf_url": "https://aclanthology.org/P10-1002.pdf",
    "title": "Dependency Parsing and Projection Based on Word-Pair Classification",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1003",
    "text": "This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a target-side tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English.",
    "pdf_url": "https://aclanthology.org/P10-1003.pdf",
    "title": "Bitext Dependency Parsing with Bilingual Subtree Constraints",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1004",
    "text": "We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars.",
    "pdf_url": "https://aclanthology.org/P10-1004.pdf",
    "title": "Computing Weakest Readings",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1005",
    "text": "This paper presents a supervised approach for identifying generic noun phrases in context. Generic statements express rule-like knowledge about kinds or events. Therefore, their identification is important for the automatic construction of knowledge bases. In particular, the distinction between generic and non-generic statements is crucial for the correct encoding of generic and instance-level information. Generic expressions have been studied extensively in formal semantics. Building on this work, we explore a corpus-based learning approach for identifying generic NPs, using selections of linguistically motivated features. Our results perform well above the baseline and existing prior work.",
    "pdf_url": "https://aclanthology.org/P10-1005.pdf",
    "title": "Identifying Generic Noun Phrases",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1006",
    "text": "Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%.",
    "pdf_url": "https://aclanthology.org/P10-1006.pdf",
    "title": "Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1007",
    "text": "We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentence-level hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces word-error rates significantly by 10.2% relative to the best baseline models.",
    "pdf_url": "https://aclanthology.org/P10-1007.pdf",
    "title": "Correcting Errors in Speech Recognition with Articulatory Dynamics",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1008",
    "text": "We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical 'jargon' names of the domain entities. In such cases, dialogue systems must be able to model the user's (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user's domain expertise.",
    "pdf_url": "https://aclanthology.org/P10-1008.pdf",
    "title": "Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1009",
    "text": "In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches.",
    "pdf_url": "https://aclanthology.org/P10-1009.pdf",
    "title": "A Risk Minimization Framework for Extractive Speech Summarization",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1010",
    "text": "We present a grand challenge to build a corpus that will include all of the world's languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world's linguistic heritage before more languages fall silent.",
    "pdf_url": "https://aclanthology.org/P10-1010.pdf",
    "title": "The Human Language Project: Building a Universal Corpus of the World’s Languages",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1011",
    "text": "Bilingual lexicons are fundamental resources. Modern automated lexicon generation methods usually require parallel corpora, which are not available for most language pairs. Lexicons can be generated using non-parallel corpora or a pivot language, but such lexicons are noisy. We present an algorithm for generating a high quality lexicon from a noisy one, which only requires an independent corpus for each language. Our algorithm introduces non-aligned signatures (NAS), a cross-lingual word context similarity score that avoids the over-constrained and inefficient nature of alignment-based methods. We use NAS to eliminate incorrect translations from the generated lexicon. We evaluate our method by improving the quality of noisy Spanish-Hebrew lexicons generated from two pivot English lexicons. Our algorithm substantially outperforms other lexicon generation methods.",
    "pdf_url": "https://aclanthology.org/P10-1011.pdf",
    "title": "Bilingual Lexicon Generation Using Non-Aligned Signatures",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1012",
    "text": "As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced using automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR-7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.",
    "pdf_url": "https://aclanthology.org/P10-1012.pdf",
    "title": "Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1013",
    "text": "Information-extraction (IE) systems seek to distill semantic relations from natural-language text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? \n \nThis paper presents WOE, an open IE system which improves dramatically on TextRunner's precision and recall. The key to WOE's performance is a novel form of self-supervised learning for open extractors -- using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE's extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.",
    "pdf_url": "https://aclanthology.org/P10-1013.pdf",
    "title": "Open Information Extraction Using Wikipedia",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1014",
    "text": "As information extraction (IE) becomes more central to enterprise applications, rule-based IE engines have become increasingly important. In this paper, we describe SystemT, a rule-based IE system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars. SystemT uses a declarative rule language, AQL, and an optimizer that generates high-performance algebraic execution plans for AQL rules. We compare SystemT's approach against cascading grammars, both theoretically and with a thorough experimental evaluation. Our results show that SystemT can deliver result quality comparable to the state-of-the-art and an order of magnitude higher annotation throughput.",
    "pdf_url": "https://aclanthology.org/P10-1014.pdf",
    "title": "SystemT: An Algebraic Approach to Declarative Information Extraction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1015",
    "text": "We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel's setting. Our results provide evidence that the majority of novels in this time period do not fit two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks.",
    "pdf_url": "https://aclanthology.org/P10-1015.pdf",
    "title": "Extracting Social Networks from Literary Fiction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1016",
    "text": "The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PB-SMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain.",
    "pdf_url": "https://aclanthology.org/P10-1016.pdf",
    "title": "Pseudo-Word for Phrase-Based Machine Translation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1017",
    "text": "We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 Bleu score increase over a state-of-the-art syntax-based machine translation system.",
    "pdf_url": "https://aclanthology.org/P10-1017.pdf",
    "title": "Hierarchical Search for Word Alignment",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1018",
    "text": "Texts and dialogues often express information indirectly. For instance, speakers' answers to yes/no questions do not always straightforwardly convey a 'yes' or 'no' answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys 'yes' or 'no'. To evaluate the methods, we collected examples of question-answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys 'yes' or 'no'. Our experimental results closely match the Turkers' response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.",
    "pdf_url": "https://aclanthology.org/P10-1018.pdf",
    "title": "“Was It Good? It Was Provocative.” Learning the Meaning of Scalar Adjectives",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1019",
    "text": "Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turn-taking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dialogues, and supports the improvement of mixed-initiative interaction.",
    "pdf_url": "https://aclanthology.org/P10-1019.pdf",
    "title": "Importance-Driven Turn-Bidding for Spoken Dialogue Systems",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1020",
    "text": "One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.",
    "pdf_url": "https://aclanthology.org/P10-1020.pdf",
    "title": "Entity-Based Local Coherence Modelling Using Topological Fields",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1021",
    "text": "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.",
    "pdf_url": "https://aclanthology.org/P10-1021.pdf",
    "title": "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1022",
    "text": "Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions. \n \nWe demonstrate this rebanking process by creating an updated version of CCG-bank that includes the predicate-argument structure of both verbs and nouns, base-NP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser.",
    "pdf_url": "https://aclanthology.org/P10-1022.pdf",
    "title": "Rebanking CCGbank for Improved NP Interpretation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1023",
    "text": "In this paper we present BabelNet -- a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.",
    "pdf_url": "https://aclanthology.org/P10-1023.pdf",
    "title": "BabelNet: Building a Very Large Multilingual Semantic Network",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1024",
    "text": "The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.",
    "pdf_url": "https://aclanthology.org/P10-1024.pdf",
    "title": "Fully Unsupervised Core-Adjunct Argument Classification",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1025",
    "text": "Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.",
    "pdf_url": "https://aclanthology.org/P10-1025.pdf",
    "title": "Towards Open-Domain Semantic Role Labeling",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1026",
    "text": "Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.",
    "pdf_url": "https://aclanthology.org/P10-1026.pdf",
    "title": "A Bayesian Method for Robust Estimation of Distributional Similarities",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1027",
    "text": "The variety of engaging interactions among users in social medial distinguishes it from traditional Web media. Such a feature should be utilized while attempting to provide intelligent services to social media participants. In this article, we present a framework to recommend relevant information in Internet forums and blogs using user comments, one of the most representative of user behaviors in online discussion. When incorporating user comments, we consider structural, semantic, and authority information carried by them. One of the most important observation from this work is that semantic contents of user comments can play a fairly different role in a different form of social media. When designing a recommendation system for this purpose, such a difference must be considered with caution.",
    "pdf_url": "https://aclanthology.org/P10-1027.pdf",
    "title": "Recommendation in Internet Forums and Blogs",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1028",
    "text": "This paper explores the use of clickthrough data for query spelling correction. First, large amounts of query-correction pairs are derived by analyzing users' query reformulation behavior encoded in the clickthrough data. Then, a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system. Experiments are carried out on a human-labeled data set. Results show that the system using the phrase-based error model outperforms significantly its baseline systems.",
    "pdf_url": "https://aclanthology.org/P10-1028.pdf",
    "title": "Learning Phrase-Based Spelling Error Models from Clickthrough Data",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1029",
    "text": "This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynamically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.",
    "pdf_url": "https://aclanthology.org/P10-1029.pdf",
    "title": "Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1030",
    "text": "Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn't scale to the thousands of relations encoded in Web text. \n \nThis paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations --- more than an order of magnitude greater than any previous approach --- with an average F1 score of 61%. Crucial to LUCHS's performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse.",
    "pdf_url": "https://aclanthology.org/P10-1030.pdf",
    "title": "Learning 5000 Relational Extractors",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1031",
    "text": "Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.",
    "pdf_url": "https://aclanthology.org/P10-1031.pdf",
    "title": "Unsupervised Ontology Induction from Text",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1032",
    "text": "We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment.",
    "pdf_url": "https://aclanthology.org/P10-1032.pdf",
    "title": "Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1033",
    "text": "While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++.",
    "pdf_url": "https://aclanthology.org/P10-1033.pdf",
    "title": "Discriminative Pruning for Discriminative ITG Alignment",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1034",
    "text": "Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on large-scale bidirectional Japanese-English translations testified the effectiveness of our approach.",
    "pdf_url": "https://aclanthology.org/P10-1034.pdf",
    "title": "Fine-Grained Tree-to-String Translation Rule Extraction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1035",
    "text": "The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCG-bank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.",
    "pdf_url": "https://aclanthology.org/P10-1035.pdf",
    "title": "Accurate Context-Free Parsing with Combinatory Categorial Grammar",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1036",
    "text": "We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest-scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.",
    "pdf_url": "https://aclanthology.org/P10-1036.pdf",
    "title": "Faster Parsing by Supertagger Adaptation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1037",
    "text": "We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.",
    "pdf_url": "https://aclanthology.org/P10-1037.pdf",
    "title": "Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1038",
    "text": "Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.",
    "pdf_url": "https://aclanthology.org/P10-1038.pdf",
    "title": "Conditional Random Fields for Word Hyphenation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1039",
    "text": "This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily generalises to larger datasets. The first algorithm Promodes, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm Promodes-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.",
    "pdf_url": "https://aclanthology.org/P10-1039.pdf",
    "title": "Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1040",
    "text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/",
    "pdf_url": "https://aclanthology.org/P10-1040.pdf",
    "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1041",
    "text": "Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word related-ness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus.",
    "pdf_url": "https://aclanthology.org/P10-1041.pdf",
    "title": "Identifying Text Polarity Using Random Walks",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1042",
    "text": "Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product's attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a human-labeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL-SOT approach is easily generalized to labeling a mix of reviews of more than one products.",
    "pdf_url": "https://aclanthology.org/P10-1042.pdf",
    "title": "Sentiment Learning on Product Reviews via Sentiment Ontology Tree",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1043",
    "text": "In this paper, we adopt two views, personal and impersonal views, and systematically employ them in both supervised and semi-supervised sentiment classification. Here, personal views consist of those sentences which directly express speaker's feeling and preference towards a target object while impersonal views focus on statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach.",
    "pdf_url": "https://aclanthology.org/P10-1043.pdf",
    "title": "Employing Personal/Impersonal Views in Supervised and Semi-Supervised Sentiment Classification",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1044",
    "text": "The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional class-based approaches, it produces human-interpretable classes describing each relation's preferences, but it is competitive with non-class-based methods in predictive power. \n \nWe compare LDA-SP to several state-of-the-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP's effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.'s system (Pantel et al., 2007).",
    "pdf_url": "https://aclanthology.org/P10-1044.pdf",
    "title": "A Latent Dirichlet Allocation Method for Selectional Preferences",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1045",
    "text": "",
    "pdf_url": "https://aclanthology.org/P10-1045.pdf",
    "title": "",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1046",
    "text": "This paper improves the use of pseudo-words as an evaluation framework for selectional preferences. While pseudo-words originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-of-the-art by 13% absolute on a newspaper domain.",
    "pdf_url": "https://aclanthology.org/P10-1046.pdf",
    "title": "Improving the Use of Pseudo-Words for Evaluating Selectional Preferences",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1047",
    "text": "We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.",
    "pdf_url": "https://aclanthology.org/P10-1047.pdf",
    "title": "Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1048",
    "text": "We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.",
    "pdf_url": "https://aclanthology.org/P10-1048.pdf",
    "title": "Hindi-to-Urdu Machine Translation through Transliteration",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1049",
    "text": "Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.",
    "pdf_url": "https://aclanthology.org/P10-1049.pdf",
    "title": "Training Phrase Translation Models with Leaving-One-Out",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1050",
    "text": "The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, CARPEDIEM (Esposito and Radicioni, 2009).",
    "pdf_url": "https://aclanthology.org/P10-1050.pdf",
    "title": "Efficient Staggered Decoding for Sequence Labeling",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1051",
    "text": "We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.",
    "pdf_url": "https://aclanthology.org/P10-1051.pdf",
    "title": "Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1052",
    "text": "Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a l penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.",
    "pdf_url": "https://aclanthology.org/P10-1052.pdf",
    "title": "Practical Very Large Scale CRFs",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1053",
    "text": "Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the definition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.",
    "pdf_url": "https://aclanthology.org/P10-1053.pdf",
    "title": "On the Computational Complexity of Dominance Links in Grammatical Formalisms",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1054",
    "text": "Linear Context-Free Rewriting Systems (LCFRSs) are a grammar formalism capable of modeling discontinuous phrases. Many parsing applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) does not exceed 2. We present an efficient algorithm for optimal reduction of the length of production right-hand side in LCFRSs with fan-out at most 2. This results in asymptotical running time improvement for known parsing algorithms for this class.",
    "pdf_url": "https://aclanthology.org/P10-1054.pdf",
    "title": "Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1055",
    "text": "Combinatory Categorial Grammar (CCG) is generally construed as a fully lexicalized formalism, where all grammars use one and the same universal set of rules, and cross-linguistic variation is isolated in the lexicon. In this paper, we show that the weak generative capacity of this 'pure' form of CCG is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG.",
    "pdf_url": "https://aclanthology.org/P10-1055.pdf",
    "title": "The Importance of Rule Restrictions in CCG",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1056",
    "text": "To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.",
    "pdf_url": "https://aclanthology.org/P10-1056.pdf",
    "title": "Automatic Evaluation of Linguistic Quality in Multi-Document Summarization",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1057",
    "text": "Identifying background (context) information in scientific articles can help scholars understand major contributions in their research area more easily. In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientific papers. We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create, and employ a Belief Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientific papers. Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone.",
    "pdf_url": "https://aclanthology.org/P10-1057.pdf",
    "title": "Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1058",
    "text": "In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating \"story highlights\"---a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model's output is comparable to human-written highlights in terms of both grammaticality and content.",
    "pdf_url": "https://aclanthology.org/P10-1058.pdf",
    "title": "Automatic Generation of Story Highlights",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1059",
    "text": "In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.",
    "pdf_url": "https://aclanthology.org/P10-1059.pdf",
    "title": "Sentence and Expression Level Annotation of Opinions in User-Generated Discourse",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1060",
    "text": "We present a method for automatically generating focused and accurate topic-specific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system.",
    "pdf_url": "https://aclanthology.org/P10-1060.pdf",
    "title": "Generating Focused Topic-Specific Sentiment Lexicons",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1061",
    "text": "Subjectivity analysis is a rapidly growing field of study. Along with its applications to various NLP tasks, much work have put efforts into multilingual subjectivity learning from existing resources. Multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages. This paper proposes to measure the multilanguage-comparability of subjectivity analysis tools, and provides meaningful comparisons of multilingual subjectivity analysis from various points of view.",
    "pdf_url": "https://aclanthology.org/P10-1061.pdf",
    "title": "Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1062",
    "text": "Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from N-best lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.",
    "pdf_url": "https://aclanthology.org/P10-1062.pdf",
    "title": "Error Detection for Statistical Machine Translation Using Linguistic Features",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1063",
    "text": "The adoption of Machine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. \n \nWe quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.",
    "pdf_url": "https://aclanthology.org/P10-1063.pdf",
    "title": "TrustRank: Inducing Trust in Automatic Translations via Ranking",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1064",
    "text": "We propose a translation recommendation framework to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We describe an implementation of this framework using an SVM binary classifier. We exploit methods to fine-tune the classifier and investigate a variety of features of different types. We rely on automatic MT evaluation metrics to approximate human judgements in our experiments. Experimental results show that our system can achieve 0.85 precision at 0.89 recall, excluding exact matches. Furthermore, it is possible for the end-user to achieve a desired balance between precision and recall by adjusting confidence levels.",
    "pdf_url": "https://aclanthology.org/P10-1064.pdf",
    "title": "Bridging SMT and TM with Translation Recommendation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1065",
    "text": "We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task.",
    "pdf_url": "https://aclanthology.org/P10-1065.pdf",
    "title": "On Jointly Recognizing and Aligning Bilingual Named Entities",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1066",
    "text": "In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles. This kind of summary templates can be useful in various applications. We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of template slots that need to be filled in. We apply our method on five Wikipedia entity categories and compare our method with two baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method.",
    "pdf_url": "https://aclanthology.org/P10-1066.pdf",
    "title": "Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1067",
    "text": "Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. In this paper, we present a novel way to automatically mine comparable entities from comparative questions that users posted online to address this difficulty. To ensure high precision and high recall, we develop a weakly supervised bootstrapping approach for comparative question identification and comparable entity extraction by leveraging a large collection of online question archive. The experimental results show our method achieves F1-measure of 82.5 percent in comparative question identification and 83.3 percent in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. Additionally, our ranking results show highly relevance to user's comparison intents in web.",
    "pdf_url": "https://aclanthology.org/P10-1067.pdf",
    "title": "Comparable Entity Mining from Comparative Questions",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1068",
    "text": "This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis. \n \nIt is shown how annotations created by seven NLP tools are mapped onto tool-independent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way. \n \nFor morphosyntactic (parts of speech) and morphological annotations of three German corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations.",
    "pdf_url": "https://aclanthology.org/P10-1068.pdf",
    "title": "Towards Robust Multi-Tool Tagging. An OWL/DL-Based Approach",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1069",
    "text": "We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages.",
    "pdf_url": "https://aclanthology.org/P10-1069.pdf",
    "title": "Temporal Information Processing of a New Language: Fast Porting with Minimal Resources",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1070",
    "text": "The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.",
    "pdf_url": "https://aclanthology.org/P10-1070.pdf",
    "title": "A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1071",
    "text": "Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.",
    "pdf_url": "https://aclanthology.org/P10-1071.pdf",
    "title": "Models of Metaphor in NLP",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1072",
    "text": "We present a game-theoretic model of bargaining over a metaphor in the context of political communication, find its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures, and suggest applications of interest to computational linguists.",
    "pdf_url": "https://aclanthology.org/P10-1072.pdf",
    "title": "A Game-Theoretic Model of Metaphorical Bargaining",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1073",
    "text": "Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well.",
    "pdf_url": "https://aclanthology.org/P10-1073.pdf",
    "title": "Kernel Based Discourse Relation Recognition with Temporal Ordering Information",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1074",
    "text": "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still under-performs compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recognition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.",
    "pdf_url": "https://aclanthology.org/P10-1074.pdf",
    "title": "Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1075",
    "text": "We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process.",
    "pdf_url": "https://aclanthology.org/P10-1075.pdf",
    "title": "Detecting Errors in Automatically-Parsed Dependency Relations",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1076",
    "text": "In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems.",
    "pdf_url": "https://aclanthology.org/P10-1076.pdf",
    "title": "Boosting-Based System Combination for Machine Translation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1077",
    "text": "Prior use of machine learning in genre classification used a list of labels as classification categories. However, genre classes are often organised into hierarchies, e.g., covering the subgenres of fiction. In this paper we present a method of using the hierarchy of labels to improve the classification accuracy. As a testbed for this approach we use the Brown Corpus as well as a range of other corpora, including the BNC, HGC and Syracuse. The results are not encouraging: apart from the Brown corpus, the improvements of our structural classifier over the flat one are not statistically significant. We discuss the relation between structural learning performance and the visual and distributional balance of the label hierarchy, suggesting that only balanced hierarchies might profit from structural learning.",
    "pdf_url": "https://aclanthology.org/P10-1077.pdf",
    "title": "Fine-Grained Genre Classification Using Structural Learning Algorithms",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1078",
    "text": "This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental results on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.",
    "pdf_url": "https://aclanthology.org/P10-1078.pdf",
    "title": "Metadata-Aware Measures for Answer Summarization in Community Question Answering",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1079",
    "text": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.",
    "pdf_url": "https://aclanthology.org/P10-1079.pdf",
    "title": "A Hybrid Rule/Model-Based Finite-State Framework for Normalizing SMS Messages",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1080",
    "text": "Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.",
    "pdf_url": "https://aclanthology.org/P10-1080.pdf",
    "title": "Letter-Phoneme Alignment: An Exploration",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1081",
    "text": "Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction.",
    "pdf_url": "https://aclanthology.org/P10-1081.pdf",
    "title": "Using Document Level Cross-Event Inference to Improve Event Extraction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1082",
    "text": "In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system.",
    "pdf_url": "https://aclanthology.org/P10-1082.pdf",
    "title": "Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1083",
    "text": "We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.",
    "pdf_url": "https://aclanthology.org/P10-1083.pdf",
    "title": "Learning to Follow Navigational Directions",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1084",
    "text": "Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ~7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.",
    "pdf_url": "https://aclanthology.org/P10-1084.pdf",
    "title": "A Hybrid Hierarchical Model for Multi-Document Summarization",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1085",
    "text": "This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system.",
    "pdf_url": "https://aclanthology.org/P10-1085.pdf",
    "title": "Improving Statistical Machine Translation with Monolingual Collocation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1086",
    "text": "This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system.",
    "pdf_url": "https://aclanthology.org/P10-1086.pdf",
    "title": "Bilingual Sense Similarity for Statistical Machine Translation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1087",
    "text": "Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identified. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world's entities and concepts.",
    "pdf_url": "https://aclanthology.org/P10-1087.pdf",
    "title": "Untangling the Cross-Lingual Link Structure of Wikipedia",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1088",
    "text": "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.",
    "pdf_url": "https://aclanthology.org/P10-1088.pdf",
    "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1089",
    "text": "In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.",
    "pdf_url": "https://aclanthology.org/P10-1089.pdf",
    "title": "Creating Robust Supervised Classifiers via Web-Scale N-Gram Data",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1090",
    "text": "This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel.",
    "pdf_url": "https://aclanthology.org/P10-1090.pdf",
    "title": "Convolution Kernel over Packed Parse Forest",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1091",
    "text": "Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data.",
    "pdf_url": "https://aclanthology.org/P10-1091.pdf",
    "title": "Estimating Strictly Piecewise Distributions",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1092",
    "text": "This paper provides a unified, learning-theoretic analysis of several learnable classes of languages discussed previously in the literature. The analysis shows that for these classes an incremental, globally consistent, locally conservative, set-driven learner always exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition.",
    "pdf_url": "https://aclanthology.org/P10-1092.pdf",
    "title": "String Extension Learning",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1093",
    "text": "We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.",
    "pdf_url": "https://aclanthology.org/P10-1093.pdf",
    "title": "Compositional Matrix-Space Models of Language",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1094",
    "text": "Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informative-ness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach.",
    "pdf_url": "https://aclanthology.org/P10-1094.pdf",
    "title": "Cross-Language Document Summarization Based on Machine Translation Quality Prediction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1095",
    "text": "Automated summarization methods can be defined as \"language-independent,\" if they are not based on any language-specific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as \"processing several languages, with summary in the same language as input.\" In this paper, we introduce MUSE, a language-independent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages---English and Hebrew---and evaluated its performance with ROUGE-1 Recall vs. state-of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.",
    "pdf_url": "https://aclanthology.org/P10-1095.pdf",
    "title": "A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1096",
    "text": "We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.",
    "pdf_url": "https://aclanthology.org/P10-1096.pdf",
    "title": "Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1097",
    "text": "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.",
    "pdf_url": "https://aclanthology.org/P10-1097.pdf",
    "title": "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1098",
    "text": "We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.",
    "pdf_url": "https://aclanthology.org/P10-1098.pdf",
    "title": "Bootstrapping Semantic Analyzers from Non-Contradictory Texts",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1099",
    "text": "Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.",
    "pdf_url": "https://aclanthology.org/P10-1099.pdf",
    "title": "Open-Domain Semantic Role Labeling by Modeling Word Spans",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1100",
    "text": "We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect natural-language descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script's temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines.",
    "pdf_url": "https://aclanthology.org/P10-1100.pdf",
    "title": "Learning Script Knowledge with Web Experiments",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1101",
    "text": "A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argument-identification steps that precede Semantic Role Labeling (SRL) training. We combine a simplified SRL with an un-supervised HMM part of speech tagger, and experiment with psycholinguistically-motivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argument-identification stages.",
    "pdf_url": "https://aclanthology.org/P10-1101.pdf",
    "title": "Starting from Scratch in Semantic Role Labeling",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1102",
    "text": "Substantial research effort has been invested in recent decades into the computational study and automatic processing of multi-party conversation. While most aspects of conversational speech have benefited from a wide availability of analytic, computationally tractable techniques, only qualitative assessments are available for characterizing multi-party turn-taking. The current paper attempts to address this deficiency by first proposing a framework for computing turn-taking model perplexity, and then by evaluating several multi-participant modeling approaches. Experiments show that direct multi-participant models do not generalize to held out data, and likely never will, for practical reasons. In contrast, the Extended-Degree-of-Overlap model represents a suitable candidate for future work in this area, and is shown to successfully predict the distribution of speech in time and across participants in previously unseen conversations.",
    "pdf_url": "https://aclanthology.org/P10-1102.pdf",
    "title": "Modeling Norms of Turn-Taking in Multi-Party Conversation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1103",
    "text": "We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive \"lower level\" features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.",
    "pdf_url": "https://aclanthology.org/P10-1103.pdf",
    "title": "Optimising Information Presentation for Spoken Dialogue Systems",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1104",
    "text": "English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model's dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.",
    "pdf_url": "https://aclanthology.org/P10-1104.pdf",
    "title": "Combining Data and Mathematical Models of Language Change",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1105",
    "text": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
    "pdf_url": "https://aclanthology.org/P10-1105.pdf",
    "title": "Finding Cognate Groups Using Phylogenies",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1106",
    "text": "Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment problems.",
    "pdf_url": "https://aclanthology.org/P10-1106.pdf",
    "title": "An Exact A* Method for Deciphering Letter-Substitution Ciphers",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1107",
    "text": "In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and high-level morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.",
    "pdf_url": "https://aclanthology.org/P10-1107.pdf",
    "title": "A Statistical Model for Lost Language Decipherment",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1108",
    "text": "Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001).",
    "pdf_url": "https://aclanthology.org/P10-1108.pdf",
    "title": "Efficient Inference through Cascades of Weighted Tree Transducers",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1109",
    "text": "A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated.",
    "pdf_url": "https://aclanthology.org/P10-1109.pdf",
    "title": "A Tree Transducer Model for Synchronous Tree-Adjoining Grammars",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1110",
    "text": "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.",
    "pdf_url": "https://aclanthology.org/P10-1110.pdf",
    "title": "Dynamic Programming for Linear-Time Incremental Parsing",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1111",
    "text": "For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebank-based (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.",
    "pdf_url": "https://aclanthology.org/P10-1111.pdf",
    "title": "Hard Constraints for Grammatical Function Labelling",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1112",
    "text": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.",
    "pdf_url": "https://aclanthology.org/P10-1112.pdf",
    "title": "Simple, Accurate Parsing with an All-Fragments Grammar",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1113",
    "text": "This paper explores joint syntactic and semantic parsing of Chinese to further improve the performance of both syntactic and semantic parsing, in particular the performance of semantic parsing (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process. Secondly, semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing. Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated parsing approach outperforms the pipeline parsing approach on n-best parse trees, a natural extension of the widely used pipeline parsing approach on the top-best parse tree. Moreover, it shows that incorporating semantic role-related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing. To our best knowledge, this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicates in an integrated way.",
    "pdf_url": "https://aclanthology.org/P10-1113.pdf",
    "title": "Joint Syntactic and Semantic Parsing of Chinese",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1114",
    "text": "We present a new approach to cross-language text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce task-specific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of inter-language correspondence modeling. \n \nWe conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.",
    "pdf_url": "https://aclanthology.org/P10-1114.pdf",
    "title": "Cross-Language Text Classification Using Structural Correspondence Learning",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1115",
    "text": "Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Probabilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.",
    "pdf_url": "https://aclanthology.org/P10-1115.pdf",
    "title": "Cross-Lingual Latent Topic Extraction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1116",
    "text": "This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. non-literal usages of potentially idiomatic expressions. In all three cases, we outperform state-of-the-art systems either quantitatively or statistically significantly.",
    "pdf_url": "https://aclanthology.org/P10-1116.pdf",
    "title": "Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1117",
    "text": "This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as \"topic models\" to produce a low-dimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.",
    "pdf_url": "https://aclanthology.org/P10-1117.pdf",
    "title": "PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1118",
    "text": "We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eye-tracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that full-scale context is mostly negligible - with the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.",
    "pdf_url": "https://aclanthology.org/P10-1118.pdf",
    "title": "A Cognitive Cost Model of Annotations Based on Eye-Tracking Data",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1119",
    "text": "A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difficulty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a finer level of analysis, predicting not aggregate measures, but the duration and location of each fixation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye movement decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions.",
    "pdf_url": "https://aclanthology.org/P10-1119.pdf",
    "title": "A Rational Model of Eye Movement Control in Reading",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1120",
    "text": "Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.",
    "pdf_url": "https://aclanthology.org/P10-1120.pdf",
    "title": "The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1121",
    "text": "Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader's incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.",
    "pdf_url": "https://aclanthology.org/P10-1121.pdf",
    "title": "Complexity Metrics in an Incremental Right-Corner Parser",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1122",
    "text": "We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual inference phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.",
    "pdf_url": "https://aclanthology.org/P10-1122.pdf",
    "title": "“Ask Not What Textual Entailment Can Do for You...”",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1123",
    "text": "Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.",
    "pdf_url": "https://aclanthology.org/P10-1123.pdf",
    "title": "Assessing the Role of Discourse References in Entailment Inference",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1124",
    "text": "We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.",
    "pdf_url": "https://aclanthology.org/P10-1124.pdf",
    "title": "Global Learning of Focused Entailment Graphs",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1125",
    "text": "Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven question-answering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.",
    "pdf_url": "https://aclanthology.org/P10-1125.pdf",
    "title": "Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1126",
    "text": "In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.",
    "pdf_url": "https://aclanthology.org/P10-1126.pdf",
    "title": "How Many Words Is a Picture Worth? Automatic Caption Generation for News Images",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1127",
    "text": "This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.",
    "pdf_url": "https://aclanthology.org/P10-1127.pdf",
    "title": "Generating Image Descriptions Using Dependency Relational Patterns",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1128",
    "text": "This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on reference resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.",
    "pdf_url": "https://aclanthology.org/P10-1128.pdf",
    "title": "Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1129",
    "text": "In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging---they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policy-gradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.",
    "pdf_url": "https://aclanthology.org/P10-1129.pdf",
    "title": "Reading between the Lines: Learning to Map High-Level Instructions to Commands",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1130",
    "text": "We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus --- nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.",
    "pdf_url": "https://aclanthology.org/P10-1130.pdf",
    "title": "Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1131",
    "text": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.",
    "pdf_url": "https://aclanthology.org/P10-1131.pdf",
    "title": "Phylogenetic Grammar Induction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1132",
    "text": "We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.",
    "pdf_url": "https://aclanthology.org/P10-1132.pdf",
    "title": "Improved Unsupervised POS Induction through Prototype Discovery",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1133",
    "text": "We present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the Web. Given an object-attribute pair, we discover and analyze attribute information for a set of comparable objects in order to infer the desired value. This allows us to approximate the desired numerical values even when no exact values can be found in the text. \n \nOur framework makes use of relation defining patterns and WordNet similarity information. First, we obtain from the Web and WordNet a list of terms similar to the given object. Then we retrieve attribute values for each term in this list, and information that allows us to compare different objects in the list and to infer the attribute value range. Finally, we combine the retrieved data for all terms from the list to select or approximate the requested value. \n \nWe evaluate our method using automated question answering, WordNet enrichment, and comparison with answers given in Wikipedia and by leading search engines. In all of these, our framework provides a significant improvement.",
    "pdf_url": "https://aclanthology.org/P10-1133.pdf",
    "title": "Extraction and Approximation of Numerical Attributes from the Web",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1134",
    "text": "Definition extraction is the task of automatically identifying definitional sentences within texts. The task has proven useful in many research areas including ontology learning, relation extraction and question answering. However, current approaches -- mostly focused on lexicosyntactic patterns -- suffer from both low recall and precision, as definitional sentences occur in highly variable syntactic structures. In this paper, we propose Word-Class Lattices (WCLs), a generalization of word lattices that we use to model textual definitions. Lattices are learned from a dataset of definitions from Wikipedia. Our method is applied to the task of definition and hypernym extraction and compares favorably to other pattern generalization methods proposed in the literature.",
    "pdf_url": "https://aclanthology.org/P10-1134.pdf",
    "title": "Learning Word-Class Lattices for Definition and Hypernym Extraction",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1135",
    "text": "An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-of-the-art information extraction algorithm.",
    "pdf_url": "https://aclanthology.org/P10-1135.pdf",
    "title": "On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1136",
    "text": "Determining the semantic intent of web queries not only involves identifying their semantic class, which is a primary focus of previous works, but also understanding their semantic structure. In this work, we formally define the semantic structure of noun phrase queries as comprised of intent heads and intent modifiers. We present methods that automatically identify these constituents as well as their semantic roles based on Markov and semi-Markov conditional random fields. We show that the use of semantic features and syntactic features significantly contribute to improving the understanding performance.",
    "pdf_url": "https://aclanthology.org/P10-1136.pdf",
    "title": "Understanding the Semantic Structure of Noun Phrase Queries",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1137",
    "text": "In a previous work of ours Chinnakotla et al. (2010) we introduced a novel framework for Pseudo-Relevance Feedback (PRF) called MultiPRF. Given a query in one language called Source, we used English as the Assisting Language to improve the performance of PRF for the source language. MulitiPRF showed remarkable improvement over plain Model Based Feedback (MBF) uniformly for 4 languages, viz., French, German, Hungarian and Finnish with English as the assisting language. This fact inspired us to study the effect of any source-assistant pair on MultiPRF performance from out of a set of languages with widely different characteristics, viz., Dutch, English, Finnish, French, German and Spanish. Carrying this further, we looked into the effect of using two assisting languages together on PRF. \n \nThe present paper is a report of these investigations, their results and conclusions drawn therefrom. While performance improvement on MultiPRF is observed whatever the assisting language and whatever the source, observations are mixed when two assisting languages are used simultaneously. Interestingly, the performance improvement is more pronounced when the source and assisting languages are closely related, e.g., French and Spanish.",
    "pdf_url": "https://aclanthology.org/P10-1137.pdf",
    "title": "Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1138",
    "text": "Is it possible to use sense inventories to improve Web search results diversity for one word queries? To answer this question, we focus on two broad-coverage lexical resources of a different nature: WordNet, as a de-facto standard used in Word Sense Disambiguation experiments; and Wikipedia, as a large coverage, updated encyclopaedic resource which may have a better coverage of relevant senses in Web pages. \n \nOur results indicate that (i) Wikipedia has a much better coverage of search results, (ii) the distribution of senses in search results can be estimated using the internal graph structure of the Wikipedia and the relative number of visits received by each sense in Wikipedia, and (iii) associating Web pages to Wikipedia senses with simple and efficient algorithms, we can produce modified rankings that cover 70% more Wikipedia senses than the original search engine rankings.",
    "pdf_url": "https://aclanthology.org/P10-1138.pdf",
    "title": "Wikipedia as Sense Inventory to Improve Diversity in Web Search Results",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1139",
    "text": "There is a growing research interest in opinion retrieval as on-line users' opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user's query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representation, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement.",
    "pdf_url": "https://aclanthology.org/P10-1139.pdf",
    "title": "A Unified Graph Model for Sentence-Based Opinion Retrieval",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1140",
    "text": "Music Recommendation Systems often recommend individual songs, as opposed to entire albums. The challenge is to generate reviews for each song, since only full album reviews are available on-line. We developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs. We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries; and a user study of the impact of the song review summaries on users' decision making processes. Users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review.",
    "pdf_url": "https://aclanthology.org/P10-1140.pdf",
    "title": "Generating Fine-Grained Reviews of Songs from Album Reviews",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1141",
    "text": "Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.",
    "pdf_url": "https://aclanthology.org/P10-1141.pdf",
    "title": "A Study of Information Retrieval Weighting Schemes for Sentiment Analysis",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1142",
    "text": "The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.",
    "pdf_url": "https://aclanthology.org/P10-1142.pdf",
    "title": "Supervised Noun Phrase Coreference Research: The First Fifteen Years",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1143",
    "text": "This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.",
    "pdf_url": "https://aclanthology.org/P10-1143.pdf",
    "title": "Unsupervised Event Coreference Resolution with Rich Linguistic Features",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1144",
    "text": "This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.",
    "pdf_url": "https://aclanthology.org/P10-1144.pdf",
    "title": "Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1145",
    "text": "Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts.",
    "pdf_url": "https://aclanthology.org/P10-1145.pdf",
    "title": "Constituency to Dependency Translation with Forests",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1146",
    "text": "Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.",
    "pdf_url": "https://aclanthology.org/P10-1146.pdf",
    "title": "Learning to Translate with Source and Target Syntax",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1147",
    "text": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
    "pdf_url": "https://aclanthology.org/P10-1147.pdf",
    "title": "Discriminative Modeling of Extraction Sets for Machine Translation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1148",
    "text": "Weblogs are a source of human activity knowledge comprising valuable information such as facts, opinions and personal experiences. In this paper, we propose a method for mining personal experiences from a large set of weblogs. We define experience as knowledge embedded in a collection of activities or events which an individual or group has actually undergone. Based on an observation that experience-revealing sentences have a certain linguistic style, we formulate the problem of detecting experience as a classification task using various features including tense, mood, aspect, modality, experiencer, and verb classes. We also present an activity verb lexicon construction method based on theories of lexical semantics. Our results demonstrate that the activity verb lexicon plays a pivotal role among selected features in the classification performance and shows that our proposed method outperforms the baseline significantly.",
    "pdf_url": "https://aclanthology.org/P10-1148.pdf",
    "title": "Detecting Experiences from Weblogs",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1149",
    "text": "Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD algorithm is the most effective. We also show that class-instance extraction can be significantly improved by adding semantic information in the form of instance-attribute edges derived from an independently developed knowledge base. All of our code and data will be made publicly available to encourage reproducible research in this area.",
    "pdf_url": "https://aclanthology.org/P10-1149.pdf",
    "title": "Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1150",
    "text": "A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using \"verb\", \"noun\", and \"verb prep\" lexico-syntactic patterns. Human-based evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge.",
    "pdf_url": "https://aclanthology.org/P10-1150.pdf",
    "title": "Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1151",
    "text": "Finding a class of structures that is rich enough for adequate linguistic representation yet restricted enough for efficient computational processing is an important problem for dependency parsing. In this paper, we present a transition system for 2-planar dependency trees -- trees that can be decomposed into at most two planar graphs -- and show that it can be used to implement a classifier-based parser that runs in linear time and outperforms a state-of-the-art transition-based parser on four data sets from the CoNLL-X shared task. In addition, we present an efficient method for determining whether an arbitrary tree is 2-planar and show that 99% or more of the trees in existing treebanks are 2-planar.",
    "pdf_url": "https://aclanthology.org/P10-1151.pdf",
    "title": "A Transition-Based Parser for 2-Planar Dependency Structures",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1152",
    "text": "We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by \"Viterbi training.\" We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.",
    "pdf_url": "https://aclanthology.org/P10-1152.pdf",
    "title": "Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1153",
    "text": "Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques.",
    "pdf_url": "https://aclanthology.org/P10-1153.pdf",
    "title": "A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1154",
    "text": "One of the main obstacles to high-performance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.",
    "pdf_url": "https://aclanthology.org/P10-1154.pdf",
    "title": "Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1155",
    "text": "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy figures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
    "pdf_url": "https://aclanthology.org/P10-1155.pdf",
    "title": "All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1156",
    "text": "Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an All-Words unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme.",
    "pdf_url": "https://aclanthology.org/P10-1156.pdf",
    "title": "Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1157",
    "text": "Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.",
    "pdf_url": "https://aclanthology.org/P10-1157.pdf",
    "title": "Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1158",
    "text": "In this paper we develop a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement. A key feature in our approach is the reliance on a story planner which we acquire automatically by recording events, their participants, and their precedence relationships in a training corpus. Contrary to previous work our system does not follow a generate-and-rank architecture. Instead, we employ evolutionary search techniques to explore the space of possible stories which we argue are well suited to the story generation task. Experiments on generating simple children's stories show that our system outperforms previous data-driven approaches.",
    "pdf_url": "https://aclanthology.org/P10-1158.pdf",
    "title": "Plot Induction and Evolutionary Search for Story Generation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1159",
    "text": "We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, finding that it performs well even under the constraints of realtime generation.",
    "pdf_url": "https://aclanthology.org/P10-1159.pdf",
    "title": "Automated Planning for Situated Natural Language Generation",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P10-1160",
    "text": "Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.",
    "pdf_url": "https://aclanthology.org/P10-1160.pdf",
    "title": "Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates",
    "pwc": "",
    "year": 2010
  },
  {
    "id": "P11-1000",
    "text": "",
    "pdf_url": "https://aclanthology.org/P11-1000.pdf",
    "title": "",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1001",
    "text": "In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. \n \nOur models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.",
    "pdf_url": "https://aclanthology.org/P11-1001.pdf",
    "title": "A Word-Class Approach to Labeling PSCFG Rules for Machine Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1002",
    "text": "In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from non-parallel text.",
    "pdf_url": "https://aclanthology.org/P11-1002.pdf",
    "title": "Deciphering Foreign Language",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1003",
    "text": "In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.",
    "pdf_url": "https://aclanthology.org/P11-1003.pdf",
    "title": "Effective Use of Function Words for Rule Generalization in Forest-Based Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1004",
    "text": "This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages (we build an English to Finnish translation system). Our methods use unsupervised morphology induction. Unlike previous work we focus on morphologically productive phrase pairs -- our decoder can combine morphemes across phrase boundaries. Morphemes in the target language may not have a corresponding morpheme or word in the source language. Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation. We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods outperform previously proposed ones and provide the best known results on the English-Finnish Europarl translation task. Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology.",
    "pdf_url": "https://aclanthology.org/P11-1004.pdf",
    "title": "Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1005",
    "text": "Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when applied to noisy data.",
    "pdf_url": "https://aclanthology.org/P11-1005.pdf",
    "title": "Evaluating the Impact of Coder Errors on Active Learning",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1006",
    "text": "This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most \"similar\" to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.",
    "pdf_url": "https://aclanthology.org/P11-1006.pdf",
    "title": "A Fast and Accurate Method for Approximate String Search",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1007",
    "text": "We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effective on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.",
    "pdf_url": "https://aclanthology.org/P11-1007.pdf",
    "title": "Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1008",
    "text": "We describe an exact decoding algorithm for syntax-based statistical translation. The approach uses Lagrangian relaxation to decompose the decoding problem into tractable sub-problems, thereby avoiding exhaustive dynamic programming. The method recovers exact solutions, with certificates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders.",
    "pdf_url": "https://aclanthology.org/P11-1008.pdf",
    "title": "Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1009",
    "text": "We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.",
    "pdf_url": "https://aclanthology.org/P11-1009.pdf",
    "title": "Jigs and Lures: Associating Web Queries with Structured Entities",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1010",
    "text": "Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance.",
    "pdf_url": "https://aclanthology.org/P11-1010.pdf",
    "title": "Semi-Supervised SimHash for Efficient Document Similarity Search",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1011",
    "text": "Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.",
    "pdf_url": "https://aclanthology.org/P11-1011.pdf",
    "title": "Joint Annotation of Search Queries",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1012",
    "text": "We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it's more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.",
    "pdf_url": "https://aclanthology.org/P11-1012.pdf",
    "title": "Query Weighting for Ranking Model Adaptation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1013",
    "text": "Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text. The only supervision required by JST model learning is domain-independent polarity word priors. In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors. We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset. Furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches. Nevertheless, our approach is much simpler and does not require difficult parameter tuning.",
    "pdf_url": "https://aclanthology.org/P11-1013.pdf",
    "title": "Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1014",
    "text": "We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.",
    "pdf_url": "https://aclanthology.org/P11-1014.pdf",
    "title": "Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1015",
    "text": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "pdf_url": "https://aclanthology.org/P11-1015.pdf",
    "title": "Learning Word Vectors for Sentiment Analysis",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1016",
    "text": "Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.",
    "pdf_url": "https://aclanthology.org/P11-1016.pdf",
    "title": "Target-dependent Twitter Sentiment Classification",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1017",
    "text": "It has been widely recognized that one of the most difficult and intriguing problems in natural language processing (NLP) is how to cope with idiosyncratic multiword expressions. This paper presents an overview of the comprehensive dictionary (JDMWE) of Japanese multiword expressions. The JDMWE is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. The dictionary contains about 104,000 expressions, potentially 750,000 expressions. This paper shows that the JDMWE's validity can be supported by comparing the dictionary with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08, generated by Google Inc. (Kudo et al. 2009).",
    "pdf_url": "https://aclanthology.org/P11-1017.pdf",
    "title": "A Comprehensive Dictionary of Multiword Expressions",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1018",
    "text": "We describe an annotation tool developed to assist in the creation of multimodal action-communication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5--30) who control their avatars, perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasi-synchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously.",
    "pdf_url": "https://aclanthology.org/P11-1018.pdf",
    "title": "Multi-Modal Annotation of Quest Games in Second Life",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1019",
    "text": "We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of 'English as a Second or Other Language' (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of 'outlier' texts, we test the validity of our model and identify cases where the model's scores diverge from that of a human examiner.",
    "pdf_url": "https://aclanthology.org/P11-1019.pdf",
    "title": "A New Dataset and Method for Automatically Grading ESOL Texts",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1020",
    "text": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.",
    "pdf_url": "https://aclanthology.org/P11-1020.pdf",
    "title": "Collecting Highly Parallel Data for Paraphrase Evaluation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1021",
    "text": "This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the BLEU score and \"readability\" when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.",
    "pdf_url": "https://aclanthology.org/P11-1021.pdf",
    "title": "A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1022",
    "text": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict word-level and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.",
    "pdf_url": "https://aclanthology.org/P11-1022.pdf",
    "title": "Goodness: A Method for Measuring Machine Translation Confidence",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1023",
    "text": "We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, non-automatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semi-automated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER.",
    "pdf_url": "https://aclanthology.org/P11-1023.pdf",
    "title": "MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1024",
    "text": "This paper presents an exponential model for translation into highly inflected languages which can be scaled to very large datasets. As in other recent proposals, it predicts target-side phrases and can be conditioned on source-side context. However, crucially for the task of modeling morphological generalizations, it estimates feature parameters from the entire training set rather than as a collection of separate classifiers. We apply it to English-Czech translation, using a variety of features capturing potential predictors for case, number, and gender, and one of the largest publicly available parallel data sets. We also describe generation and modeling of inflected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies.",
    "pdf_url": "https://aclanthology.org/P11-1024.pdf",
    "title": "An exponential translation model for target language morphology",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1025",
    "text": "We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.",
    "pdf_url": "https://aclanthology.org/P11-1025.pdf",
    "title": "Bayesian Inference for Zodiac and Other Homophonic Ciphers",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1026",
    "text": null,
    "pdf_url": "https://aclanthology.org/P11-1026.pdf",
    "title": "Interactive topic modeling",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1027",
    "text": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.",
    "pdf_url": "https://aclanthology.org/P11-1027.pdf",
    "title": "Faster and Smaller N-Gram Language Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1028",
    "text": "This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with high-level guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built-in AI of Civilization II.",
    "pdf_url": "https://aclanthology.org/P11-1028.pdf",
    "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1029",
    "text": "Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text's potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented.",
    "pdf_url": "https://aclanthology.org/P11-1029.pdf",
    "title": "Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1030",
    "text": "This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.",
    "pdf_url": "https://aclanthology.org/P11-1030.pdf",
    "title": "Local Histograms of Character N-grams for Authorship Attribution",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1031",
    "text": "While computational estimation of difficulty of words in the lexicon is useful in many educational and assessment applications, the concept of scalar word difficulty and current corpus-based methods for its estimation are inadequate. We propose a new paradigm called word meaning maturity which tracks the degree of knowledge of each word at different stages of language learning. We present a computational algorithm for estimating word maturity, based on modeling language acquisition with Latent Semantic Analysis. We demonstrate that the resulting metric not only correlates well with external indicators, but captures deeper semantic effects in language.",
    "pdf_url": "https://aclanthology.org/P11-1031.pdf",
    "title": "Word Maturity: Computational Modeling of Word Knowledge",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1032",
    "text": "Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.",
    "pdf_url": "https://aclanthology.org/P11-1032.pdf",
    "title": "Finding Deceptive Opinion Spam by Any Stretch of the Imagination",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1033",
    "text": "Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%--8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines.",
    "pdf_url": "https://aclanthology.org/P11-1033.pdf",
    "title": "Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1034",
    "text": "This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker's opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.",
    "pdf_url": "https://aclanthology.org/P11-1034.pdf",
    "title": "A Pilot Study of Opinion Summarization in Conversations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1035",
    "text": "We present disputant relation-based method for classifying news articles on contentious issues. We observe that the disputants of a contention are an important feature for understanding the discourse. It performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame. The readers can attain balanced understanding on the contention, free from a specific biased view. We applied a modified version of HITS algorithm and an SVM classifier trained with pseudo-relevant data for article analysis.",
    "pdf_url": "https://aclanthology.org/P11-1035.pdf",
    "title": "Contrasting Opposing Views of News Articles on Contentious Issues",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1036",
    "text": "We present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. Our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. This approach directly enables discovery of highly rated or inconsistent properties of a product. Our model admits an efficient variational mean-field inference algorithm which can be parallelized and run on large snippet collections. We evaluate our model on a large corpus of snippets from Yelp reviews to assess property and attribute prediction. We demonstrate that it outperforms applicable baselines by a considerable margin.",
    "pdf_url": "https://aclanthology.org/P11-1036.pdf",
    "title": "Content Models with Attitude",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1037",
    "text": "The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailability of training data. We propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to tackle these challenges. The KNN based classifier conducts pre-labeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semi-supervised learning.",
    "pdf_url": "https://aclanthology.org/P11-1037.pdf",
    "title": "Recognizing Named Entities in Tweets",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1038",
    "text": "Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.",
    "pdf_url": "https://aclanthology.org/P11-1038.pdf",
    "title": "Lexical Normalisation of Short Text Messages: Makn Sens a #twitter",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1039",
    "text": "Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction.",
    "pdf_url": "https://aclanthology.org/P11-1039.pdf",
    "title": "Topical Keyphrase Extraction from Twitter",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1040",
    "text": "We present a novel method for record extraction from social streams such as Twitter. Unlike typical extraction setups, these environments are characterized by short, one sentence messages with heavily colloquial speech. To further complicate matters, individual messages may not express the full relation to be uncovered, as is often assumed in extraction tasks. We develop a graphical model that addresses these problems by learning a latent set of records and a record-message alignment simultaneously; the output of our model is a set of canonical records, the values of which are consistent with aligned messages. We demonstrate that our approach is able to accurately induce event records from Twitter messages, evaluated against events from a local city guide. Our method achieves significant error reduction over baseline methods.",
    "pdf_url": "https://aclanthology.org/P11-1040.pdf",
    "title": "Event Discovery in Social Media Feeds",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1041",
    "text": "Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs.",
    "pdf_url": "https://aclanthology.org/P11-1041.pdf",
    "title": "How do you pronounce your name? Improving G2P with transliterations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1042",
    "text": "We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, non-independent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.",
    "pdf_url": "https://aclanthology.org/P11-1042.pdf",
    "title": "Unsupervised Word Alignment with Arbitrary Features",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1043",
    "text": "Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.",
    "pdf_url": "https://aclanthology.org/P11-1043.pdf",
    "title": "Model-Based Aligner Combination Using Dual Decomposition",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1044",
    "text": "We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.",
    "pdf_url": "https://aclanthology.org/P11-1044.pdf",
    "title": "An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1045",
    "text": "Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. Using a log linear model, we learn the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions.",
    "pdf_url": "https://aclanthology.org/P11-1045.pdf",
    "title": "Beam-Width Prediction for Efficient Context-Free Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1046",
    "text": "We study the problem of finding the best head-driven parsing strategy for Linear Context-Free Rewriting System productions. A head-driven strategy must begin with a specified righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing.",
    "pdf_url": "https://aclanthology.org/P11-1046.pdf",
    "title": "Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1047",
    "text": "We present a method for the computation of prefix probabilities for synchronous context-free grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.",
    "pdf_url": "https://aclanthology.org/P11-1047.pdf",
    "title": "Prefix Probability for Probabilistic Synchronous Context-Free Grammars",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1048",
    "text": "Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.",
    "pdf_url": "https://aclanthology.org/P11-1048.pdf",
    "title": "A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1049",
    "text": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
    "pdf_url": "https://aclanthology.org/P11-1049.pdf",
    "title": "Jointly Learning to Extract and Compress",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1050",
    "text": "Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.",
    "pdf_url": "https://aclanthology.org/P11-1050.pdf",
    "title": "Discovery of Topically Coherent Sentences for Extractive Summarization",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1051",
    "text": "In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency.",
    "pdf_url": "https://aclanthology.org/P11-1051.pdf",
    "title": "Coherent Citation-Based Summarization of Scientific Papers",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1052",
    "text": "We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.",
    "pdf_url": "https://aclanthology.org/P11-1052.pdf",
    "title": "A Class of Submodular Functions for Document Summarization",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1053",
    "text": "We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system.",
    "pdf_url": "https://aclanthology.org/P11-1053.pdf",
    "title": "Semi-supervised Relation Extraction with Large-scale Word Clustering",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1054",
    "text": "We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance.",
    "pdf_url": "https://aclanthology.org/P11-1054.pdf",
    "title": "In-domain Relation Discovery with Meta-constraints via Posterior Regularization",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1055",
    "text": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). \n \nThis paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.",
    "pdf_url": "https://aclanthology.org/P11-1055.pdf",
    "title": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1056",
    "text": "In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.",
    "pdf_url": "https://aclanthology.org/P11-1056.pdf",
    "title": "Exploiting Syntactico-Semantic Structures for Relation Extraction",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1057",
    "text": "Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.",
    "pdf_url": "https://aclanthology.org/P11-1057.pdf",
    "title": "Together We Can: Bilingual Bootstrapping for WSD",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1058",
    "text": "Resolving polysemy and synonymy is required for high-quality information extraction. We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to. ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. Domain knowledge (the ontology) guides concept creation by defining a set of possible semantic types for concepts. Word sense induction is performed by inferring a set of semantic types for each noun phrase. Synonym detection exploits redundant information to train several domain-specific synonym classifiers in a semi-supervised fashion. When ConceptResolver is run on NELL's knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms.",
    "pdf_url": "https://aclanthology.org/P11-1058.pdf",
    "title": "Which Noun Phrases Denote Which Concepts?",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1059",
    "text": "Negation is present in all human languages and it is used to reverse the polarity of part of statements that are otherwise affirmative by default. A negated statement often carries positive implicit meaning, but to pinpoint the positive part from the negative part is rather difficult. This paper aims at thoroughly representing the semantics of negation by revealing implicit positive meaning. The proposed representation relies on focus of negation detection. For this, new annotation over PropBank and a learning algorithm are proposed.",
    "pdf_url": "https://aclanthology.org/P11-1059.pdf",
    "title": "Semantic Representation of Negation Using Focus Detection",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1060",
    "text": "",
    "pdf_url": "https://aclanthology.org/P11-1060.pdf",
    "title": "",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1061",
    "text": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.",
    "pdf_url": "https://aclanthology.org/P11-1061.pdf",
    "title": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1062",
    "text": "Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs.",
    "pdf_url": "https://aclanthology.org/P11-1062.pdf",
    "title": "Global Learning of Typed Entailment Rules",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1063",
    "text": "This paper describes a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing. Bottom-up and top-down parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.",
    "pdf_url": "https://aclanthology.org/P11-1063.pdf",
    "title": "Incremental Syntactic Language Models for Phrase-based Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1064",
    "text": "We present an unsupervised model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.",
    "pdf_url": "https://aclanthology.org/P11-1064.pdf",
    "title": "An Unsupervised Model for Joint Phrase Alignment and Extraction",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1065",
    "text": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.",
    "pdf_url": "https://aclanthology.org/P11-1065.pdf",
    "title": "Learning Hierarchical Translation Structure with Linguistic Annotations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1066",
    "text": "Community-based question answer (Q&A) has become an important issue due to the popularity of Q&A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrase-based translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A data demonstrate that our proposed phrase-based translation model significantly outperforms the state-of-the-art word-based translation model.",
    "pdf_url": "https://aclanthology.org/P11-1066.pdf",
    "title": "Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1067",
    "text": "Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon.",
    "pdf_url": "https://aclanthology.org/P11-1067.pdf",
    "title": "Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1068",
    "text": "We develop a general dynamic programming technique for the tabulation of transition-based dependency parsers, and apply it to obtain novel, polynomial-time algorithms for parsing with the arc-standard and arc-eager models. We also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods. Additionally, we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms.",
    "pdf_url": "https://aclanthology.org/P11-1068.pdf",
    "title": "Dynamic Programming Algorithms for Transition-Based Dependency Parsers",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1069",
    "text": "CCGs are directly compatible with binary-branching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.",
    "pdf_url": "https://aclanthology.org/P11-1069.pdf",
    "title": "Shift-Reduce CCG Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1070",
    "text": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
    "pdf_url": "https://aclanthology.org/P11-1070.pdf",
    "title": "Web-Scale Features for Full-Scale Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1071",
    "text": "Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisy-channel model. We show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance. \n \nOur approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model. We use large language models, introduce new features into this reranker and examine different optimisation strategies. We obtain a disfluency detection f-scores of 0.838 which improves upon the current state-of-the-art.",
    "pdf_url": "https://aclanthology.org/P11-1071.pdf",
    "title": "The impact of language models and loss functions on repair disfluency detection",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1072",
    "text": "Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.",
    "pdf_url": "https://aclanthology.org/P11-1072.pdf",
    "title": "Learning Sub-Word Units for Open Vocabulary Speech Recognition",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1073",
    "text": "This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. \n \nOur goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. \n \nOn human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2.",
    "pdf_url": "https://aclanthology.org/P11-1073.pdf",
    "title": "Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1074",
    "text": "In this paper, we adopt an n-best rescoring scheme using pitch-accent patterns to improve automatic speech recognition (ASR) performance. The pitch-accent model is decoupled from the main ASR system, thus allowing us to develop it independently. N-best hypotheses from recognizers are rescored by additional scores that measure the correlation of the pitch-accent patterns between the acoustic signal and lexical cues. To test the robustness of our algorithm, we use two different data sets and recognition setups: the first one is English radio news data that has pitch accent labels, but the recognizer is trained from a small amount of data and has high error rate; the second one is English broadcast news data using a state-of-the-art SRI recognizer. Our experimental results demonstrate that our approach is able to reduce word error rate relatively by about 3%. This gain is consistent across the two different tests, showing promising future directions of incorporating prosodic information to improve speech recognition.",
    "pdf_url": "https://aclanthology.org/P11-1074.pdf",
    "title": "N-Best Rescoring Based on Pitch-accent Patterns",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1075",
    "text": "The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.",
    "pdf_url": "https://aclanthology.org/P11-1075.pdf",
    "title": "Lexically-Triggered Hidden Markov Models for Clinical Document Coding",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1076",
    "text": "In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.",
    "pdf_url": "https://aclanthology.org/P11-1076.pdf",
    "title": "Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1077",
    "text": "We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy.",
    "pdf_url": "https://aclanthology.org/P11-1077.pdf",
    "title": "Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1078",
    "text": "Sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects. This paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. Our results validate the treatment of lect modeling as a text classification problem -- albeit a hard one -- and constitute a case for future research in computational sociolinguistics.",
    "pdf_url": "https://aclanthology.org/P11-1078.pdf",
    "title": "Extracting Social Power Relationships from Natural Language",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1079",
    "text": "In this paper, we present an unsupervised framework that bootstraps a complete coreference resolution (CoRe) system from word associations mined from a large unlabeled corpus. We show that word associations are useful for CoRe -- e.g., the strong association between Obama and President is an indicator of likely coreference. Association information has so far not been used in CoRe because it is sparse and difficult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system has better CoRe performance than other learning approaches that do not use manually labeled data.",
    "pdf_url": "https://aclanthology.org/P11-1079.pdf",
    "title": "Bootstrapping coreference resolution using word associations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1080",
    "text": "Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.",
    "pdf_url": "https://aclanthology.org/P11-1080.pdf",
    "title": "Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1081",
    "text": "We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis/Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for explicitly realized anaphors.",
    "pdf_url": "https://aclanthology.org/P11-1081.pdf",
    "title": "A Cross-Lingual ILP Solution to Zero Anaphora Resolution",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1082",
    "text": "While world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver. Hence, it is not clear whether these benefits can carry over to a stronger baseline. Moreover, since there has been no attempt to apply different sources of world knowledge in combination to coreference resolution, it is not clear whether they offer complementary benefits to a resolver. We systematically compare commonly-used and under-investigated sources of world knowledge for coreference resolution by applying them to two learning-based coreference models and evaluating them on documents annotated with two different annotation schemes.",
    "pdf_url": "https://aclanthology.org/P11-1082.pdf",
    "title": "Coreference Resolution with World Knowledge",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1083",
    "text": "The local multi bottom-up tree transducer is introduced and related to the (non-contiguous) synchronous tree sequence substitution grammar. It is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus. Finally, the problem of non-preservation of regularity is addressed. Three properties that ensure preservation are introduced, and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled.",
    "pdf_url": "https://aclanthology.org/P11-1083.pdf",
    "title": "How to train your multi bottom-up tree transducer",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1084",
    "text": "Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forest-to-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-to-string decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.",
    "pdf_url": "https://aclanthology.org/P11-1084.pdf",
    "title": "Binarized Forest to String Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1085",
    "text": "We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is statistically significant.",
    "pdf_url": "https://aclanthology.org/P11-1085.pdf",
    "title": "Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1086",
    "text": "Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B) as composed rules.",
    "pdf_url": "https://aclanthology.org/P11-1086.pdf",
    "title": "Rule Markov Models for Fast Tree-to-String Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1087",
    "text": "In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.",
    "pdf_url": "https://aclanthology.org/P11-1087.pdf",
    "title": "A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1088",
    "text": "Arabic handwriting recognition (HR) is a challenging problem due to Arabic's connected letter forms, consonantal diacritics and rich morphology. In this paper we isolate the task of identification of erroneous words in HR from the task of producing corrections for these words. We consider a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify these errors. Our best approach achieves a roughly ~15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words.",
    "pdf_url": "https://aclanthology.org/P11-1088.pdf",
    "title": "Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1089",
    "text": "Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the \"pipeline\" approach, assuming that morphological information has been separately obtained. \n \nHowever, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.",
    "pdf_url": "https://aclanthology.org/P11-1089.pdf",
    "title": "A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1090",
    "text": "This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.",
    "pdf_url": "https://aclanthology.org/P11-1090.pdf",
    "title": "Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1091",
    "text": "Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.",
    "pdf_url": "https://aclanthology.org/P11-1091.pdf",
    "title": "A Graph Approach to Spelling Correction in Domain-Centric Search",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1092",
    "text": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.",
    "pdf_url": "https://aclanthology.org/P11-1092.pdf",
    "title": "Grammatical Error Correction with Alternating Structure Optimization",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1093",
    "text": "We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction - algorithm selection and model adaptation to the first language of the ESL learner. \n \nA variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incomparable data sets. We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. Our results hold for different training sets, genres, and feature sets. \n \nA second key issue in ESL error correction is the adaptation of a model to the first language of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the non-native writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods.",
    "pdf_url": "https://aclanthology.org/P11-1093.pdf",
    "title": "Algorithm Selection and Model Adaptation for ESL Correction Tasks",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1094",
    "text": "Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope. We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the burden of acquiring a large corpora of corrected sentences. We also present a cheap and efficient way to provide automated evaluation results for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.",
    "pdf_url": "https://aclanthology.org/P11-1094.pdf",
    "title": "Automated Whole Sentence Grammar Correction Using a Noisy Channel Model",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1095",
    "text": "Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base. Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. In this paper, we propose a generative probabilistic model, called entity-mention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task. In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s\\e), and the distribution of possible contexts of a specific entity P(c\\e). To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s\\e) and P(c\\e). Experimental results show that our method can significantly outperform the traditional methods.",
    "pdf_url": "https://aclanthology.org/P11-1095.pdf",
    "title": "A Generative Entity-Mention Model for Linking Entities with Knowledge Base",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1096",
    "text": "We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.",
    "pdf_url": "https://aclanthology.org/P11-1096.pdf",
    "title": "Simple supervised document geolocation with geodesic grids",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1097",
    "text": "We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.",
    "pdf_url": "https://aclanthology.org/P11-1097.pdf",
    "title": "Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1098",
    "text": "Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.",
    "pdf_url": "https://aclanthology.org/P11-1098.pdf",
    "title": "Template-Based Information Extraction without the Templates",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1099",
    "text": "Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accuracies of 63--91% in one-against-others classification and 80--94% in pairwise classification (baseline = 50% in both cases).",
    "pdf_url": "https://aclanthology.org/P11-1099.pdf",
    "title": "Classifying arguments by scheme",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1100",
    "text": "We present a novel model to represent and assess the discourse coherence of text. Our model assumes that coherent text implicitly favors certain types of discourse relation transitions. We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. The experimental results demonstrate that our model is able to significantly outperform the state-of-the-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task.",
    "pdf_url": "https://aclanthology.org/P11-1100.pdf",
    "title": "Automatically Evaluating Text Coherence Using Discourse Relations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1101",
    "text": "This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice.",
    "pdf_url": "https://aclanthology.org/P11-1101.pdf",
    "title": "Underspecifying and Predicting Voice for Surface Realisation Ranking",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1102",
    "text": "We present a novel computational formulation of speaker authority in discourse. This notion, which focuses on how speakers position themselves relative to each other in discourse, is first developed into a reliable coding scheme (0.71 agreement between human annotators). We also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming. We show that this constrained model's analyses of speaker authority correlates very strongly with expert human judgments (r2 coefficient of 0.947).",
    "pdf_url": "https://aclanthology.org/P11-1102.pdf",
    "title": "Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1103",
    "text": "One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the Bleu score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of Bleu scores.",
    "pdf_url": "https://aclanthology.org/P11-1103.pdf",
    "title": "Reordering Metrics for MT",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1104",
    "text": "This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods.",
    "pdf_url": "https://aclanthology.org/P11-1104.pdf",
    "title": "Reordering with Source Language Collocations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1105",
    "text": "We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the \"N-gram\" model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.",
    "pdf_url": "https://aclanthology.org/P11-1105.pdf",
    "title": "A Joint Sequence Translation Model with Integrated Reordering",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1106",
    "text": "A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.",
    "pdf_url": "https://aclanthology.org/P11-1106.pdf",
    "title": "Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1107",
    "text": "When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. I argue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.",
    "pdf_url": "https://aclanthology.org/P11-1107.pdf",
    "title": "Metagrammar engineering: Towards systematic exploration of implemented grammars",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1108",
    "text": "We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing---the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsu-pervised parser, Seginer's (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.",
    "pdf_url": "https://aclanthology.org/P11-1108.pdf",
    "title": "Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1109",
    "text": "We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%.",
    "pdf_url": "https://aclanthology.org/P11-1109.pdf",
    "title": "Extracting Paraphrases from Definition Sentences on the Web",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1110",
    "text": "We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.",
    "pdf_url": "https://aclanthology.org/P11-1110.pdf",
    "title": "Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1111",
    "text": "In this work, we present a novel approach to the generation task of ordering prenominal modifiers. We take a maximum entropy reranking approach to the problem which admits arbitrary features on a permutation of modifiers, exploiting hundreds of thousands of features in total. We compare our error rates to the state-of-the-art and to a strong Google n-gram count baseline. We attain a maximum error reduction of 69.8% and average error reduction across all test sets of 59.1% compared to the state-of-the-art and a maximum error reduction of 68.4% and average error reduction across all test sets of 41.8% compared to our Google n-gram count baseline.",
    "pdf_url": "https://aclanthology.org/P11-1111.pdf",
    "title": "Ordering Prenominal Modifiers with a Reranking Approach",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1112",
    "text": "In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.",
    "pdf_url": "https://aclanthology.org/P11-1112.pdf",
    "title": "Unsupervised Semantic Role Induction via Split-Merge Clustering",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1113",
    "text": "Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entity-type consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction.",
    "pdf_url": "https://aclanthology.org/P11-1113.pdf",
    "title": "Using Cross-Entity Inference to Improve Event Extraction",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1114",
    "text": "The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multi-layered event extraction architecture that progressively \"zooms in\" on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.",
    "pdf_url": "https://aclanthology.org/P11-1114.pdf",
    "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1115",
    "text": "In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (\"slots\") derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.",
    "pdf_url": "https://aclanthology.org/P11-1115.pdf",
    "title": "Knowledge Base Population: Successful Approaches and Challenges",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1116",
    "text": "This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision.",
    "pdf_url": "https://aclanthology.org/P11-1116.pdf",
    "title": "Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1117",
    "text": "This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.",
    "pdf_url": "https://aclanthology.org/P11-1117.pdf",
    "title": "A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1118",
    "text": "We evaluate several popular models of local discourse coherence for domain and task generality by applying them to chat disentanglement. Using experiments on synthetic multiparty conversations, we show that most models transfer well from text to dialogue. Coherence models improve results overall when good parses and topic models are available, and on a constrained task for real chat data.",
    "pdf_url": "https://aclanthology.org/P11-1118.pdf",
    "title": "Disentangling Chat with Local Coherence Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1119",
    "text": "Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affect-enriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.",
    "pdf_url": "https://aclanthology.org/P11-1119.pdf",
    "title": "An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1120",
    "text": "We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.",
    "pdf_url": "https://aclanthology.org/P11-1120.pdf",
    "title": "Fine-Grained Class Label Markup of Search Queries",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1121",
    "text": "The availability of learner corpora, especially those which have been manually error-tagged or shallow-parsed, is still limited. This means that researchers do not have a common development and test set for natural language processing of learner English such as for grammatical error detection. Given this background, we created a novel learner corpus that was manually error-tagged and shallow-parsed. This corpus is available for research and educational purposes on the web. In this paper, we describe it in detail together with its data-collection method and annotation schemes. Another contribution of this paper is that we take the first step toward evaluating the performance of existing POS-tagging/chunking techniques on learner corpora using the created corpus. These contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring.",
    "pdf_url": "https://aclanthology.org/P11-1121.pdf",
    "title": "Creating a manually error-tagged and shallow-parsed learner corpus",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1122",
    "text": "Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.",
    "pdf_url": "https://aclanthology.org/P11-1122.pdf",
    "title": "Crowdsourcing Translation: Professional Quality from Non-Professionals",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1123",
    "text": "In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.",
    "pdf_url": "https://aclanthology.org/P11-1123.pdf",
    "title": "A Statistical Tree Annotator and Its Applications",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1124",
    "text": "We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar 'translation example' retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English--Chinese technical documents.",
    "pdf_url": "https://aclanthology.org/P11-1124.pdf",
    "title": "Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1125",
    "text": "The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.",
    "pdf_url": "https://aclanthology.org/P11-1125.pdf",
    "title": "Machine Translation System Combination by Confusion Forest",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1126",
    "text": "This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.",
    "pdf_url": "https://aclanthology.org/P11-1126.pdf",
    "title": "Hypothesis Mixture Decoding for Statistical Machine Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1127",
    "text": "We present minimum Bayes-risk system combination, a method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems' probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions. MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.",
    "pdf_url": "https://aclanthology.org/P11-1127.pdf",
    "title": "Minimum Bayes-risk System Combination",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1128",
    "text": "We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-to-string system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets.",
    "pdf_url": "https://aclanthology.org/P11-1128.pdf",
    "title": "Adjoining Tree-to-String Translation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1129",
    "text": "In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.",
    "pdf_url": "https://aclanthology.org/P11-1129.pdf",
    "title": "Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1130",
    "text": "We propose a novel approach to translating from a morphologically complex language. Unlike previous research, which has targeted word inflections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as potential paraphrases and handle using paraphrasing techniques at the word, phrase, and sentence level. An important advantage of this framework is that it can cope with derivational morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens).",
    "pdf_url": "https://aclanthology.org/P11-1130.pdf",
    "title": "Translating from Morphologically Complex Languages: A Paraphrase-Based Approach",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1131",
    "text": "We propose a principled and efficient phrase-to-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include \"gappy phrases\" (such as French ne * pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.",
    "pdf_url": "https://aclanthology.org/P11-1131.pdf",
    "title": "Gappy Phrasal Alignment By Agreement",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1132",
    "text": "While it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. Some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. Using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. There are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. Significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original.",
    "pdf_url": "https://aclanthology.org/P11-1132.pdf",
    "title": "Translationese and Its Dialects",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1133",
    "text": "We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1 to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.",
    "pdf_url": "https://aclanthology.org/P11-1133.pdf",
    "title": "Rare Word Translation Extraction from Aligned Comparable Documents",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1134",
    "text": "This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.",
    "pdf_url": "https://aclanthology.org/P11-1134.pdf",
    "title": "Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1135",
    "text": "Resolving coordination ambiguity is a classic hard problem. This paper looks at co-ordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don't do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training. The co-trained classifier achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations.",
    "pdf_url": "https://aclanthology.org/P11-1135.pdf",
    "title": "Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1136",
    "text": "We propose a novel unsupervised method for separating out distinct authorial components of a document. In particular, we show that, given a book artificially \"munged\" from two thematically similar biblical books, we can separate out the two constituent books almost perfectly. This allows us to automatically recapitulate many conclusions reached by Bible scholars over centuries of research. One of the key elements of our method is exploitation of differences in synonym choice by different authors.",
    "pdf_url": "https://aclanthology.org/P11-1136.pdf",
    "title": "Unsupervised Decomposition of a Document into Authorial Components",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1137",
    "text": "We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors' geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite e1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.",
    "pdf_url": "https://aclanthology.org/P11-1137.pdf",
    "title": "Discovering Sociolinguistic Associations with Structured Sparsity",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1138",
    "text": "Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.",
    "pdf_url": "https://aclanthology.org/P11-1138.pdf",
    "title": "Local and Global Algorithms for Disambiguation to Wikipedia",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1139",
    "text": "The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is efficient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Tree-bank shows that our model yields improvements over the best system reported in the literature.",
    "pdf_url": "https://aclanthology.org/P11-1139.pdf",
    "title": "A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1140",
    "text": "Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training, they pose a challenge for translation systems. Previous decompounding methods have often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. Furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach.",
    "pdf_url": "https://aclanthology.org/P11-1140.pdf",
    "title": "Language-independent compound splitting with morphological operations",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1141",
    "text": "Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way.",
    "pdf_url": "https://aclanthology.org/P11-1141.pdf",
    "title": "Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1142",
    "text": "There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn't a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.",
    "pdf_url": "https://aclanthology.org/P11-1142.pdf",
    "title": "A Simple Measure to Assess Non-response",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1143",
    "text": "In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users' information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user's query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.",
    "pdf_url": "https://aclanthology.org/P11-1143.pdf",
    "title": "Improving Question Recommendation by Exploiting Information Need",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1144",
    "text": "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline.",
    "pdf_url": "https://aclanthology.org/P11-1144.pdf",
    "title": "Semi-Supervised Frame-Semantic Parsing for Unknown Predicates",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1145",
    "text": "We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical Pitman-Yor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the Metropolis-Hastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.",
    "pdf_url": "https://aclanthology.org/P11-1145.pdf",
    "title": "A Bayesian Model for Unsupervised Semantic Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1146",
    "text": "This paper presents an unsupervised method for deriving inference axioms by composing semantic relations. The method is independent of any particular relation inventory. It relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. The method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over PropBank.",
    "pdf_url": "https://aclanthology.org/P11-1146.pdf",
    "title": "Unsupervised Learning of Semantic Relation Composition",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1147",
    "text": "Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicate-argument structures like \"quarterbacks throw passes to receivers\". Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.",
    "pdf_url": "https://aclanthology.org/P11-1147.pdf",
    "title": "Unsupervised Discovery of Domain-Specific Knowledge from Text",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1148",
    "text": "In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the semeval-2010 word sense induction and disambiguation task, on which it reaches state-of-the-art results.",
    "pdf_url": "https://aclanthology.org/P11-1148.pdf",
    "title": "Latent Semantic Word Sense Induction and Disambiguation",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1149",
    "text": "Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. \n \nWe argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.",
    "pdf_url": "https://aclanthology.org/P11-1149.pdf",
    "title": "Confidence Driven Unsupervised Semantic Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1150",
    "text": "In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers' opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers' opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers' opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of document-level sentiment classification, and improve the performance significantly.",
    "pdf_url": "https://aclanthology.org/P11-1150.pdf",
    "title": "Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1151",
    "text": "This paper explores approaches to sentiment classification of U. S. Congressional floor-debate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.",
    "pdf_url": "https://aclanthology.org/P11-1151.pdf",
    "title": "Collective Classification of Congressional Floor-Debate Transcripts",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1152",
    "text": "Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models.",
    "pdf_url": "https://aclanthology.org/P11-1152.pdf",
    "title": "Integrating history-length interpolation and classes in language modeling",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1153",
    "text": "Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. However, existing topic models generally cannot capture the latent topical structures in documents. Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks. \n \nIn this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order Markov chain. Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering.",
    "pdf_url": "https://aclanthology.org/P11-1153.pdf",
    "title": "Structural Topic Model for Latent Topical Structure Analysis",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1154",
    "text": "We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.",
    "pdf_url": "https://aclanthology.org/P11-1154.pdf",
    "title": "Automatic Labelling of Topic Models",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1155",
    "text": "Cross-language document summarization is defined as the task of producing a summary in a target language (e.g. Chinese) for a set of documents in a source language (e.g. English). Existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. In this study, we propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods.",
    "pdf_url": "https://aclanthology.org/P11-1155.pdf",
    "title": "Using Bilingual Information for Cross-Language Document Summarization",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1156",
    "text": "In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to word-to-word selectional preferences by using web-scale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.",
    "pdf_url": "https://aclanthology.org/P11-1156.pdf",
    "title": "Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1157",
    "text": "It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective -- it outperforms random data selection on both languages examined, English and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English.",
    "pdf_url": "https://aclanthology.org/P11-1157.pdf",
    "title": "Effective Measures of Domain Similarity for Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1158",
    "text": "We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser's search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.",
    "pdf_url": "https://aclanthology.org/P11-1158.pdf",
    "title": "Efficient CCG Parsing: A* versus Adaptive Supertagging",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1159",
    "text": "We explore the contribution of morphological features -- both lexical and inflectional -- to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., \"broken plurals\") and the related rationality feature improve over form-based features. It is the first time functional morphological features are used for Arabic NLP.",
    "pdf_url": "https://aclanthology.org/P11-1159.pdf",
    "title": "Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1160",
    "text": "Recent work has shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignments. The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. In this paper, we present a greedy non-directional parsing algorithm which doesn't need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi.",
    "pdf_url": "https://aclanthology.org/P11-1160.pdf",
    "title": "Partial Parsing from Bitext Projections",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1161",
    "text": "The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using document-based counts.",
    "pdf_url": "https://aclanthology.org/P11-1161.pdf",
    "title": "Ranking Class Labels Using Query Sessions",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1162",
    "text": "Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.",
    "pdf_url": "https://aclanthology.org/P11-1162.pdf",
    "title": "Insights from Network Structure for Text Mining",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1163",
    "text": "Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a \"crime\" event can cause a \"investigation\" event, which can lead to an \"arrest\" event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP'09 shared task with a F1 score of 53.5% in development and 48.6% in testing.",
    "pdf_url": "https://aclanthology.org/P11-1163.pdf",
    "title": "Event Extraction as Dependency Parsing",
    "pwc": "",
    "year": 2011
  },
  {
    "id": "P11-1164",
    "text": "The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use.",
    "pdf_url": "https://aclanthology.org/P11-1164.pdf",
    "title": "Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification",
    "pwc": "",
    "year": 2011
  }
]